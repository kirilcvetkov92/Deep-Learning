{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kiko-PC\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import datasets\n",
    "import tensorflow as tf\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 83097/83097 [00:02<00:00, 27763.60it/s]\n"
     ]
    }
   ],
   "source": [
    "data_o = datasets.readCornellData('data/cornell')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('there', 'where'),\n",
       " ('have fun tonight', 'tons'),\n",
       " ('what good stuff', 'the real you'),\n",
       " ('wow', 'lets go'),\n",
       " ('she okay', 'i hope so')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_o[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.3333)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kiko-PC\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\gensim\\utils.py:1209: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.models.keyedvectors import KeyedVectors as kv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_symbol = '[^]'\n",
    "end_symbol = '[$]'\n",
    "padding_symbol = '[#]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean = []\n",
    "word_set = set([start_symbol, end_symbol, padding_symbol])\n",
    "for chats in data_o:\n",
    "    words = ' '.join(chats).split()\n",
    "    word_set.update(words)\n",
    "    data_clean.append([chats[0].split(), chats[1].split()])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(data_clean[:5000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24792"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2id = {symbol:i for i, symbol in enumerate(word_set)}\n",
    "id2word = {i:symbol for symbol, i in word2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding_dim = 300\n",
    "start_symbol_id = word2id[start_symbol]\n",
    "end_symbol_id = word2id[end_symbol]\n",
    "pad_symbol_id = word2id[padding_symbol]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6115\n",
      "5423\n",
      "1928\n"
     ]
    }
   ],
   "source": [
    "print(start_symbol_id)\n",
    "print(end_symbol_id)\n",
    "print(pad_symbol_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_ids(sentence, word2id, padded_len):\n",
    "    \"\"\" Converts a sequence of symbols to a padded sequence of their ids.\n",
    "    \n",
    "      sentence: a string, input/output sequence of symbols.\n",
    "      word2id: a dict, a mapping from original symbols to ids.\n",
    "      padded_len: an integer, a desirable length of the sequence.\n",
    "\n",
    "      result: a tuple of (a list of ids, an actual length of sentence).\n",
    "    \"\"\"\n",
    "            \n",
    "   \n",
    "    sent_ids = [word2id[word] for word in sentence[:padded_len]]\n",
    "    if padded_len>len(sentence):\n",
    "        sent_ids.append(word2id['[$]'])\n",
    "        sent_ids += [word2id['[#]']]*(padded_len-len(sent_ids))\n",
    "        return sent_ids, len(sentence)+1\n",
    "    sent_ids[-1] = word2id['[$]']\n",
    "    return sent_ids, padded_len\n",
    "\n",
    "\n",
    "def  ids_to_sentence(ids, id2word):\n",
    "    \"\"\" Converts a sequence of ids to a sequence of symbols.\n",
    "    \n",
    "          ids: a list, indices for the padded sequence.\n",
    "          id2word:  a dict, a mapping from ids to original symbols.\n",
    "\n",
    "          result: a list of symbols.\n",
    "    \"\"\"\n",
    " \n",
    "    return [id2word[i] for i in ids]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_to_ids(sentences, word2id, max_len):\n",
    "    \"\"\"Prepares batches of indices. \n",
    "    \n",
    "       Sequences are padded to match the longest sequence in the batch,\n",
    "       if it's longer than max_len, then max_len is used instead.\n",
    "\n",
    "        sentences: a list of strings, original sequences.\n",
    "        word2id: a dict, a mapping from original symbols to ids.\n",
    "        max_len: an integer, max len of sequences allowed.\n",
    "\n",
    "        result: a list of lists of ids, a list of actual lengths.\n",
    "    \"\"\"\n",
    "    \n",
    "    max_len_in_batch = min(max(len(s) for s in sentences) + 1, max_len)\n",
    "    batch_ids, batch_ids_len = [], []\n",
    "    for sentence in sentences:\n",
    "        ids, ids_len = sentence_to_ids(sentence, word2id, max_len_in_batch)\n",
    "        batch_ids.append(ids)\n",
    "        batch_ids_len.append(ids_len)\n",
    "    return batch_ids, batch_ids_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batches(samples, batch_size=64):\n",
    "    X, Y = [], []\n",
    "    for i, (x, y) in enumerate(samples, 1):\n",
    "        X.append(x)\n",
    "        Y.append(y)\n",
    "        if i % batch_size == 0:\n",
    "            yield X, Y\n",
    "            X, Y = [], []\n",
    "    if X and Y:\n",
    "        yield X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_prepare(text):\n",
    "    \"\"\"Performs tokenization and simple preprocessing.\"\"\"\n",
    "    \n",
    "    replace_by_space_re = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "    good_symbols_re = re.compile('[^0-9a-z #+_]')\n",
    "\n",
    "    text = text.lower()\n",
    "    text = replace_by_space_re.sub(' ', text)\n",
    "    text = good_symbols_re.sub('', text)\n",
    "\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Seq2SeqModel(object):\n",
    "\n",
    "    def __init__(self, vocab_size, embeddings_size, hidden_size,\n",
    "                 max_iter, start_symbol_id, end_symbol_id, padding_symbol_id):\n",
    "\n",
    "\n",
    "        self.declare_placeholders()\n",
    "        self.create_embeddings(vocab_size, embeddings_size)\n",
    "        self.build_encoder(hidden_size)\n",
    "        self.build_decoder(hidden_size, vocab_size, max_iter, start_symbol_id, end_symbol_id)\n",
    "\n",
    "        # Compute loss and back-propagate.\n",
    "        self.compute_loss()\n",
    "        self.perform_optimization()\n",
    "\n",
    "        # Get predictions for evaluation.\n",
    "        self.train_predictions = self.train_outputs.sample_id\n",
    "        self.infer_predictions = self.infer_outputs.sample_id\n",
    "\n",
    "    def declare_placeholders(self):\n",
    "        \"\"\"Specifies placeholders for the model.\"\"\"\n",
    "\n",
    "        # Placeholders for input and its actual lengths.\n",
    "        self.input_batch = tf.placeholder(shape=(None, None), dtype=tf.int32, name='input_batch')\n",
    "        self.input_batch_lengths = tf.placeholder(shape=(None,), dtype=tf.int32, name='input_batch_lengths')\n",
    "\n",
    "        # Placeholders for groundtruth and its actual lengths.\n",
    "        self.ground_truth = tf.placeholder(shape=(None, None), dtype=tf.int32, name='ground_thruth')\n",
    "        self.ground_truth_lengths = tf.placeholder(shape=(None,), dtype=tf.int32, name='ground_thrugth_length')\n",
    "\n",
    "        self.dropout_ph = tf.placeholder_with_default(tf.cast(1.0, tf.float32), shape=[])\n",
    "        self.learning_rate_ph = tf.placeholder_with_default(tf.cast(0.001, tf.float32), shape=[])\n",
    "\n",
    "    def create_embeddings(self, vocab_size, embeddings_size):\n",
    "        \"\"\"Specifies embeddings layer and embeds an input batch.\"\"\"\n",
    "\n",
    "        random_initializer = tf.random_uniform((vocab_size, embeddings_size), -1.0, 1.0)\n",
    "        self.embeddings = tf.Variable(initial_value=random_initializer, dtype=tf.float32, name='embeding_matrix')\n",
    "\n",
    "        # Perform embeddings lookup for self.input_batch.\n",
    "        self.input_batch_embedded = tf.nn.embedding_lookup(self.embeddings, self.input_batch)\n",
    "\n",
    "    def build_encoder(self, hidden_size):\n",
    "        \"\"\"Specifies encoder architecture and computes its output.\"\"\"\n",
    "\n",
    "        # Create GRUCell with dropout.\n",
    "        encoder_cell = tf.contrib.rnn.GRUCell(num_units=hidden_size)\n",
    "        encoder_cell_dropout = tf.contrib.rnn.DropoutWrapper(encoder_cell, input_keep_prob=self.dropout_ph)\n",
    "\n",
    "        # Create RNN with the predefined cell.\n",
    "        _, self.final_encoder_state = tf.nn.dynamic_rnn(cell=encoder_cell_dropout,\n",
    "                                                        inputs=self.input_batch_embedded,\n",
    "                                                        sequence_length=self.input_batch_lengths,\n",
    "                                                        dtype=tf.float32)\n",
    "\n",
    "    def build_decoder(self, hidden_size, vocab_size, max_iter, start_symbol_id, end_symbol_id):\n",
    "        \"\"\"Specifies decoder architecture and computes the output.\n",
    "\n",
    "            Uses different helpers:\n",
    "              - for train: feeding ground truth\n",
    "              - for inference: feeding generated output\n",
    "\n",
    "            As a result, self.train_outputs and self.infer_outputs are created.\n",
    "            Each of them contains two fields:\n",
    "              rnn_output (predicted logits)\n",
    "              sample_id (predictions).\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # Use start symbols as the decoder inputs at the first time step.\n",
    "        batch_size = tf.shape(self.input_batch)[0]\n",
    "        start_tokens = tf.fill([batch_size], start_symbol_id)\n",
    "        ground_truth_as_input = tf.concat([tf.expand_dims(start_tokens, 1), self.ground_truth], 1)\n",
    "\n",
    "        # Use the embedding layer defined before to lookup embedings for ground_truth_as_input.\n",
    "        self.ground_truth_embedded = tf.nn.embedding_lookup(self.embeddings, ground_truth_as_input)\n",
    "\n",
    "        # Create TrainingHelper for the train stage.\n",
    "        train_helper = tf.contrib.seq2seq.TrainingHelper(self.ground_truth_embedded,\n",
    "                                                         self.ground_truth_lengths)\n",
    "\n",
    "        # Create GreedyEmbeddingHelper for the inference stage.\n",
    "        # You should provide the embedding layer, start_tokens and index of the end symbol.\n",
    "        infer_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(self.embeddings, start_tokens, end_symbol_id)\n",
    "\n",
    "        def decode(helper, scope, reuse=None):\n",
    "            \"\"\"Creates decoder and return the results of the decoding with a given helper.\"\"\"\n",
    "\n",
    "            with tf.variable_scope(scope, reuse=reuse):\n",
    "                # Create GRUCell with dropout. Do not forget to set the reuse flag properly.\n",
    "                decoder_cell = tf.contrib.rnn.GRUCell(num_units=hidden_size, reuse=reuse)\n",
    "                decoder_cell_dropout = tf.contrib.rnn.DropoutWrapper(decoder_cell, input_keep_prob=self.dropout_ph)\n",
    "\n",
    "                # Create a projection wrapper.\n",
    "                decoder_cell = tf.contrib.rnn.OutputProjectionWrapper(decoder_cell_dropout, vocab_size, reuse=reuse)\n",
    "\n",
    "                # Create BasicDecoder, pass the defined cell, a helper, and initial state.\n",
    "                # The initial state should be equal to the final state of the encoder!\n",
    "                decoder = tf.contrib.seq2seq.BasicDecoder(cell=decoder_cell, helper=helper,\n",
    "                                                          initial_state=self.final_encoder_state)\n",
    "\n",
    "                # The first returning argument of dynamic_decode contains two fields:\n",
    "                #   rnn_output (predicted logits)\n",
    "                #   sample_id (predictions)\n",
    "                outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(decoder=decoder, maximum_iterations=max_iter,\n",
    "                                                                  output_time_major=False, impute_finished=True)\n",
    "\n",
    "                return outputs\n",
    "\n",
    "        self.train_outputs = decode(train_helper, 'decode')\n",
    "        self.infer_outputs = decode(infer_helper, 'decode', reuse=True)\n",
    "\n",
    "    def compute_loss(self):\n",
    "        \"\"\"Computes sequence loss (masked cross-entopy loss with logits).\"\"\"\n",
    "\n",
    "        weights = tf.cast(tf.sequence_mask(self.ground_truth_lengths), dtype=tf.float32)\n",
    "\n",
    "        self.loss = tf.contrib.seq2seq.sequence_loss(\n",
    "            logits=self.train_outputs.rnn_output,\n",
    "            targets=self.ground_truth,\n",
    "            weights=weights)\n",
    "\n",
    "    def perform_optimization(self):\n",
    "        \"\"\"Specifies train_op that optimizes self.loss.\"\"\"\n",
    "\n",
    "        self.train_op = tf.contrib.layers.optimize_loss(\n",
    "            loss=self.loss,\n",
    "            global_step=tf.train.get_global_step(),\n",
    "            learning_rate=self.learning_rate_ph,\n",
    "            optimizer='Adam',\n",
    "            clip_gradients=1.0\n",
    "        )\n",
    "\n",
    "    def get_response(self, session, input_sentence):\n",
    "        sentence = text_prepare(input_sentence)\n",
    "        X = []\n",
    "        row = []\n",
    "        for word in sentence:\n",
    "            if word in word2id:\n",
    "                row.append(word2id[word])\n",
    "            else:\n",
    "                row.append(start_symbol_id)\n",
    "        X.append(row)\n",
    "        X = np.array(X)\n",
    "\n",
    "        feed_dict = {\n",
    "            self.input_batch: X,\n",
    "            self.input_batch_lengths: np.array([len(input_sentence)]),\n",
    "        }\n",
    "        predictions = session.run([self.infer_predictions], feed_dict=feed_dict)\n",
    "        return \" \".join([id2word[index] for index in predictions[0][0][:-1]])\n",
    "\n",
    "\n",
    "    def train_on_batch(self, session, X, X_seq_len, Y, Y_seq_len, learning_rate, dropout_keep_probability):\n",
    "        feed_dict = {\n",
    "            self.input_batch: X,\n",
    "            self.input_batch_lengths: X_seq_len,\n",
    "            self.ground_truth: Y,\n",
    "            self.ground_truth_lengths: Y_seq_len,\n",
    "            self.learning_rate_ph: learning_rate,\n",
    "            self.dropout_ph: dropout_keep_probability\n",
    "        }\n",
    "        pred, loss, _ = session.run([\n",
    "            self.train_predictions,\n",
    "            self.loss,\n",
    "            self.train_op], feed_dict=feed_dict)\n",
    "        return pred, loss\n",
    "\n",
    "    def predict_for_batch(self, session, X, X_seq_len):\n",
    "        feed_dict = {\n",
    "            self.input_batch: X,\n",
    "            self.input_batch_lengths: X_seq_len\n",
    "        }\n",
    "        pred = session.run([\n",
    "            self.infer_predictions\n",
    "        ], feed_dict=feed_dict)[0]\n",
    "        return pred\n",
    "\n",
    "    def predict_for_batch_with_loss(self, session, X, X_seq_len, Y, Y_seq_len):\n",
    "        feed_dict = {\n",
    "            self.input_batch: X,\n",
    "            self.input_batch_lengths: X_seq_len,\n",
    "            self.ground_truth: Y,\n",
    "            self.ground_truth_lengths: Y_seq_len\n",
    "        }\n",
    "        pred, loss = session.run([\n",
    "            self.infer_predictions,\n",
    "            self.loss,\n",
    "        ], feed_dict=feed_dict)\n",
    "        return pred, loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_set, test_set = train_test_split(data_clean, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(word2id, open( \"word2id.p\", \"wb\" ) )\n",
    "pickle.dump(id2word, open( \"id2word.p\", \"wb\" ) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Kiko-PC\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "session = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "\n",
    "\n",
    "model = Seq2SeqModel(vocab_size = len(word2id),\n",
    "                    embeddings_size = 300,\n",
    "                    hidden_size = 128,\n",
    "                    max_iter = 20,\n",
    "                    start_symbol_id=word2id['[^]'],\n",
    "                    end_symbol_id=word2id['[$]'],\n",
    "                    padding_symbol_id=word2id['[#]'])\n",
    "\n",
    "batch_size = 8\n",
    "n_epochs = 900\n",
    "learning_rate = 0.001\n",
    "dropout_keep_probability = 0.4\n",
    "max_len = 20\n",
    "\n",
    "n_step = int(len(train_set) / batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training... \n",
      "\n",
      "Train: epoch 1\n",
      "Epoch: [1/900], step: [1/2479], loss: 9.089571\n",
      "Epoch: [1/900], step: [201/2479], loss: 5.162564\n",
      "Epoch: [1/900], step: [401/2479], loss: 4.956809\n",
      "Epoch: [1/900], step: [601/2479], loss: 4.736184\n",
      "Epoch: [1/900], step: [801/2479], loss: 3.839516\n",
      "Epoch: [1/900], step: [1001/2479], loss: 4.797082\n",
      "Epoch: [1/900], step: [1201/2479], loss: 5.164343\n",
      "Epoch: [1/900], step: [1401/2479], loss: 4.954337\n",
      "Epoch: [1/900], step: [1601/2479], loss: 5.012486\n",
      "Epoch: [1/900], step: [1801/2479], loss: 4.893082\n",
      "Epoch: [1/900], step: [2001/2479], loss: 4.135650\n",
      "Epoch: [1/900], step: [2201/2479], loss: 4.223727\n",
      "Epoch: [1/900], step: [2401/2479], loss: 4.644779\n",
      "Test: epoch 1 loss: 5.068281\n",
      "Train: epoch 2\n",
      "Epoch: [2/900], step: [1/2479], loss: 4.712706\n",
      "Epoch: [2/900], step: [201/2479], loss: 4.148042\n",
      "Epoch: [2/900], step: [401/2479], loss: 3.550639\n",
      "Epoch: [2/900], step: [601/2479], loss: 4.268332\n",
      "Epoch: [2/900], step: [801/2479], loss: 4.344828\n",
      "Epoch: [2/900], step: [1001/2479], loss: 3.419678\n",
      "Epoch: [2/900], step: [1201/2479], loss: 4.292996\n",
      "Epoch: [2/900], step: [1401/2479], loss: 4.474560\n",
      "Epoch: [2/900], step: [1601/2479], loss: 3.759495\n",
      "Epoch: [2/900], step: [1801/2479], loss: 3.686760\n",
      "Epoch: [2/900], step: [2001/2479], loss: 4.670513\n",
      "Epoch: [2/900], step: [2201/2479], loss: 3.997840\n",
      "Epoch: [2/900], step: [2401/2479], loss: 3.898637\n",
      "Test: epoch 2 loss: 4.134375\n",
      "Train: epoch 3\n",
      "Epoch: [3/900], step: [1/2479], loss: 3.846487\n",
      "Epoch: [3/900], step: [201/2479], loss: 4.493883\n",
      "Epoch: [3/900], step: [401/2479], loss: 4.874606\n",
      "Epoch: [3/900], step: [601/2479], loss: 4.525443\n",
      "Epoch: [3/900], step: [801/2479], loss: 3.968007\n",
      "Epoch: [3/900], step: [1001/2479], loss: 4.232474\n",
      "Epoch: [3/900], step: [1201/2479], loss: 3.266680\n",
      "Epoch: [3/900], step: [1401/2479], loss: 4.359877\n",
      "Epoch: [3/900], step: [1601/2479], loss: 4.299685\n",
      "Epoch: [3/900], step: [1801/2479], loss: 3.972859\n",
      "Epoch: [3/900], step: [2001/2479], loss: 3.830865\n",
      "Epoch: [3/900], step: [2201/2479], loss: 5.083568\n",
      "Epoch: [3/900], step: [2401/2479], loss: 3.691370\n",
      "Test: epoch 3 loss: 5.0200095\n",
      "Train: epoch 4\n",
      "Epoch: [4/900], step: [1/2479], loss: 3.512683\n",
      "Epoch: [4/900], step: [201/2479], loss: 4.700377\n",
      "Epoch: [4/900], step: [401/2479], loss: 3.901200\n",
      "Epoch: [4/900], step: [601/2479], loss: 4.159946\n",
      "Epoch: [4/900], step: [801/2479], loss: 3.699928\n",
      "Epoch: [4/900], step: [1001/2479], loss: 3.431776\n",
      "Epoch: [4/900], step: [1201/2479], loss: 3.415840\n",
      "Epoch: [4/900], step: [1401/2479], loss: 4.006413\n",
      "Epoch: [4/900], step: [1601/2479], loss: 2.905669\n",
      "Epoch: [4/900], step: [1801/2479], loss: 4.339501\n",
      "Epoch: [4/900], step: [2001/2479], loss: 3.658136\n",
      "Epoch: [4/900], step: [2201/2479], loss: 4.318285\n",
      "Epoch: [4/900], step: [2401/2479], loss: 3.629748\n",
      "Test: epoch 4 loss: 4.4136066\n",
      "Train: epoch 5\n",
      "Epoch: [5/900], step: [1/2479], loss: 3.451169\n",
      "Epoch: [5/900], step: [201/2479], loss: 2.838926\n",
      "Epoch: [5/900], step: [401/2479], loss: 3.474917\n",
      "Epoch: [5/900], step: [601/2479], loss: 3.969579\n",
      "Epoch: [5/900], step: [801/2479], loss: 3.805155\n",
      "Epoch: [5/900], step: [1001/2479], loss: 3.752315\n",
      "Epoch: [5/900], step: [1201/2479], loss: 4.072240\n",
      "Epoch: [5/900], step: [1401/2479], loss: 3.751053\n",
      "Epoch: [5/900], step: [1601/2479], loss: 2.767656\n",
      "Epoch: [5/900], step: [1801/2479], loss: 4.716569\n",
      "Epoch: [5/900], step: [2001/2479], loss: 4.104830\n",
      "Epoch: [5/900], step: [2201/2479], loss: 4.048040\n",
      "Epoch: [5/900], step: [2401/2479], loss: 3.792219\n",
      "Test: epoch 5 loss: 5.8797073\n",
      "Train: epoch 6\n",
      "Epoch: [6/900], step: [1/2479], loss: 3.208487\n",
      "Epoch: [6/900], step: [201/2479], loss: 3.643999\n",
      "Epoch: [6/900], step: [401/2479], loss: 3.768408\n",
      "Epoch: [6/900], step: [601/2479], loss: 3.375479\n",
      "Epoch: [6/900], step: [801/2479], loss: 3.794970\n",
      "Epoch: [6/900], step: [1001/2479], loss: 2.549913\n",
      "Epoch: [6/900], step: [1201/2479], loss: 4.042628\n",
      "Epoch: [6/900], step: [1401/2479], loss: 3.249924\n",
      "Epoch: [6/900], step: [1601/2479], loss: 3.934448\n",
      "Epoch: [6/900], step: [1801/2479], loss: 2.945546\n",
      "Epoch: [6/900], step: [2001/2479], loss: 4.285038\n",
      "Epoch: [6/900], step: [2201/2479], loss: 3.269445\n",
      "Epoch: [6/900], step: [2401/2479], loss: 3.222528\n",
      "Test: epoch 6 loss: 6.441992\n",
      "Train: epoch 7\n",
      "Epoch: [7/900], step: [1/2479], loss: 2.962357\n",
      "Epoch: [7/900], step: [201/2479], loss: 3.160345\n",
      "Epoch: [7/900], step: [401/2479], loss: 3.376291\n",
      "Epoch: [7/900], step: [601/2479], loss: 3.618333\n",
      "Epoch: [7/900], step: [801/2479], loss: 2.901421\n",
      "Epoch: [7/900], step: [1001/2479], loss: 3.666006\n",
      "Epoch: [7/900], step: [1201/2479], loss: 3.884686\n",
      "Epoch: [7/900], step: [1401/2479], loss: 3.500212\n",
      "Epoch: [7/900], step: [1601/2479], loss: 3.808565\n",
      "Epoch: [7/900], step: [1801/2479], loss: 3.325385\n",
      "Epoch: [7/900], step: [2001/2479], loss: 3.324807\n",
      "Epoch: [7/900], step: [2201/2479], loss: 3.384318\n",
      "Epoch: [7/900], step: [2401/2479], loss: 3.336470\n",
      "Test: epoch 7 loss: 4.262443\n",
      "Train: epoch 8\n",
      "Epoch: [8/900], step: [1/2479], loss: 3.594547\n",
      "Epoch: [8/900], step: [201/2479], loss: 3.128002\n",
      "Epoch: [8/900], step: [401/2479], loss: 3.172592\n",
      "Epoch: [8/900], step: [601/2479], loss: 3.791451\n",
      "Epoch: [8/900], step: [801/2479], loss: 2.934617\n",
      "Epoch: [8/900], step: [1001/2479], loss: 3.613508\n",
      "Epoch: [8/900], step: [1201/2479], loss: 3.156611\n",
      "Epoch: [8/900], step: [1401/2479], loss: 3.651366\n",
      "Epoch: [8/900], step: [1601/2479], loss: 2.953481\n",
      "Epoch: [8/900], step: [1801/2479], loss: 3.029768\n",
      "Epoch: [8/900], step: [2001/2479], loss: 3.207941\n",
      "Epoch: [8/900], step: [2201/2479], loss: 3.465447\n",
      "Epoch: [8/900], step: [2401/2479], loss: 3.518657\n",
      "Test: epoch 8 loss: 3.6579113\n",
      "Train: epoch 9\n",
      "Epoch: [9/900], step: [1/2479], loss: 3.094900\n",
      "Epoch: [9/900], step: [201/2479], loss: 3.339401\n",
      "Epoch: [9/900], step: [401/2479], loss: 3.563821\n",
      "Epoch: [9/900], step: [601/2479], loss: 3.677076\n",
      "Epoch: [9/900], step: [801/2479], loss: 3.790687\n",
      "Epoch: [9/900], step: [1001/2479], loss: 3.412495\n",
      "Epoch: [9/900], step: [1201/2479], loss: 3.762015\n",
      "Epoch: [9/900], step: [1401/2479], loss: 2.730577\n",
      "Epoch: [9/900], step: [1601/2479], loss: 2.999068\n",
      "Epoch: [9/900], step: [1801/2479], loss: 3.469777\n",
      "Epoch: [9/900], step: [2001/2479], loss: 3.215914\n",
      "Epoch: [9/900], step: [2201/2479], loss: 3.947694\n",
      "Epoch: [9/900], step: [2401/2479], loss: 2.891361\n",
      "Test: epoch 9 loss: 6.0624127\n",
      "Train: epoch 10\n",
      "Epoch: [10/900], step: [1/2479], loss: 2.945077\n",
      "Epoch: [10/900], step: [201/2479], loss: 3.245709\n",
      "Epoch: [10/900], step: [401/2479], loss: 2.488335\n",
      "Epoch: [10/900], step: [601/2479], loss: 3.429522\n",
      "Epoch: [10/900], step: [801/2479], loss: 3.510513\n",
      "Epoch: [10/900], step: [1001/2479], loss: 3.303874\n",
      "Epoch: [10/900], step: [1201/2479], loss: 3.649708\n",
      "Epoch: [10/900], step: [1401/2479], loss: 3.140384\n",
      "Epoch: [10/900], step: [1601/2479], loss: 2.916121\n",
      "Epoch: [10/900], step: [1801/2479], loss: 3.438856\n",
      "Epoch: [10/900], step: [2001/2479], loss: 3.313589\n",
      "Epoch: [10/900], step: [2201/2479], loss: 3.267947\n",
      "Epoch: [10/900], step: [2401/2479], loss: 3.073239\n",
      "Test: epoch 10 loss: 4.874749\n",
      "Train: epoch 11\n",
      "Epoch: [11/900], step: [1/2479], loss: 3.183143\n",
      "Epoch: [11/900], step: [201/2479], loss: 2.624439\n",
      "Epoch: [11/900], step: [401/2479], loss: 3.334702\n",
      "Epoch: [11/900], step: [601/2479], loss: 3.522865\n",
      "Epoch: [11/900], step: [801/2479], loss: 3.360543\n",
      "Epoch: [11/900], step: [1001/2479], loss: 2.790900\n",
      "Epoch: [11/900], step: [1201/2479], loss: 2.569406\n",
      "Epoch: [11/900], step: [1401/2479], loss: 3.056393\n",
      "Epoch: [11/900], step: [1601/2479], loss: 3.113167\n",
      "Epoch: [11/900], step: [1801/2479], loss: 3.003249\n",
      "Epoch: [11/900], step: [2001/2479], loss: 2.821766\n",
      "Epoch: [11/900], step: [2201/2479], loss: 3.636623\n",
      "Epoch: [11/900], step: [2401/2479], loss: 3.114661\n",
      "Test: epoch 11 loss: 4.6505337\n",
      "Train: epoch 12\n",
      "Epoch: [12/900], step: [1/2479], loss: 2.877100\n",
      "Epoch: [12/900], step: [201/2479], loss: 2.649037\n",
      "Epoch: [12/900], step: [401/2479], loss: 2.759874\n",
      "Epoch: [12/900], step: [601/2479], loss: 2.964546\n",
      "Epoch: [12/900], step: [801/2479], loss: 2.489976\n",
      "Epoch: [12/900], step: [1001/2479], loss: 2.827140\n",
      "Epoch: [12/900], step: [1201/2479], loss: 2.738072\n",
      "Epoch: [12/900], step: [1401/2479], loss: 2.676208\n",
      "Epoch: [12/900], step: [1601/2479], loss: 3.562887\n",
      "Epoch: [12/900], step: [1801/2479], loss: 2.969109\n",
      "Epoch: [12/900], step: [2001/2479], loss: 3.006344\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [12/900], step: [2201/2479], loss: 2.970464\n",
      "Epoch: [12/900], step: [2401/2479], loss: 2.892827\n",
      "Test: epoch 12 loss: 4.604541\n",
      "Train: epoch 13\n",
      "Epoch: [13/900], step: [1/2479], loss: 2.784136\n",
      "Epoch: [13/900], step: [201/2479], loss: 2.925337\n",
      "Epoch: [13/900], step: [401/2479], loss: 3.364030\n",
      "Epoch: [13/900], step: [601/2479], loss: 3.238554\n",
      "Epoch: [13/900], step: [801/2479], loss: 3.117167\n",
      "Epoch: [13/900], step: [1001/2479], loss: 3.014548\n",
      "Epoch: [13/900], step: [1201/2479], loss: 2.995309\n",
      "Epoch: [13/900], step: [1401/2479], loss: 2.906691\n",
      "Epoch: [13/900], step: [1601/2479], loss: 3.133435\n",
      "Epoch: [13/900], step: [1801/2479], loss: 2.423719\n",
      "Epoch: [13/900], step: [2001/2479], loss: 3.782163\n",
      "Epoch: [13/900], step: [2201/2479], loss: 3.088079\n",
      "Epoch: [13/900], step: [2401/2479], loss: 3.938178\n",
      "Test: epoch 13 loss: 3.7842705\n",
      "Train: epoch 14\n",
      "Epoch: [14/900], step: [1/2479], loss: 2.144584\n",
      "Epoch: [14/900], step: [201/2479], loss: 2.947818\n",
      "Epoch: [14/900], step: [401/2479], loss: 2.098896\n",
      "Epoch: [14/900], step: [601/2479], loss: 3.047337\n",
      "Epoch: [14/900], step: [801/2479], loss: 3.277899\n",
      "Epoch: [14/900], step: [1001/2479], loss: 2.959649\n",
      "Epoch: [14/900], step: [1201/2479], loss: 2.675199\n",
      "Epoch: [14/900], step: [1401/2479], loss: 2.864324\n",
      "Epoch: [14/900], step: [1601/2479], loss: 2.945752\n",
      "Epoch: [14/900], step: [1801/2479], loss: 3.594752\n",
      "Epoch: [14/900], step: [2001/2479], loss: 2.819438\n",
      "Epoch: [14/900], step: [2201/2479], loss: 3.403425\n",
      "Epoch: [14/900], step: [2401/2479], loss: 3.288868\n",
      "Test: epoch 14 loss: 6.8452516\n",
      "Train: epoch 15\n",
      "Epoch: [15/900], step: [1/2479], loss: 2.599871\n",
      "Epoch: [15/900], step: [201/2479], loss: 2.970701\n",
      "Epoch: [15/900], step: [401/2479], loss: 2.953353\n",
      "Epoch: [15/900], step: [601/2479], loss: 3.586308\n",
      "Epoch: [15/900], step: [801/2479], loss: 3.249993\n",
      "Epoch: [15/900], step: [1001/2479], loss: 3.117764\n",
      "Epoch: [15/900], step: [1201/2479], loss: 3.090640\n",
      "Epoch: [15/900], step: [1401/2479], loss: 2.986595\n",
      "Epoch: [15/900], step: [1601/2479], loss: 3.048785\n",
      "Epoch: [15/900], step: [1801/2479], loss: 3.228193\n",
      "Epoch: [15/900], step: [2001/2479], loss: 3.218126\n",
      "Epoch: [15/900], step: [2201/2479], loss: 3.295137\n",
      "Epoch: [15/900], step: [2401/2479], loss: 2.845775\n",
      "Test: epoch 15 loss: 5.5219193\n",
      "Train: epoch 16\n",
      "Epoch: [16/900], step: [1/2479], loss: 2.852473\n",
      "Epoch: [16/900], step: [201/2479], loss: 2.410073\n",
      "Epoch: [16/900], step: [401/2479], loss: 2.991690\n",
      "Epoch: [16/900], step: [601/2479], loss: 2.476247\n",
      "Epoch: [16/900], step: [801/2479], loss: 2.961711\n",
      "Epoch: [16/900], step: [1001/2479], loss: 2.307508\n",
      "Epoch: [16/900], step: [1201/2479], loss: 2.914051\n",
      "Epoch: [16/900], step: [1401/2479], loss: 2.775938\n",
      "Epoch: [16/900], step: [1601/2479], loss: 2.806489\n",
      "Epoch: [16/900], step: [1801/2479], loss: 2.740766\n",
      "Epoch: [16/900], step: [2001/2479], loss: 2.825302\n",
      "Epoch: [16/900], step: [2201/2479], loss: 3.190458\n",
      "Epoch: [16/900], step: [2401/2479], loss: 2.885354\n",
      "Test: epoch 16 loss: 4.8243012\n",
      "Train: epoch 17\n",
      "Epoch: [17/900], step: [1/2479], loss: 2.975029\n",
      "Epoch: [17/900], step: [201/2479], loss: 2.734661\n",
      "Epoch: [17/900], step: [401/2479], loss: 3.343088\n",
      "Epoch: [17/900], step: [601/2479], loss: 2.608807\n",
      "Epoch: [17/900], step: [801/2479], loss: 3.034295\n",
      "Epoch: [17/900], step: [1001/2479], loss: 2.480875\n",
      "Epoch: [17/900], step: [1201/2479], loss: 2.818922\n",
      "Epoch: [17/900], step: [1401/2479], loss: 3.401804\n",
      "Epoch: [17/900], step: [1601/2479], loss: 2.981006\n",
      "Epoch: [17/900], step: [1801/2479], loss: 2.534411\n",
      "Epoch: [17/900], step: [2001/2479], loss: 2.790534\n",
      "Epoch: [17/900], step: [2201/2479], loss: 3.199969\n",
      "Epoch: [17/900], step: [2401/2479], loss: 2.710445\n",
      "Test: epoch 17 loss: 5.1243405\n",
      "Train: epoch 18\n",
      "Epoch: [18/900], step: [1/2479], loss: 2.462587\n",
      "Epoch: [18/900], step: [201/2479], loss: 2.483930\n",
      "Epoch: [18/900], step: [401/2479], loss: 2.848015\n",
      "Epoch: [18/900], step: [601/2479], loss: 2.874693\n",
      "Epoch: [18/900], step: [801/2479], loss: 2.125328\n",
      "Epoch: [18/900], step: [1001/2479], loss: 2.360451\n",
      "Epoch: [18/900], step: [1201/2479], loss: 2.605781\n",
      "Epoch: [18/900], step: [1401/2479], loss: 2.619478\n",
      "Epoch: [18/900], step: [1601/2479], loss: 2.845835\n",
      "Epoch: [18/900], step: [1801/2479], loss: 2.799458\n",
      "Epoch: [18/900], step: [2001/2479], loss: 3.026456\n",
      "Epoch: [18/900], step: [2201/2479], loss: 2.476830\n",
      "Epoch: [18/900], step: [2401/2479], loss: 2.818470\n",
      "Test: epoch 18 loss: 5.8382673\n",
      "Train: epoch 19\n",
      "Epoch: [19/900], step: [1/2479], loss: 2.282042\n",
      "Epoch: [19/900], step: [201/2479], loss: 1.991321\n",
      "Epoch: [19/900], step: [401/2479], loss: 2.999259\n",
      "Epoch: [19/900], step: [601/2479], loss: 2.254008\n",
      "Epoch: [19/900], step: [801/2479], loss: 2.349867\n",
      "Epoch: [19/900], step: [1001/2479], loss: 3.101201\n",
      "Epoch: [19/900], step: [1201/2479], loss: 2.589631\n",
      "Epoch: [19/900], step: [1401/2479], loss: 2.428068\n",
      "Epoch: [19/900], step: [1601/2479], loss: 2.329130\n",
      "Epoch: [19/900], step: [1801/2479], loss: 2.263986\n",
      "Epoch: [19/900], step: [2001/2479], loss: 2.119103\n",
      "Epoch: [19/900], step: [2201/2479], loss: 2.935284\n",
      "Epoch: [19/900], step: [2401/2479], loss: 2.517622\n",
      "Test: epoch 19 loss: 4.6494765\n",
      "Train: epoch 20\n",
      "Epoch: [20/900], step: [1/2479], loss: 1.975285\n",
      "Epoch: [20/900], step: [201/2479], loss: 2.609156\n",
      "Epoch: [20/900], step: [401/2479], loss: 2.100026\n",
      "Epoch: [20/900], step: [601/2479], loss: 2.996608\n",
      "Epoch: [20/900], step: [801/2479], loss: 2.622830\n",
      "Epoch: [20/900], step: [1001/2479], loss: 2.657837\n",
      "Epoch: [20/900], step: [1201/2479], loss: 2.763883\n",
      "Epoch: [20/900], step: [1401/2479], loss: 2.619965\n",
      "Epoch: [20/900], step: [1601/2479], loss: 2.495879\n",
      "Epoch: [20/900], step: [1801/2479], loss: 2.485982\n",
      "Epoch: [20/900], step: [2001/2479], loss: 2.813466\n",
      "Epoch: [20/900], step: [2201/2479], loss: 2.737896\n",
      "Epoch: [20/900], step: [2401/2479], loss: 2.863173\n",
      "Test: epoch 20 loss: 5.9053955\n",
      "Train: epoch 21\n",
      "Epoch: [21/900], step: [1/2479], loss: 2.373083\n",
      "Epoch: [21/900], step: [201/2479], loss: 2.501332\n",
      "Epoch: [21/900], step: [401/2479], loss: 2.479334\n",
      "Epoch: [21/900], step: [601/2479], loss: 2.338695\n",
      "Epoch: [21/900], step: [801/2479], loss: 2.787452\n",
      "Epoch: [21/900], step: [1001/2479], loss: 2.412168\n",
      "Epoch: [21/900], step: [1201/2479], loss: 2.834311\n",
      "Epoch: [21/900], step: [1401/2479], loss: 2.465929\n",
      "Epoch: [21/900], step: [1601/2479], loss: 3.009567\n",
      "Epoch: [21/900], step: [1801/2479], loss: 2.591664\n",
      "Epoch: [21/900], step: [2001/2479], loss: 2.701160\n",
      "Epoch: [21/900], step: [2201/2479], loss: 3.240168\n",
      "Epoch: [21/900], step: [2401/2479], loss: 2.818471\n",
      "Test: epoch 21 loss: 7.370583\n",
      "Train: epoch 22\n",
      "Epoch: [22/900], step: [1/2479], loss: 2.428764\n",
      "Epoch: [22/900], step: [201/2479], loss: 2.212727\n",
      "Epoch: [22/900], step: [401/2479], loss: 2.198794\n",
      "Epoch: [22/900], step: [601/2479], loss: 2.062515\n",
      "Epoch: [22/900], step: [801/2479], loss: 2.703700\n",
      "Epoch: [22/900], step: [1001/2479], loss: 2.710909\n",
      "Epoch: [22/900], step: [1201/2479], loss: 2.418854\n",
      "Epoch: [22/900], step: [1401/2479], loss: 2.814499\n",
      "Epoch: [22/900], step: [1601/2479], loss: 2.551584\n",
      "Epoch: [22/900], step: [1801/2479], loss: 2.564621\n",
      "Epoch: [22/900], step: [2001/2479], loss: 3.081874\n",
      "Epoch: [22/900], step: [2201/2479], loss: 3.099666\n",
      "Epoch: [22/900], step: [2401/2479], loss: 3.450607\n",
      "Test: epoch 22 loss: 5.492884\n",
      "Train: epoch 23\n",
      "Epoch: [23/900], step: [1/2479], loss: 1.661921\n",
      "Epoch: [23/900], step: [201/2479], loss: 2.731788\n",
      "Epoch: [23/900], step: [401/2479], loss: 2.697299\n",
      "Epoch: [23/900], step: [601/2479], loss: 2.584391\n",
      "Epoch: [23/900], step: [801/2479], loss: 1.955564\n",
      "Epoch: [23/900], step: [1001/2479], loss: 2.499813\n",
      "Epoch: [23/900], step: [1201/2479], loss: 2.744044\n",
      "Epoch: [23/900], step: [1401/2479], loss: 3.022027\n",
      "Epoch: [23/900], step: [1601/2479], loss: 2.640288\n",
      "Epoch: [23/900], step: [1801/2479], loss: 3.047519\n",
      "Epoch: [23/900], step: [2001/2479], loss: 2.085150\n",
      "Epoch: [23/900], step: [2201/2479], loss: 2.261114\n",
      "Epoch: [23/900], step: [2401/2479], loss: 2.603447\n",
      "Test: epoch 23 loss: 6.229462\n",
      "Train: epoch 24\n",
      "Epoch: [24/900], step: [1/2479], loss: 2.483055\n",
      "Epoch: [24/900], step: [201/2479], loss: 3.024471\n",
      "Epoch: [24/900], step: [401/2479], loss: 2.118479\n",
      "Epoch: [24/900], step: [601/2479], loss: 2.518165\n",
      "Epoch: [24/900], step: [801/2479], loss: 2.861293\n",
      "Epoch: [24/900], step: [1001/2479], loss: 2.094202\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [24/900], step: [1201/2479], loss: 2.784442\n",
      "Epoch: [24/900], step: [1401/2479], loss: 2.559507\n",
      "Epoch: [24/900], step: [1601/2479], loss: 2.117641\n",
      "Epoch: [24/900], step: [1801/2479], loss: 2.651484\n",
      "Epoch: [24/900], step: [2001/2479], loss: 3.064284\n",
      "Epoch: [24/900], step: [2201/2479], loss: 2.617297\n",
      "Epoch: [24/900], step: [2401/2479], loss: 2.187746\n",
      "Test: epoch 24 loss: 5.5017786\n",
      "Train: epoch 25\n",
      "Epoch: [25/900], step: [1/2479], loss: 2.406727\n",
      "Epoch: [25/900], step: [201/2479], loss: 2.497492\n",
      "Epoch: [25/900], step: [401/2479], loss: 2.645295\n",
      "Epoch: [25/900], step: [601/2479], loss: 2.898926\n",
      "Epoch: [25/900], step: [801/2479], loss: 2.396592\n",
      "Epoch: [25/900], step: [1001/2479], loss: 2.448204\n",
      "Epoch: [25/900], step: [1201/2479], loss: 2.913520\n",
      "Epoch: [25/900], step: [1401/2479], loss: 2.932544\n",
      "Epoch: [25/900], step: [1601/2479], loss: 2.948591\n",
      "Epoch: [25/900], step: [1801/2479], loss: 2.311488\n",
      "Epoch: [25/900], step: [2001/2479], loss: 3.143854\n",
      "Epoch: [25/900], step: [2201/2479], loss: 1.825507\n",
      "Epoch: [25/900], step: [2401/2479], loss: 2.536961\n",
      "Test: epoch 25 loss: 4.475236\n",
      "Train: epoch 26\n",
      "Epoch: [26/900], step: [1/2479], loss: 2.297470\n",
      "Epoch: [26/900], step: [201/2479], loss: 2.736886\n",
      "Epoch: [26/900], step: [401/2479], loss: 2.513539\n",
      "Epoch: [26/900], step: [601/2479], loss: 2.576716\n",
      "Epoch: [26/900], step: [801/2479], loss: 2.460578\n",
      "Epoch: [26/900], step: [1001/2479], loss: 2.041318\n",
      "Epoch: [26/900], step: [1201/2479], loss: 2.397172\n",
      "Epoch: [26/900], step: [1401/2479], loss: 2.656234\n",
      "Epoch: [26/900], step: [1601/2479], loss: 2.096840\n",
      "Epoch: [26/900], step: [1801/2479], loss: 2.172139\n",
      "Epoch: [26/900], step: [2001/2479], loss: 2.727162\n",
      "Epoch: [26/900], step: [2201/2479], loss: 2.517294\n",
      "Epoch: [26/900], step: [2401/2479], loss: 2.220299\n",
      "Test: epoch 26 loss: 4.7617264\n",
      "Train: epoch 27\n",
      "Epoch: [27/900], step: [1/2479], loss: 2.289293\n",
      "Epoch: [27/900], step: [201/2479], loss: 2.134721\n",
      "Epoch: [27/900], step: [401/2479], loss: 1.885255\n",
      "Epoch: [27/900], step: [601/2479], loss: 2.471672\n",
      "Epoch: [27/900], step: [801/2479], loss: 2.664399\n",
      "Epoch: [27/900], step: [1001/2479], loss: 2.418437\n",
      "Epoch: [27/900], step: [1201/2479], loss: 2.172306\n",
      "Epoch: [27/900], step: [1401/2479], loss: 3.029651\n",
      "Epoch: [27/900], step: [1601/2479], loss: 2.102084\n",
      "Epoch: [27/900], step: [1801/2479], loss: 2.141487\n",
      "Epoch: [27/900], step: [2001/2479], loss: 2.391926\n",
      "Epoch: [27/900], step: [2201/2479], loss: 2.245326\n",
      "Epoch: [27/900], step: [2401/2479], loss: 1.675434\n",
      "Test: epoch 27 loss: 6.076639\n",
      "Train: epoch 28\n",
      "Epoch: [28/900], step: [1/2479], loss: 2.343265\n",
      "Epoch: [28/900], step: [201/2479], loss: 2.050183\n",
      "Epoch: [28/900], step: [401/2479], loss: 2.497609\n",
      "Epoch: [28/900], step: [601/2479], loss: 2.462448\n",
      "Epoch: [28/900], step: [801/2479], loss: 3.145512\n",
      "Epoch: [28/900], step: [1001/2479], loss: 2.534391\n",
      "Epoch: [28/900], step: [1201/2479], loss: 2.177701\n",
      "Epoch: [28/900], step: [1401/2479], loss: 2.487069\n",
      "Epoch: [28/900], step: [1601/2479], loss: 2.674659\n",
      "Epoch: [28/900], step: [1801/2479], loss: 2.404027\n",
      "Epoch: [28/900], step: [2001/2479], loss: 2.225343\n",
      "Epoch: [28/900], step: [2201/2479], loss: 2.282024\n",
      "Epoch: [28/900], step: [2401/2479], loss: 2.242647\n",
      "Test: epoch 28 loss: 5.5055776\n",
      "Train: epoch 29\n",
      "Epoch: [29/900], step: [1/2479], loss: 2.436030\n",
      "Epoch: [29/900], step: [201/2479], loss: 2.274523\n",
      "Epoch: [29/900], step: [401/2479], loss: 1.717721\n",
      "Epoch: [29/900], step: [601/2479], loss: 2.139240\n",
      "Epoch: [29/900], step: [801/2479], loss: 2.889445\n",
      "Epoch: [29/900], step: [1001/2479], loss: 2.677533\n",
      "Epoch: [29/900], step: [1201/2479], loss: 2.222348\n",
      "Epoch: [29/900], step: [1401/2479], loss: 2.474854\n",
      "Epoch: [29/900], step: [1601/2479], loss: 2.327203\n",
      "Epoch: [29/900], step: [1801/2479], loss: 2.634542\n",
      "Epoch: [29/900], step: [2001/2479], loss: 1.998793\n",
      "Epoch: [29/900], step: [2201/2479], loss: 2.486891\n",
      "Epoch: [29/900], step: [2401/2479], loss: 1.620550\n",
      "Test: epoch 29 loss: 2.8690042\n",
      "Train: epoch 30\n",
      "Epoch: [30/900], step: [1/2479], loss: 2.483203\n",
      "Epoch: [30/900], step: [201/2479], loss: 1.863319\n",
      "Epoch: [30/900], step: [401/2479], loss: 2.664189\n",
      "Epoch: [30/900], step: [601/2479], loss: 3.143860\n",
      "Epoch: [30/900], step: [801/2479], loss: 1.961828\n",
      "Epoch: [30/900], step: [1001/2479], loss: 1.635978\n",
      "Epoch: [30/900], step: [1201/2479], loss: 2.466791\n",
      "Epoch: [30/900], step: [1401/2479], loss: 2.888401\n",
      "Epoch: [30/900], step: [1601/2479], loss: 2.649991\n",
      "Epoch: [30/900], step: [1801/2479], loss: 2.494638\n",
      "Epoch: [30/900], step: [2001/2479], loss: 2.343649\n",
      "Epoch: [30/900], step: [2201/2479], loss: 2.861048\n",
      "Epoch: [30/900], step: [2401/2479], loss: 2.612453\n",
      "Test: epoch 30 loss: 6.928223\n",
      "Train: epoch 31\n",
      "Epoch: [31/900], step: [1/2479], loss: 2.243753\n",
      "Epoch: [31/900], step: [201/2479], loss: 2.632366\n",
      "Epoch: [31/900], step: [401/2479], loss: 1.776022\n",
      "Epoch: [31/900], step: [601/2479], loss: 2.321227\n",
      "Epoch: [31/900], step: [801/2479], loss: 2.522707\n",
      "Epoch: [31/900], step: [1001/2479], loss: 1.928173\n",
      "Epoch: [31/900], step: [1201/2479], loss: 2.157262\n",
      "Epoch: [31/900], step: [1401/2479], loss: 2.581688\n",
      "Epoch: [31/900], step: [1601/2479], loss: 2.291061\n",
      "Epoch: [31/900], step: [1801/2479], loss: 2.220722\n",
      "Epoch: [31/900], step: [2001/2479], loss: 2.216736\n",
      "Epoch: [31/900], step: [2201/2479], loss: 1.917742\n",
      "Epoch: [31/900], step: [2401/2479], loss: 1.968697\n",
      "Test: epoch 31 loss: 4.3710966\n",
      "Train: epoch 32\n",
      "Epoch: [32/900], step: [1/2479], loss: 2.357795\n",
      "Epoch: [32/900], step: [201/2479], loss: 2.429770\n",
      "Epoch: [32/900], step: [401/2479], loss: 2.530870\n",
      "Epoch: [32/900], step: [601/2479], loss: 2.046752\n",
      "Epoch: [32/900], step: [801/2479], loss: 2.761303\n",
      "Epoch: [32/900], step: [1001/2479], loss: 2.142022\n",
      "Epoch: [32/900], step: [1201/2479], loss: 3.289123\n",
      "Epoch: [32/900], step: [1401/2479], loss: 2.378905\n",
      "Epoch: [32/900], step: [1601/2479], loss: 2.345815\n",
      "Epoch: [32/900], step: [1801/2479], loss: 2.261456\n",
      "Epoch: [32/900], step: [2001/2479], loss: 2.552328\n",
      "Epoch: [32/900], step: [2201/2479], loss: 2.451111\n",
      "Epoch: [32/900], step: [2401/2479], loss: 2.250085\n",
      "Test: epoch 32 loss: 5.376726\n",
      "Train: epoch 33\n",
      "Epoch: [33/900], step: [1/2479], loss: 1.823958\n",
      "Epoch: [33/900], step: [201/2479], loss: 1.979696\n",
      "Epoch: [33/900], step: [401/2479], loss: 1.908346\n",
      "Epoch: [33/900], step: [601/2479], loss: 2.450917\n",
      "Epoch: [33/900], step: [801/2479], loss: 1.899540\n",
      "Epoch: [33/900], step: [1001/2479], loss: 2.029199\n",
      "Epoch: [33/900], step: [1201/2479], loss: 1.761279\n",
      "Epoch: [33/900], step: [1401/2479], loss: 2.134023\n",
      "Epoch: [33/900], step: [1601/2479], loss: 2.830422\n",
      "Epoch: [33/900], step: [1801/2479], loss: 2.239606\n",
      "Epoch: [33/900], step: [2001/2479], loss: 2.184347\n",
      "Epoch: [33/900], step: [2201/2479], loss: 2.661811\n",
      "Epoch: [33/900], step: [2401/2479], loss: 2.075987\n",
      "Test: epoch 33 loss: 4.7921023\n",
      "Train: epoch 34\n",
      "Epoch: [34/900], step: [1/2479], loss: 2.708323\n",
      "Epoch: [34/900], step: [201/2479], loss: 2.233890\n",
      "Epoch: [34/900], step: [401/2479], loss: 1.490950\n",
      "Epoch: [34/900], step: [601/2479], loss: 2.179369\n",
      "Epoch: [34/900], step: [801/2479], loss: 1.978073\n",
      "Epoch: [34/900], step: [1001/2479], loss: 2.419845\n",
      "Epoch: [34/900], step: [1201/2479], loss: 2.136385\n",
      "Epoch: [34/900], step: [1401/2479], loss: 2.865249\n",
      "Epoch: [34/900], step: [1601/2479], loss: 2.518326\n",
      "Epoch: [34/900], step: [1801/2479], loss: 2.358081\n",
      "Epoch: [34/900], step: [2001/2479], loss: 2.715299\n",
      "Epoch: [34/900], step: [2201/2479], loss: 2.617696\n",
      "Epoch: [34/900], step: [2401/2479], loss: 2.263425\n",
      "Test: epoch 34 loss: 4.258048\n",
      "Train: epoch 35\n",
      "Epoch: [35/900], step: [1/2479], loss: 2.079402\n",
      "Epoch: [35/900], step: [201/2479], loss: 2.545839\n",
      "Epoch: [35/900], step: [401/2479], loss: 2.398756\n",
      "Epoch: [35/900], step: [601/2479], loss: 2.846501\n",
      "Epoch: [35/900], step: [801/2479], loss: 2.259125\n",
      "Epoch: [35/900], step: [1001/2479], loss: 2.401208\n",
      "Epoch: [35/900], step: [1201/2479], loss: 2.076961\n",
      "Epoch: [35/900], step: [1401/2479], loss: 2.015044\n",
      "Epoch: [35/900], step: [1601/2479], loss: 2.902360\n",
      "Epoch: [35/900], step: [1801/2479], loss: 2.171526\n",
      "Epoch: [35/900], step: [2001/2479], loss: 2.641676\n",
      "Epoch: [35/900], step: [2201/2479], loss: 1.648234\n",
      "Epoch: [35/900], step: [2401/2479], loss: 2.175859\n",
      "Test: epoch 35 loss: 5.6128254\n",
      "Train: epoch 36\n",
      "Epoch: [36/900], step: [1/2479], loss: 1.974747\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [36/900], step: [201/2479], loss: 2.361049\n",
      "Epoch: [36/900], step: [401/2479], loss: 2.513638\n",
      "Epoch: [36/900], step: [601/2479], loss: 2.408422\n",
      "Epoch: [36/900], step: [801/2479], loss: 2.667599\n",
      "Epoch: [36/900], step: [1001/2479], loss: 2.775263\n",
      "Epoch: [36/900], step: [1201/2479], loss: 1.715851\n",
      "Epoch: [36/900], step: [1401/2479], loss: 2.450883\n",
      "Epoch: [36/900], step: [1601/2479], loss: 2.009586\n",
      "Epoch: [36/900], step: [1801/2479], loss: 2.314884\n",
      "Epoch: [36/900], step: [2001/2479], loss: 1.935795\n",
      "Epoch: [36/900], step: [2201/2479], loss: 2.645828\n",
      "Epoch: [36/900], step: [2401/2479], loss: 2.277745\n",
      "Test: epoch 36 loss: 8.572964\n",
      "Train: epoch 37\n",
      "Epoch: [37/900], step: [1/2479], loss: 1.692852\n",
      "Epoch: [37/900], step: [201/2479], loss: 1.387664\n",
      "Epoch: [37/900], step: [401/2479], loss: 1.999169\n",
      "Epoch: [37/900], step: [601/2479], loss: 2.513089\n",
      "Epoch: [37/900], step: [801/2479], loss: 2.085521\n",
      "Epoch: [37/900], step: [1001/2479], loss: 3.090790\n",
      "Epoch: [37/900], step: [1201/2479], loss: 2.632483\n",
      "Epoch: [37/900], step: [1401/2479], loss: 2.335875\n",
      "Epoch: [37/900], step: [1601/2479], loss: 2.344005\n",
      "Epoch: [37/900], step: [1801/2479], loss: 2.888768\n",
      "Epoch: [37/900], step: [2001/2479], loss: 2.662788\n",
      "Epoch: [37/900], step: [2201/2479], loss: 2.919391\n",
      "Epoch: [37/900], step: [2401/2479], loss: 1.517067\n",
      "Test: epoch 37 loss: 4.0582576\n",
      "Train: epoch 38\n",
      "Epoch: [38/900], step: [1/2479], loss: 2.102761\n",
      "Epoch: [38/900], step: [201/2479], loss: 2.228788\n",
      "Epoch: [38/900], step: [401/2479], loss: 2.229654\n",
      "Epoch: [38/900], step: [601/2479], loss: 2.543473\n",
      "Epoch: [38/900], step: [801/2479], loss: 1.875971\n",
      "Epoch: [38/900], step: [1001/2479], loss: 2.261848\n",
      "Epoch: [38/900], step: [1201/2479], loss: 2.445172\n",
      "Epoch: [38/900], step: [1401/2479], loss: 2.775629\n",
      "Epoch: [38/900], step: [1601/2479], loss: 2.055313\n",
      "Epoch: [38/900], step: [1801/2479], loss: 2.013055\n",
      "Epoch: [38/900], step: [2001/2479], loss: 2.316660\n",
      "Epoch: [38/900], step: [2201/2479], loss: 2.196113\n",
      "Epoch: [38/900], step: [2401/2479], loss: 2.067058\n",
      "Test: epoch 38 loss: 4.8681555\n",
      "Train: epoch 39\n",
      "Epoch: [39/900], step: [1/2479], loss: 1.995691\n",
      "Epoch: [39/900], step: [201/2479], loss: 2.321532\n",
      "Epoch: [39/900], step: [401/2479], loss: 2.385756\n",
      "Epoch: [39/900], step: [601/2479], loss: 2.402190\n",
      "Epoch: [39/900], step: [801/2479], loss: 2.697003\n",
      "Epoch: [39/900], step: [1001/2479], loss: 2.869011\n",
      "Epoch: [39/900], step: [1201/2479], loss: 1.894547\n",
      "Epoch: [39/900], step: [1401/2479], loss: 2.348562\n",
      "Epoch: [39/900], step: [1601/2479], loss: 2.334150\n",
      "Epoch: [39/900], step: [1801/2479], loss: 1.958746\n",
      "Epoch: [39/900], step: [2001/2479], loss: 2.315805\n",
      "Epoch: [39/900], step: [2201/2479], loss: 2.609199\n",
      "Epoch: [39/900], step: [2401/2479], loss: 2.540960\n",
      "Test: epoch 39 loss: 5.895412\n",
      "Train: epoch 40\n",
      "Epoch: [40/900], step: [1/2479], loss: 2.072693\n",
      "Epoch: [40/900], step: [201/2479], loss: 2.506536\n",
      "Epoch: [40/900], step: [401/2479], loss: 2.390217\n",
      "Epoch: [40/900], step: [601/2479], loss: 1.717819\n",
      "Epoch: [40/900], step: [801/2479], loss: 2.572262\n",
      "Epoch: [40/900], step: [1001/2479], loss: 2.188889\n",
      "Epoch: [40/900], step: [1201/2479], loss: 2.126921\n",
      "Epoch: [40/900], step: [1401/2479], loss: 2.437475\n",
      "Epoch: [40/900], step: [1601/2479], loss: 2.539128\n",
      "Epoch: [40/900], step: [1801/2479], loss: 2.882949\n",
      "Epoch: [40/900], step: [2001/2479], loss: 2.523242\n",
      "Epoch: [40/900], step: [2201/2479], loss: 2.224521\n",
      "Epoch: [40/900], step: [2401/2479], loss: 1.971705\n",
      "Test: epoch 40 loss: 10.213897\n",
      "Train: epoch 41\n",
      "Epoch: [41/900], step: [1/2479], loss: 1.928134\n",
      "Epoch: [41/900], step: [201/2479], loss: 2.037051\n",
      "Epoch: [41/900], step: [401/2479], loss: 2.632075\n",
      "Epoch: [41/900], step: [601/2479], loss: 2.202391\n",
      "Epoch: [41/900], step: [801/2479], loss: 2.887304\n",
      "Epoch: [41/900], step: [1001/2479], loss: 2.541908\n",
      "Epoch: [41/900], step: [1201/2479], loss: 1.835230\n",
      "Epoch: [41/900], step: [1401/2479], loss: 2.703022\n",
      "Epoch: [41/900], step: [1601/2479], loss: 2.664495\n",
      "Epoch: [41/900], step: [1801/2479], loss: 2.349755\n",
      "Epoch: [41/900], step: [2001/2479], loss: 2.136589\n",
      "Epoch: [41/900], step: [2201/2479], loss: 1.454723\n",
      "Epoch: [41/900], step: [2401/2479], loss: 1.416239\n",
      "Test: epoch 41 loss: 4.5291157\n",
      "Train: epoch 42\n",
      "Epoch: [42/900], step: [1/2479], loss: 2.097090\n",
      "Epoch: [42/900], step: [201/2479], loss: 2.488336\n",
      "Epoch: [42/900], step: [401/2479], loss: 2.237098\n",
      "Epoch: [42/900], step: [601/2479], loss: 1.533317\n",
      "Epoch: [42/900], step: [801/2479], loss: 2.216795\n",
      "Epoch: [42/900], step: [1001/2479], loss: 2.002441\n",
      "Epoch: [42/900], step: [1201/2479], loss: 2.285383\n",
      "Epoch: [42/900], step: [1401/2479], loss: 1.889288\n",
      "Epoch: [42/900], step: [1601/2479], loss: 2.412799\n",
      "Epoch: [42/900], step: [1801/2479], loss: 2.470881\n",
      "Epoch: [42/900], step: [2001/2479], loss: 2.356246\n",
      "Epoch: [42/900], step: [2201/2479], loss: 2.220447\n",
      "Epoch: [42/900], step: [2401/2479], loss: 2.452807\n",
      "Test: epoch 42 loss: 4.628897\n",
      "Train: epoch 43\n",
      "Epoch: [43/900], step: [1/2479], loss: 1.772812\n",
      "Epoch: [43/900], step: [201/2479], loss: 1.879617\n",
      "Epoch: [43/900], step: [401/2479], loss: 2.188172\n",
      "Epoch: [43/900], step: [601/2479], loss: 2.211512\n",
      "Epoch: [43/900], step: [801/2479], loss: 2.042221\n",
      "Epoch: [43/900], step: [1001/2479], loss: 2.953408\n",
      "Epoch: [43/900], step: [1201/2479], loss: 2.263164\n",
      "Epoch: [43/900], step: [1401/2479], loss: 1.776387\n",
      "Epoch: [43/900], step: [1601/2479], loss: 2.306582\n",
      "Epoch: [43/900], step: [1801/2479], loss: 2.194263\n",
      "Epoch: [43/900], step: [2001/2479], loss: 2.145977\n",
      "Epoch: [43/900], step: [2201/2479], loss: 2.039705\n",
      "Epoch: [43/900], step: [2401/2479], loss: 2.335413\n",
      "Test: epoch 43 loss: 6.101168\n",
      "Train: epoch 44\n",
      "Epoch: [44/900], step: [1/2479], loss: 1.484035\n",
      "Epoch: [44/900], step: [201/2479], loss: 1.891043\n",
      "Epoch: [44/900], step: [401/2479], loss: 2.030216\n",
      "Epoch: [44/900], step: [601/2479], loss: 2.753901\n",
      "Epoch: [44/900], step: [801/2479], loss: 2.204894\n",
      "Epoch: [44/900], step: [1001/2479], loss: 1.956737\n",
      "Epoch: [44/900], step: [1201/2479], loss: 1.820392\n",
      "Epoch: [44/900], step: [1401/2479], loss: 1.755466\n",
      "Epoch: [44/900], step: [1601/2479], loss: 2.120929\n",
      "Epoch: [44/900], step: [1801/2479], loss: 2.340727\n",
      "Epoch: [44/900], step: [2001/2479], loss: 2.444707\n",
      "Epoch: [44/900], step: [2201/2479], loss: 2.346389\n",
      "Epoch: [44/900], step: [2401/2479], loss: 2.615156\n",
      "Test: epoch 44 loss: 8.047147\n",
      "Train: epoch 45\n",
      "Epoch: [45/900], step: [1/2479], loss: 1.941386\n",
      "Epoch: [45/900], step: [201/2479], loss: 1.894063\n",
      "Epoch: [45/900], step: [401/2479], loss: 1.658273\n",
      "Epoch: [45/900], step: [601/2479], loss: 1.728873\n",
      "Epoch: [45/900], step: [801/2479], loss: 2.450361\n",
      "Epoch: [45/900], step: [1001/2479], loss: 1.806687\n",
      "Epoch: [45/900], step: [1201/2479], loss: 1.781796\n",
      "Epoch: [45/900], step: [1401/2479], loss: 1.785508\n",
      "Epoch: [45/900], step: [1601/2479], loss: 1.798506\n",
      "Epoch: [45/900], step: [1801/2479], loss: 2.306370\n",
      "Epoch: [45/900], step: [2001/2479], loss: 1.903457\n",
      "Epoch: [45/900], step: [2201/2479], loss: 2.253355\n",
      "Epoch: [45/900], step: [2401/2479], loss: 2.329183\n",
      "Test: epoch 45 loss: 5.069593\n",
      "Train: epoch 46\n",
      "Epoch: [46/900], step: [1/2479], loss: 1.480781\n",
      "Epoch: [46/900], step: [201/2479], loss: 1.652051\n",
      "Epoch: [46/900], step: [401/2479], loss: 1.749641\n",
      "Epoch: [46/900], step: [601/2479], loss: 2.445609\n",
      "Epoch: [46/900], step: [801/2479], loss: 2.622787\n",
      "Epoch: [46/900], step: [1001/2479], loss: 2.280336\n",
      "Epoch: [46/900], step: [1201/2479], loss: 2.166458\n",
      "Epoch: [46/900], step: [1401/2479], loss: 2.634790\n",
      "Epoch: [46/900], step: [1601/2479], loss: 2.108120\n",
      "Epoch: [46/900], step: [1801/2479], loss: 2.483366\n",
      "Epoch: [46/900], step: [2001/2479], loss: 2.903406\n",
      "Epoch: [46/900], step: [2201/2479], loss: 2.596231\n",
      "Epoch: [46/900], step: [2401/2479], loss: 1.743185\n",
      "Test: epoch 46 loss: 4.1338687\n",
      "Train: epoch 47\n",
      "Epoch: [47/900], step: [1/2479], loss: 2.424461\n",
      "Epoch: [47/900], step: [201/2479], loss: 2.291343\n",
      "Epoch: [47/900], step: [401/2479], loss: 1.435630\n",
      "Epoch: [47/900], step: [601/2479], loss: 2.450742\n",
      "Epoch: [47/900], step: [801/2479], loss: 1.861572\n",
      "Epoch: [47/900], step: [1001/2479], loss: 1.672768\n",
      "Epoch: [47/900], step: [1201/2479], loss: 2.191560\n",
      "Epoch: [47/900], step: [1401/2479], loss: 1.729584\n",
      "Epoch: [47/900], step: [1601/2479], loss: 2.597501\n",
      "Epoch: [47/900], step: [1801/2479], loss: 1.954502\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [47/900], step: [2001/2479], loss: 2.881992\n",
      "Epoch: [47/900], step: [2201/2479], loss: 2.160333\n",
      "Epoch: [47/900], step: [2401/2479], loss: 2.221586\n",
      "Test: epoch 47 loss: 4.334727\n",
      "Train: epoch 48\n",
      "Epoch: [48/900], step: [1/2479], loss: 1.564216\n",
      "Epoch: [48/900], step: [201/2479], loss: 1.802980\n",
      "Epoch: [48/900], step: [401/2479], loss: 1.822131\n",
      "Epoch: [48/900], step: [601/2479], loss: 2.479000\n",
      "Epoch: [48/900], step: [801/2479], loss: 2.135602\n",
      "Epoch: [48/900], step: [1001/2479], loss: 2.000051\n",
      "Epoch: [48/900], step: [1201/2479], loss: 2.378141\n",
      "Epoch: [48/900], step: [1401/2479], loss: 1.984845\n",
      "Epoch: [48/900], step: [1601/2479], loss: 1.905633\n",
      "Epoch: [48/900], step: [1801/2479], loss: 1.950553\n",
      "Epoch: [48/900], step: [2001/2479], loss: 2.899110\n",
      "Epoch: [48/900], step: [2201/2479], loss: 2.488944\n",
      "Epoch: [48/900], step: [2401/2479], loss: 1.911402\n",
      "Test: epoch 48 loss: 4.9420266\n",
      "Train: epoch 49\n",
      "Epoch: [49/900], step: [1/2479], loss: 1.928851\n",
      "Epoch: [49/900], step: [201/2479], loss: 2.064047\n",
      "Epoch: [49/900], step: [401/2479], loss: 2.745276\n",
      "Epoch: [49/900], step: [601/2479], loss: 1.885504\n",
      "Epoch: [49/900], step: [801/2479], loss: 2.111588\n",
      "Epoch: [49/900], step: [1001/2479], loss: 2.249445\n",
      "Epoch: [49/900], step: [1201/2479], loss: 2.177581\n",
      "Epoch: [49/900], step: [1401/2479], loss: 1.429910\n",
      "Epoch: [49/900], step: [1601/2479], loss: 2.111531\n",
      "Epoch: [49/900], step: [1801/2479], loss: 2.665125\n",
      "Epoch: [49/900], step: [2001/2479], loss: 1.618222\n",
      "Epoch: [49/900], step: [2201/2479], loss: 1.747335\n",
      "Epoch: [49/900], step: [2401/2479], loss: 2.004320\n",
      "Test: epoch 49 loss: 5.8036175\n",
      "Train: epoch 50\n",
      "Epoch: [50/900], step: [1/2479], loss: 1.654616\n",
      "Epoch: [50/900], step: [201/2479], loss: 2.176348\n",
      "Epoch: [50/900], step: [401/2479], loss: 2.672930\n",
      "Epoch: [50/900], step: [601/2479], loss: 2.091054\n",
      "Epoch: [50/900], step: [801/2479], loss: 2.087736\n",
      "Epoch: [50/900], step: [1001/2479], loss: 2.084696\n",
      "Epoch: [50/900], step: [1201/2479], loss: 1.990534\n",
      "Epoch: [50/900], step: [1401/2479], loss: 1.916589\n",
      "Epoch: [50/900], step: [1601/2479], loss: 2.499778\n",
      "Epoch: [50/900], step: [1801/2479], loss: 2.136236\n",
      "Epoch: [50/900], step: [2001/2479], loss: 1.568130\n",
      "Epoch: [50/900], step: [2201/2479], loss: 2.114872\n",
      "Epoch: [50/900], step: [2401/2479], loss: 1.984585\n",
      "Test: epoch 50 loss: 5.140958\n",
      "Train: epoch 51\n",
      "Epoch: [51/900], step: [1/2479], loss: 1.789312\n",
      "Epoch: [51/900], step: [201/2479], loss: 1.903193\n",
      "Epoch: [51/900], step: [401/2479], loss: 1.932781\n",
      "Epoch: [51/900], step: [601/2479], loss: 2.180593\n",
      "Epoch: [51/900], step: [801/2479], loss: 2.216326\n",
      "Epoch: [51/900], step: [1001/2479], loss: 2.177065\n",
      "Epoch: [51/900], step: [1201/2479], loss: 2.008779\n",
      "Epoch: [51/900], step: [1401/2479], loss: 2.098732\n",
      "Epoch: [51/900], step: [1601/2479], loss: 2.414438\n",
      "Epoch: [51/900], step: [1801/2479], loss: 1.813490\n",
      "Epoch: [51/900], step: [2001/2479], loss: 2.276063\n",
      "Epoch: [51/900], step: [2201/2479], loss: 2.049648\n",
      "Epoch: [51/900], step: [2401/2479], loss: 2.300816\n",
      "Test: epoch 51 loss: 6.9172683\n",
      "Train: epoch 52\n",
      "Epoch: [52/900], step: [1/2479], loss: 2.202520\n",
      "Epoch: [52/900], step: [201/2479], loss: 1.954678\n",
      "Epoch: [52/900], step: [401/2479], loss: 1.937143\n",
      "Epoch: [52/900], step: [601/2479], loss: 2.311574\n",
      "Epoch: [52/900], step: [801/2479], loss: 2.341893\n",
      "Epoch: [52/900], step: [1001/2479], loss: 1.768129\n",
      "Epoch: [52/900], step: [1201/2479], loss: 1.738654\n",
      "Epoch: [52/900], step: [1401/2479], loss: 2.160670\n",
      "Epoch: [52/900], step: [1601/2479], loss: 2.254260\n",
      "Epoch: [52/900], step: [1801/2479], loss: 1.703310\n",
      "Epoch: [52/900], step: [2001/2479], loss: 2.445643\n",
      "Epoch: [52/900], step: [2201/2479], loss: 1.797775\n",
      "Epoch: [52/900], step: [2401/2479], loss: 1.890173\n",
      "Test: epoch 52 loss: 4.3418255\n",
      "Train: epoch 53\n",
      "Epoch: [53/900], step: [1/2479], loss: 1.698023\n",
      "Epoch: [53/900], step: [201/2479], loss: 2.223049\n",
      "Epoch: [53/900], step: [401/2479], loss: 2.005749\n",
      "Epoch: [53/900], step: [601/2479], loss: 2.675020\n",
      "Epoch: [53/900], step: [801/2479], loss: 2.244657\n",
      "Epoch: [53/900], step: [1001/2479], loss: 1.898905\n",
      "Epoch: [53/900], step: [1201/2479], loss: 2.378910\n",
      "Epoch: [53/900], step: [1401/2479], loss: 1.987317\n",
      "Epoch: [53/900], step: [1601/2479], loss: 2.120652\n",
      "Epoch: [53/900], step: [1801/2479], loss: 1.771058\n",
      "Epoch: [53/900], step: [2001/2479], loss: 2.909514\n",
      "Epoch: [53/900], step: [2201/2479], loss: 2.487696\n",
      "Epoch: [53/900], step: [2401/2479], loss: 2.118710\n",
      "Test: epoch 53 loss: 6.978903\n",
      "Train: epoch 54\n",
      "Epoch: [54/900], step: [1/2479], loss: 1.462714\n",
      "Epoch: [54/900], step: [201/2479], loss: 2.294838\n",
      "Epoch: [54/900], step: [401/2479], loss: 1.602243\n",
      "Epoch: [54/900], step: [601/2479], loss: 2.194358\n",
      "Epoch: [54/900], step: [801/2479], loss: 2.372539\n",
      "Epoch: [54/900], step: [1001/2479], loss: 2.687720\n",
      "Epoch: [54/900], step: [1201/2479], loss: 1.593401\n",
      "Epoch: [54/900], step: [1401/2479], loss: 1.604492\n",
      "Epoch: [54/900], step: [1601/2479], loss: 2.119877\n",
      "Epoch: [54/900], step: [1801/2479], loss: 2.195942\n",
      "Epoch: [54/900], step: [2001/2479], loss: 2.202084\n",
      "Epoch: [54/900], step: [2201/2479], loss: 2.328506\n",
      "Epoch: [54/900], step: [2401/2479], loss: 2.701869\n",
      "Test: epoch 54 loss: 6.0653834\n",
      "Train: epoch 55\n",
      "Epoch: [55/900], step: [1/2479], loss: 1.983086\n",
      "Epoch: [55/900], step: [201/2479], loss: 2.398915\n",
      "Epoch: [55/900], step: [401/2479], loss: 1.802615\n",
      "Epoch: [55/900], step: [601/2479], loss: 1.791642\n",
      "Epoch: [55/900], step: [801/2479], loss: 2.620588\n",
      "Epoch: [55/900], step: [1001/2479], loss: 1.648198\n",
      "Epoch: [55/900], step: [1201/2479], loss: 2.401099\n",
      "Epoch: [55/900], step: [1401/2479], loss: 1.765643\n",
      "Epoch: [55/900], step: [1601/2479], loss: 1.702399\n",
      "Epoch: [55/900], step: [1801/2479], loss: 1.874519\n",
      "Epoch: [55/900], step: [2001/2479], loss: 2.611656\n",
      "Epoch: [55/900], step: [2201/2479], loss: 1.702914\n",
      "Epoch: [55/900], step: [2401/2479], loss: 1.897393\n",
      "Test: epoch 55 loss: 7.451376\n",
      "Train: epoch 56\n",
      "Epoch: [56/900], step: [1/2479], loss: 2.085698\n",
      "Epoch: [56/900], step: [201/2479], loss: 1.488545\n",
      "Epoch: [56/900], step: [401/2479], loss: 2.327117\n",
      "Epoch: [56/900], step: [601/2479], loss: 1.891089\n",
      "Epoch: [56/900], step: [801/2479], loss: 1.897699\n",
      "Epoch: [56/900], step: [1001/2479], loss: 1.674554\n",
      "Epoch: [56/900], step: [1201/2479], loss: 1.693253\n",
      "Epoch: [56/900], step: [1401/2479], loss: 2.174358\n",
      "Epoch: [56/900], step: [1601/2479], loss: 2.359075\n",
      "Epoch: [56/900], step: [1801/2479], loss: 2.697709\n",
      "Epoch: [56/900], step: [2001/2479], loss: 2.429672\n",
      "Epoch: [56/900], step: [2201/2479], loss: 1.998349\n",
      "Epoch: [56/900], step: [2401/2479], loss: 1.977392\n",
      "Test: epoch 56 loss: 3.210241\n",
      "Train: epoch 57\n",
      "Epoch: [57/900], step: [1/2479], loss: 1.474543\n",
      "Epoch: [57/900], step: [201/2479], loss: 1.671622\n",
      "Epoch: [57/900], step: [401/2479], loss: 2.177894\n",
      "Epoch: [57/900], step: [601/2479], loss: 1.511441\n",
      "Epoch: [57/900], step: [801/2479], loss: 1.990697\n",
      "Epoch: [57/900], step: [1001/2479], loss: 2.385286\n",
      "Epoch: [57/900], step: [1201/2479], loss: 1.910485\n",
      "Epoch: [57/900], step: [1401/2479], loss: 2.004641\n",
      "Epoch: [57/900], step: [1601/2479], loss: 1.903160\n",
      "Epoch: [57/900], step: [1801/2479], loss: 2.368540\n",
      "Epoch: [57/900], step: [2001/2479], loss: 1.760769\n",
      "Epoch: [57/900], step: [2201/2479], loss: 2.777815\n",
      "Epoch: [57/900], step: [2401/2479], loss: 2.369366\n",
      "Test: epoch 57 loss: 5.103473\n",
      "Train: epoch 58\n",
      "Epoch: [58/900], step: [1/2479], loss: 2.314705\n",
      "Epoch: [58/900], step: [201/2479], loss: 2.868805\n",
      "Epoch: [58/900], step: [401/2479], loss: 1.473454\n",
      "Epoch: [58/900], step: [601/2479], loss: 2.130997\n",
      "Epoch: [58/900], step: [801/2479], loss: 1.939818\n",
      "Epoch: [58/900], step: [1001/2479], loss: 2.408759\n",
      "Epoch: [58/900], step: [1201/2479], loss: 2.261043\n",
      "Epoch: [58/900], step: [1401/2479], loss: 1.730189\n",
      "Epoch: [58/900], step: [1601/2479], loss: 1.650548\n",
      "Epoch: [58/900], step: [1801/2479], loss: 1.895994\n",
      "Epoch: [58/900], step: [2001/2479], loss: 1.981388\n",
      "Epoch: [58/900], step: [2201/2479], loss: 2.320532\n",
      "Epoch: [58/900], step: [2401/2479], loss: 1.637875\n",
      "Test: epoch 58 loss: 4.2684073\n",
      "Train: epoch 59\n",
      "Epoch: [59/900], step: [1/2479], loss: 2.037697\n",
      "Epoch: [59/900], step: [201/2479], loss: 2.393177\n",
      "Epoch: [59/900], step: [401/2479], loss: 1.958477\n",
      "Epoch: [59/900], step: [601/2479], loss: 2.300226\n",
      "Epoch: [59/900], step: [801/2479], loss: 2.084212\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [59/900], step: [1001/2479], loss: 1.956298\n",
      "Epoch: [59/900], step: [1201/2479], loss: 2.143432\n",
      "Epoch: [59/900], step: [1401/2479], loss: 1.924211\n",
      "Epoch: [59/900], step: [1601/2479], loss: 1.314072\n",
      "Epoch: [59/900], step: [1801/2479], loss: 1.959152\n",
      "Epoch: [59/900], step: [2001/2479], loss: 1.802448\n",
      "Epoch: [59/900], step: [2201/2479], loss: 2.081960\n",
      "Epoch: [59/900], step: [2401/2479], loss: 2.212619\n",
      "Test: epoch 59 loss: 7.635249\n",
      "Train: epoch 60\n",
      "Epoch: [60/900], step: [1/2479], loss: 1.775971\n",
      "Epoch: [60/900], step: [201/2479], loss: 2.172951\n",
      "Epoch: [60/900], step: [401/2479], loss: 2.067847\n",
      "Epoch: [60/900], step: [601/2479], loss: 1.903731\n",
      "Epoch: [60/900], step: [801/2479], loss: 2.109278\n",
      "Epoch: [60/900], step: [1001/2479], loss: 2.424789\n",
      "Epoch: [60/900], step: [1201/2479], loss: 1.973078\n",
      "Epoch: [60/900], step: [1401/2479], loss: 2.573982\n",
      "Epoch: [60/900], step: [1601/2479], loss: 2.405604\n",
      "Epoch: [60/900], step: [1801/2479], loss: 1.493515\n",
      "Epoch: [60/900], step: [2001/2479], loss: 1.997939\n",
      "Epoch: [60/900], step: [2201/2479], loss: 2.215529\n",
      "Epoch: [60/900], step: [2401/2479], loss: 2.458780\n",
      "Test: epoch 60 loss: 5.231601\n",
      "Train: epoch 61\n",
      "Epoch: [61/900], step: [1/2479], loss: 0.942146\n",
      "Epoch: [61/900], step: [201/2479], loss: 2.038642\n",
      "Epoch: [61/900], step: [401/2479], loss: 2.182606\n",
      "Epoch: [61/900], step: [601/2479], loss: 2.112521\n",
      "Epoch: [61/900], step: [801/2479], loss: 1.739330\n",
      "Epoch: [61/900], step: [1001/2479], loss: 2.321879\n",
      "Epoch: [61/900], step: [1201/2479], loss: 1.681267\n",
      "Epoch: [61/900], step: [1401/2479], loss: 2.583529\n",
      "Epoch: [61/900], step: [1601/2479], loss: 1.585833\n",
      "Epoch: [61/900], step: [1801/2479], loss: 2.215046\n",
      "Epoch: [61/900], step: [2001/2479], loss: 2.300614\n",
      "Epoch: [61/900], step: [2201/2479], loss: 2.719898\n",
      "Epoch: [61/900], step: [2401/2479], loss: 1.719479\n",
      "Test: epoch 61 loss: 4.0513353\n",
      "Train: epoch 62\n",
      "Epoch: [62/900], step: [1/2479], loss: 1.773423\n",
      "Epoch: [62/900], step: [201/2479], loss: 1.655169\n",
      "Epoch: [62/900], step: [401/2479], loss: 2.597899\n",
      "Epoch: [62/900], step: [601/2479], loss: 2.458119\n",
      "Epoch: [62/900], step: [801/2479], loss: 1.761121\n",
      "Epoch: [62/900], step: [1001/2479], loss: 1.944829\n",
      "Epoch: [62/900], step: [1201/2479], loss: 2.040376\n",
      "Epoch: [62/900], step: [1401/2479], loss: 2.390440\n",
      "Epoch: [62/900], step: [1601/2479], loss: 1.588904\n",
      "Epoch: [62/900], step: [1801/2479], loss: 1.292794\n",
      "Epoch: [62/900], step: [2001/2479], loss: 2.269519\n",
      "Epoch: [62/900], step: [2201/2479], loss: 2.532445\n",
      "Epoch: [62/900], step: [2401/2479], loss: 2.311242\n",
      "Test: epoch 62 loss: 4.8754\n",
      "Train: epoch 63\n",
      "Epoch: [63/900], step: [1/2479], loss: 2.115188\n",
      "Epoch: [63/900], step: [201/2479], loss: 1.641612\n",
      "Epoch: [63/900], step: [401/2479], loss: 1.494487\n",
      "Epoch: [63/900], step: [601/2479], loss: 2.365269\n",
      "Epoch: [63/900], step: [801/2479], loss: 1.900003\n",
      "Epoch: [63/900], step: [1001/2479], loss: 2.066289\n",
      "Epoch: [63/900], step: [1201/2479], loss: 2.041011\n",
      "Epoch: [63/900], step: [1401/2479], loss: 1.679827\n",
      "Epoch: [63/900], step: [1601/2479], loss: 2.107034\n",
      "Epoch: [63/900], step: [1801/2479], loss: 1.789286\n",
      "Epoch: [63/900], step: [2001/2479], loss: 2.718539\n",
      "Epoch: [63/900], step: [2201/2479], loss: 1.888977\n",
      "Epoch: [63/900], step: [2401/2479], loss: 2.250211\n",
      "Test: epoch 63 loss: 6.9031153\n",
      "Train: epoch 64\n",
      "Epoch: [64/900], step: [1/2479], loss: 1.991806\n",
      "Epoch: [64/900], step: [201/2479], loss: 2.274671\n",
      "Epoch: [64/900], step: [401/2479], loss: 1.662010\n",
      "Epoch: [64/900], step: [601/2479], loss: 2.536393\n",
      "Epoch: [64/900], step: [801/2479], loss: 2.172134\n",
      "Epoch: [64/900], step: [1001/2479], loss: 1.993994\n",
      "Epoch: [64/900], step: [1201/2479], loss: 1.975715\n",
      "Epoch: [64/900], step: [1401/2479], loss: 2.279863\n",
      "Epoch: [64/900], step: [1601/2479], loss: 1.902225\n",
      "Epoch: [64/900], step: [1801/2479], loss: 2.718497\n",
      "Epoch: [64/900], step: [2001/2479], loss: 1.446870\n",
      "Epoch: [64/900], step: [2201/2479], loss: 2.678119\n",
      "Epoch: [64/900], step: [2401/2479], loss: 2.211864\n",
      "Test: epoch 64 loss: 5.1961784\n",
      "Train: epoch 65\n",
      "Epoch: [65/900], step: [1/2479], loss: 2.270955\n",
      "Epoch: [65/900], step: [201/2479], loss: 1.806900\n",
      "Epoch: [65/900], step: [401/2479], loss: 1.916625\n",
      "Epoch: [65/900], step: [601/2479], loss: 2.177682\n",
      "Epoch: [65/900], step: [801/2479], loss: 2.000085\n",
      "Epoch: [65/900], step: [1001/2479], loss: 2.358721\n",
      "Epoch: [65/900], step: [1201/2479], loss: 2.301635\n",
      "Epoch: [65/900], step: [1401/2479], loss: 2.715978\n",
      "Epoch: [65/900], step: [1601/2479], loss: 2.533840\n",
      "Epoch: [65/900], step: [1801/2479], loss: 2.450317\n",
      "Epoch: [65/900], step: [2001/2479], loss: 1.871817\n",
      "Epoch: [65/900], step: [2201/2479], loss: 1.891514\n",
      "Epoch: [65/900], step: [2401/2479], loss: 2.033926\n",
      "Test: epoch 65 loss: 6.7946143\n",
      "Train: epoch 66\n",
      "Epoch: [66/900], step: [1/2479], loss: 2.776235\n",
      "Epoch: [66/900], step: [201/2479], loss: 2.227974\n",
      "Epoch: [66/900], step: [401/2479], loss: 1.883728\n",
      "Epoch: [66/900], step: [601/2479], loss: 1.905142\n",
      "Epoch: [66/900], step: [801/2479], loss: 1.963397\n",
      "Epoch: [66/900], step: [1001/2479], loss: 2.274676\n",
      "Epoch: [66/900], step: [1201/2479], loss: 2.008568\n",
      "Epoch: [66/900], step: [1401/2479], loss: 1.561386\n",
      "Epoch: [66/900], step: [1601/2479], loss: 2.238679\n",
      "Epoch: [66/900], step: [1801/2479], loss: 2.271660\n",
      "Epoch: [66/900], step: [2001/2479], loss: 2.662726\n",
      "Epoch: [66/900], step: [2201/2479], loss: 2.050894\n",
      "Epoch: [66/900], step: [2401/2479], loss: 1.863732\n",
      "Test: epoch 66 loss: 5.6176233\n",
      "Train: epoch 67\n",
      "Epoch: [67/900], step: [1/2479], loss: 1.301906\n",
      "Epoch: [67/900], step: [201/2479], loss: 1.630474\n",
      "Epoch: [67/900], step: [401/2479], loss: 2.231878\n",
      "Epoch: [67/900], step: [601/2479], loss: 2.260904\n",
      "Epoch: [67/900], step: [801/2479], loss: 1.294472\n",
      "Epoch: [67/900], step: [1001/2479], loss: 2.108418\n",
      "Epoch: [67/900], step: [1201/2479], loss: 1.941036\n",
      "Epoch: [67/900], step: [1401/2479], loss: 2.190218\n",
      "Epoch: [67/900], step: [1601/2479], loss: 1.820229\n",
      "Epoch: [67/900], step: [1801/2479], loss: 1.776601\n",
      "Epoch: [67/900], step: [2001/2479], loss: 1.418256\n",
      "Epoch: [67/900], step: [2201/2479], loss: 2.184729\n",
      "Epoch: [67/900], step: [2401/2479], loss: 1.815207\n",
      "Test: epoch 67 loss: 5.948044\n",
      "Train: epoch 68\n",
      "Epoch: [68/900], step: [1/2479], loss: 1.514333\n",
      "Epoch: [68/900], step: [201/2479], loss: 1.607052\n",
      "Epoch: [68/900], step: [401/2479], loss: 1.642924\n",
      "Epoch: [68/900], step: [601/2479], loss: 1.974495\n",
      "Epoch: [68/900], step: [801/2479], loss: 1.811730\n",
      "Epoch: [68/900], step: [1001/2479], loss: 1.732687\n",
      "Epoch: [68/900], step: [1201/2479], loss: 2.082137\n",
      "Epoch: [68/900], step: [1401/2479], loss: 1.569786\n",
      "Epoch: [68/900], step: [1601/2479], loss: 2.328523\n",
      "Epoch: [68/900], step: [1801/2479], loss: 1.787724\n",
      "Epoch: [68/900], step: [2001/2479], loss: 2.076612\n",
      "Epoch: [68/900], step: [2201/2479], loss: 2.271892\n",
      "Epoch: [68/900], step: [2401/2479], loss: 1.998382\n",
      "Test: epoch 68 loss: 5.2983413\n",
      "Train: epoch 69\n",
      "Epoch: [69/900], step: [1/2479], loss: 1.981737\n",
      "Epoch: [69/900], step: [201/2479], loss: 1.771809\n",
      "Epoch: [69/900], step: [401/2479], loss: 1.765390\n",
      "Epoch: [69/900], step: [601/2479], loss: 2.082149\n",
      "Epoch: [69/900], step: [801/2479], loss: 2.346780\n",
      "Epoch: [69/900], step: [1001/2479], loss: 1.830126\n",
      "Epoch: [69/900], step: [1201/2479], loss: 2.379616\n",
      "Epoch: [69/900], step: [1401/2479], loss: 1.170591\n",
      "Epoch: [69/900], step: [1601/2479], loss: 2.244220\n",
      "Epoch: [69/900], step: [1801/2479], loss: 1.982662\n",
      "Epoch: [69/900], step: [2001/2479], loss: 2.196540\n",
      "Epoch: [69/900], step: [2201/2479], loss: 2.509650\n",
      "Epoch: [69/900], step: [2401/2479], loss: 2.566246\n",
      "Test: epoch 69 loss: 3.4269478\n",
      "Train: epoch 70\n",
      "Epoch: [70/900], step: [1/2479], loss: 1.586285\n",
      "Epoch: [70/900], step: [201/2479], loss: 2.492867\n",
      "Epoch: [70/900], step: [401/2479], loss: 1.940252\n",
      "Epoch: [70/900], step: [601/2479], loss: 2.249300\n",
      "Epoch: [70/900], step: [801/2479], loss: 1.721644\n",
      "Epoch: [70/900], step: [1001/2479], loss: 2.516208\n",
      "Epoch: [70/900], step: [1201/2479], loss: 2.063923\n",
      "Epoch: [70/900], step: [1401/2479], loss: 1.469597\n",
      "Epoch: [70/900], step: [1601/2479], loss: 2.187635\n",
      "Epoch: [70/900], step: [1801/2479], loss: 2.348088\n",
      "Epoch: [70/900], step: [2001/2479], loss: 2.543599\n",
      "Epoch: [70/900], step: [2201/2479], loss: 2.611364\n",
      "Epoch: [70/900], step: [2401/2479], loss: 1.818939\n",
      "Test: epoch 70 loss: 5.2106943\n",
      "Train: epoch 71\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [71/900], step: [1/2479], loss: 1.938320\n",
      "Epoch: [71/900], step: [201/2479], loss: 1.854643\n",
      "Epoch: [71/900], step: [401/2479], loss: 1.649571\n",
      "Epoch: [71/900], step: [601/2479], loss: 3.208989\n",
      "Epoch: [71/900], step: [801/2479], loss: 1.987198\n",
      "Epoch: [71/900], step: [1001/2479], loss: 2.413112\n",
      "Epoch: [71/900], step: [1201/2479], loss: 1.606021\n",
      "Epoch: [71/900], step: [1401/2479], loss: 2.503923\n",
      "Epoch: [71/900], step: [1601/2479], loss: 2.038549\n",
      "Epoch: [71/900], step: [1801/2479], loss: 2.217619\n",
      "Epoch: [71/900], step: [2001/2479], loss: 2.201038\n",
      "Epoch: [71/900], step: [2201/2479], loss: 1.916034\n",
      "Epoch: [71/900], step: [2401/2479], loss: 1.745366\n",
      "Test: epoch 71 loss: 5.887672\n",
      "Train: epoch 72\n",
      "Epoch: [72/900], step: [1/2479], loss: 1.742571\n",
      "Epoch: [72/900], step: [201/2479], loss: 1.908734\n",
      "Epoch: [72/900], step: [401/2479], loss: 1.960262\n",
      "Epoch: [72/900], step: [601/2479], loss: 1.717284\n",
      "Epoch: [72/900], step: [801/2479], loss: 1.646533\n",
      "Epoch: [72/900], step: [1001/2479], loss: 1.795344\n",
      "Epoch: [72/900], step: [1201/2479], loss: 1.958334\n",
      "Epoch: [72/900], step: [1401/2479], loss: 2.002114\n",
      "Epoch: [72/900], step: [1601/2479], loss: 1.844414\n",
      "Epoch: [72/900], step: [1801/2479], loss: 1.636727\n",
      "Epoch: [72/900], step: [2001/2479], loss: 1.772148\n",
      "Epoch: [72/900], step: [2201/2479], loss: 1.811303\n",
      "Epoch: [72/900], step: [2401/2479], loss: 2.874650\n",
      "Test: epoch 72 loss: 5.880472\n",
      "Train: epoch 73\n",
      "Epoch: [73/900], step: [1/2479], loss: 1.525532\n",
      "Epoch: [73/900], step: [201/2479], loss: 2.005699\n",
      "Epoch: [73/900], step: [401/2479], loss: 2.214641\n",
      "Epoch: [73/900], step: [601/2479], loss: 2.328048\n",
      "Epoch: [73/900], step: [801/2479], loss: 1.625554\n",
      "Epoch: [73/900], step: [1001/2479], loss: 1.653241\n",
      "Epoch: [73/900], step: [1201/2479], loss: 2.124570\n",
      "Epoch: [73/900], step: [1401/2479], loss: 1.292494\n",
      "Epoch: [73/900], step: [1601/2479], loss: 1.755003\n",
      "Epoch: [73/900], step: [1801/2479], loss: 2.001951\n",
      "Epoch: [73/900], step: [2001/2479], loss: 1.631460\n",
      "Epoch: [73/900], step: [2201/2479], loss: 1.768271\n",
      "Epoch: [73/900], step: [2401/2479], loss: 2.129248\n",
      "Test: epoch 73 loss: 5.711086\n",
      "Train: epoch 74\n",
      "Epoch: [74/900], step: [1/2479], loss: 1.890175\n",
      "Epoch: [74/900], step: [201/2479], loss: 1.545988\n",
      "Epoch: [74/900], step: [401/2479], loss: 1.595255\n",
      "Epoch: [74/900], step: [601/2479], loss: 2.129615\n",
      "Epoch: [74/900], step: [801/2479], loss: 2.233746\n",
      "Epoch: [74/900], step: [1001/2479], loss: 1.847503\n",
      "Epoch: [74/900], step: [1201/2479], loss: 1.413001\n",
      "Epoch: [74/900], step: [1401/2479], loss: 1.997978\n",
      "Epoch: [74/900], step: [1601/2479], loss: 1.873485\n",
      "Epoch: [74/900], step: [1801/2479], loss: 2.236644\n",
      "Epoch: [74/900], step: [2001/2479], loss: 1.597318\n",
      "Epoch: [74/900], step: [2201/2479], loss: 1.582119\n",
      "Epoch: [74/900], step: [2401/2479], loss: 1.384610\n",
      "Test: epoch 74 loss: 4.411753\n",
      "Train: epoch 75\n",
      "Epoch: [75/900], step: [1/2479], loss: 2.185148\n",
      "Epoch: [75/900], step: [201/2479], loss: 1.426094\n",
      "Epoch: [75/900], step: [401/2479], loss: 1.669652\n",
      "Epoch: [75/900], step: [601/2479], loss: 2.040291\n",
      "Epoch: [75/900], step: [801/2479], loss: 1.991423\n",
      "Epoch: [75/900], step: [1001/2479], loss: 1.707104\n",
      "Epoch: [75/900], step: [1201/2479], loss: 1.585684\n",
      "Epoch: [75/900], step: [1401/2479], loss: 1.968019\n",
      "Epoch: [75/900], step: [1601/2479], loss: 2.155048\n",
      "Epoch: [75/900], step: [1801/2479], loss: 1.748343\n",
      "Epoch: [75/900], step: [2001/2479], loss: 2.363602\n",
      "Epoch: [75/900], step: [2201/2479], loss: 1.822577\n",
      "Epoch: [75/900], step: [2401/2479], loss: 1.739268\n",
      "Test: epoch 75 loss: 5.84675\n",
      "Train: epoch 76\n",
      "Epoch: [76/900], step: [1/2479], loss: 2.130255\n",
      "Epoch: [76/900], step: [201/2479], loss: 1.515364\n",
      "Epoch: [76/900], step: [401/2479], loss: 1.890262\n",
      "Epoch: [76/900], step: [601/2479], loss: 2.261611\n",
      "Epoch: [76/900], step: [801/2479], loss: 2.136402\n",
      "Epoch: [76/900], step: [1001/2479], loss: 2.247084\n",
      "Epoch: [76/900], step: [1201/2479], loss: 2.122267\n",
      "Epoch: [76/900], step: [1401/2479], loss: 1.630598\n",
      "Epoch: [76/900], step: [1601/2479], loss: 2.578894\n",
      "Epoch: [76/900], step: [1801/2479], loss: 2.463616\n",
      "Epoch: [76/900], step: [2001/2479], loss: 2.328606\n",
      "Epoch: [76/900], step: [2201/2479], loss: 1.512653\n",
      "Epoch: [76/900], step: [2401/2479], loss: 1.758488\n",
      "Test: epoch 76 loss: 7.700175\n",
      "Train: epoch 77\n",
      "Epoch: [77/900], step: [1/2479], loss: 1.941334\n",
      "Epoch: [77/900], step: [201/2479], loss: 1.590304\n",
      "Epoch: [77/900], step: [401/2479], loss: 2.000879\n",
      "Epoch: [77/900], step: [601/2479], loss: 2.339011\n",
      "Epoch: [77/900], step: [801/2479], loss: 2.717378\n",
      "Epoch: [77/900], step: [1001/2479], loss: 1.602088\n",
      "Epoch: [77/900], step: [1201/2479], loss: 2.404479\n",
      "Epoch: [77/900], step: [1401/2479], loss: 1.520732\n",
      "Epoch: [77/900], step: [1601/2479], loss: 2.241534\n",
      "Epoch: [77/900], step: [1801/2479], loss: 2.455985\n",
      "Epoch: [77/900], step: [2001/2479], loss: 1.878176\n",
      "Epoch: [77/900], step: [2201/2479], loss: 1.692526\n",
      "Epoch: [77/900], step: [2401/2479], loss: 2.279710\n",
      "Test: epoch 77 loss: 4.6944637\n",
      "Train: epoch 78\n",
      "Epoch: [78/900], step: [1/2479], loss: 1.640103\n",
      "Epoch: [78/900], step: [201/2479], loss: 1.865712\n",
      "Epoch: [78/900], step: [401/2479], loss: 1.847034\n",
      "Epoch: [78/900], step: [601/2479], loss: 2.066958\n",
      "Epoch: [78/900], step: [801/2479], loss: 2.205824\n",
      "Epoch: [78/900], step: [1001/2479], loss: 2.241457\n",
      "Epoch: [78/900], step: [1201/2479], loss: 2.421516\n",
      "Epoch: [78/900], step: [1401/2479], loss: 2.014267\n",
      "Epoch: [78/900], step: [1601/2479], loss: 1.521351\n",
      "Epoch: [78/900], step: [1801/2479], loss: 1.448735\n",
      "Epoch: [78/900], step: [2001/2479], loss: 1.639820\n",
      "Epoch: [78/900], step: [2201/2479], loss: 1.987286\n",
      "Epoch: [78/900], step: [2401/2479], loss: 2.518691\n",
      "Test: epoch 78 loss: 6.922476\n",
      "Train: epoch 79\n",
      "Epoch: [79/900], step: [1/2479], loss: 1.796720\n",
      "Epoch: [79/900], step: [201/2479], loss: 1.777239\n",
      "Epoch: [79/900], step: [401/2479], loss: 2.177314\n",
      "Epoch: [79/900], step: [601/2479], loss: 1.563622\n",
      "Epoch: [79/900], step: [801/2479], loss: 2.177871\n",
      "Epoch: [79/900], step: [1001/2479], loss: 2.201266\n",
      "Epoch: [79/900], step: [1201/2479], loss: 1.772467\n",
      "Epoch: [79/900], step: [1401/2479], loss: 1.888609\n",
      "Epoch: [79/900], step: [1601/2479], loss: 1.783320\n",
      "Epoch: [79/900], step: [1801/2479], loss: 1.911684\n",
      "Epoch: [79/900], step: [2001/2479], loss: 1.991023\n",
      "Epoch: [79/900], step: [2201/2479], loss: 1.671388\n",
      "Epoch: [79/900], step: [2401/2479], loss: 2.299302\n",
      "Test: epoch 79 loss: 3.2850688\n",
      "Train: epoch 80\n",
      "Epoch: [80/900], step: [1/2479], loss: 1.610577\n",
      "Epoch: [80/900], step: [201/2479], loss: 1.439626\n",
      "Epoch: [80/900], step: [401/2479], loss: 1.860649\n",
      "Epoch: [80/900], step: [601/2479], loss: 2.065029\n",
      "Epoch: [80/900], step: [801/2479], loss: 1.982815\n",
      "Epoch: [80/900], step: [1001/2479], loss: 2.018569\n",
      "Epoch: [80/900], step: [1201/2479], loss: 2.518406\n",
      "Epoch: [80/900], step: [1401/2479], loss: 2.349133\n",
      "Epoch: [80/900], step: [1601/2479], loss: 2.152802\n",
      "Epoch: [80/900], step: [1801/2479], loss: 2.104676\n",
      "Epoch: [80/900], step: [2001/2479], loss: 1.604846\n",
      "Epoch: [80/900], step: [2201/2479], loss: 1.768600\n",
      "Epoch: [80/900], step: [2401/2479], loss: 1.985945\n",
      "Test: epoch 80 loss: 5.9672017\n",
      "Train: epoch 81\n",
      "Epoch: [81/900], step: [1/2479], loss: 1.889921\n",
      "Epoch: [81/900], step: [201/2479], loss: 1.708944\n",
      "Epoch: [81/900], step: [401/2479], loss: 1.550368\n",
      "Epoch: [81/900], step: [601/2479], loss: 2.279341\n",
      "Epoch: [81/900], step: [801/2479], loss: 1.710072\n",
      "Epoch: [81/900], step: [1001/2479], loss: 1.713106\n",
      "Epoch: [81/900], step: [1201/2479], loss: 2.471392\n",
      "Epoch: [81/900], step: [1401/2479], loss: 1.794274\n",
      "Epoch: [81/900], step: [1601/2479], loss: 2.013757\n",
      "Epoch: [81/900], step: [1801/2479], loss: 2.058285\n",
      "Epoch: [81/900], step: [2001/2479], loss: 2.169306\n",
      "Epoch: [81/900], step: [2201/2479], loss: 3.481131\n",
      "Epoch: [81/900], step: [2401/2479], loss: 1.749714\n",
      "Test: epoch 81 loss: 8.013686\n",
      "Train: epoch 82\n",
      "Epoch: [82/900], step: [1/2479], loss: 2.283536\n",
      "Epoch: [82/900], step: [201/2479], loss: 1.712403\n",
      "Epoch: [82/900], step: [401/2479], loss: 2.195341\n",
      "Epoch: [82/900], step: [601/2479], loss: 1.704374\n",
      "Epoch: [82/900], step: [801/2479], loss: 1.986028\n",
      "Epoch: [82/900], step: [1001/2479], loss: 2.200567\n",
      "Epoch: [82/900], step: [1201/2479], loss: 1.970556\n",
      "Epoch: [82/900], step: [1401/2479], loss: 1.856667\n",
      "Epoch: [82/900], step: [1601/2479], loss: 1.843212\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [82/900], step: [1801/2479], loss: 1.585885\n",
      "Epoch: [82/900], step: [2001/2479], loss: 1.466191\n",
      "Epoch: [82/900], step: [2201/2479], loss: 1.700557\n",
      "Epoch: [82/900], step: [2401/2479], loss: 2.240819\n",
      "Test: epoch 82 loss: 5.9050183\n",
      "Train: epoch 83\n",
      "Epoch: [83/900], step: [1/2479], loss: 2.631446\n",
      "Epoch: [83/900], step: [201/2479], loss: 1.251616\n",
      "Epoch: [83/900], step: [401/2479], loss: 1.473358\n",
      "Epoch: [83/900], step: [601/2479], loss: 1.494940\n",
      "Epoch: [83/900], step: [801/2479], loss: 1.735320\n",
      "Epoch: [83/900], step: [1001/2479], loss: 2.106371\n",
      "Epoch: [83/900], step: [1201/2479], loss: 1.443219\n",
      "Epoch: [83/900], step: [1401/2479], loss: 1.274338\n",
      "Epoch: [83/900], step: [1601/2479], loss: 2.024905\n",
      "Epoch: [83/900], step: [1801/2479], loss: 1.902372\n",
      "Epoch: [83/900], step: [2001/2479], loss: 1.457568\n",
      "Epoch: [83/900], step: [2201/2479], loss: 1.927399\n",
      "Epoch: [83/900], step: [2401/2479], loss: 2.013811\n",
      "Test: epoch 83 loss: 5.502715\n",
      "Train: epoch 84\n",
      "Epoch: [84/900], step: [1/2479], loss: 1.759981\n",
      "Epoch: [84/900], step: [201/2479], loss: 2.269113\n",
      "Epoch: [84/900], step: [401/2479], loss: 1.717742\n",
      "Epoch: [84/900], step: [601/2479], loss: 1.849568\n",
      "Epoch: [84/900], step: [801/2479], loss: 1.722100\n",
      "Epoch: [84/900], step: [1001/2479], loss: 2.371976\n",
      "Epoch: [84/900], step: [1201/2479], loss: 2.465759\n",
      "Epoch: [84/900], step: [1401/2479], loss: 2.118308\n",
      "Epoch: [84/900], step: [1601/2479], loss: 1.853240\n",
      "Epoch: [84/900], step: [1801/2479], loss: 2.318136\n",
      "Epoch: [84/900], step: [2001/2479], loss: 2.114345\n",
      "Epoch: [84/900], step: [2201/2479], loss: 2.724784\n",
      "Epoch: [84/900], step: [2401/2479], loss: 2.277650\n",
      "Test: epoch 84 loss: 5.8810463\n",
      "Train: epoch 85\n",
      "Epoch: [85/900], step: [1/2479], loss: 1.758450\n",
      "Epoch: [85/900], step: [201/2479], loss: 1.678562\n",
      "Epoch: [85/900], step: [401/2479], loss: 1.707751\n",
      "Epoch: [85/900], step: [601/2479], loss: 1.961693\n",
      "Epoch: [85/900], step: [801/2479], loss: 1.880617\n",
      "Epoch: [85/900], step: [1001/2479], loss: 1.597692\n",
      "Epoch: [85/900], step: [1201/2479], loss: 2.077414\n",
      "Epoch: [85/900], step: [1401/2479], loss: 1.893897\n",
      "Epoch: [85/900], step: [1601/2479], loss: 1.696832\n",
      "Epoch: [85/900], step: [1801/2479], loss: 1.757565\n",
      "Epoch: [85/900], step: [2001/2479], loss: 2.277215\n",
      "Epoch: [85/900], step: [2201/2479], loss: 1.448517\n",
      "Epoch: [85/900], step: [2401/2479], loss: 1.711706\n",
      "Test: epoch 85 loss: 10.067115\n",
      "Train: epoch 86\n",
      "Epoch: [86/900], step: [1/2479], loss: 1.559828\n",
      "Epoch: [86/900], step: [201/2479], loss: 1.800858\n",
      "Epoch: [86/900], step: [401/2479], loss: 1.278342\n",
      "Epoch: [86/900], step: [601/2479], loss: 1.593074\n",
      "Epoch: [86/900], step: [801/2479], loss: 2.094401\n",
      "Epoch: [86/900], step: [1001/2479], loss: 1.147418\n",
      "Epoch: [86/900], step: [1201/2479], loss: 1.971325\n",
      "Epoch: [86/900], step: [1401/2479], loss: 1.356022\n",
      "Epoch: [86/900], step: [1601/2479], loss: 1.499812\n",
      "Epoch: [86/900], step: [1801/2479], loss: 1.643359\n",
      "Epoch: [86/900], step: [2001/2479], loss: 1.588696\n",
      "Epoch: [86/900], step: [2201/2479], loss: 1.907636\n",
      "Epoch: [86/900], step: [2401/2479], loss: 2.291571\n",
      "Test: epoch 86 loss: 6.767033\n",
      "Train: epoch 87\n",
      "Epoch: [87/900], step: [1/2479], loss: 1.990390\n",
      "Epoch: [87/900], step: [201/2479], loss: 1.466335\n",
      "Epoch: [87/900], step: [401/2479], loss: 1.962062\n",
      "Epoch: [87/900], step: [601/2479], loss: 2.262784\n",
      "Epoch: [87/900], step: [801/2479], loss: 1.396754\n",
      "Epoch: [87/900], step: [1001/2479], loss: 1.990956\n",
      "Epoch: [87/900], step: [1201/2479], loss: 2.289558\n",
      "Epoch: [87/900], step: [1401/2479], loss: 1.482930\n",
      "Epoch: [87/900], step: [1601/2479], loss: 1.904509\n",
      "Epoch: [87/900], step: [1801/2479], loss: 2.245999\n",
      "Epoch: [87/900], step: [2001/2479], loss: 1.505446\n",
      "Epoch: [87/900], step: [2201/2479], loss: 1.921098\n",
      "Epoch: [87/900], step: [2401/2479], loss: 1.843502\n",
      "Test: epoch 87 loss: 5.265191\n",
      "Train: epoch 88\n",
      "Epoch: [88/900], step: [1/2479], loss: 2.531788\n",
      "Epoch: [88/900], step: [201/2479], loss: 1.444344\n",
      "Epoch: [88/900], step: [401/2479], loss: 1.806976\n",
      "Epoch: [88/900], step: [601/2479], loss: 1.781448\n",
      "Epoch: [88/900], step: [801/2479], loss: 2.293718\n",
      "Epoch: [88/900], step: [1001/2479], loss: 1.459966\n",
      "Epoch: [88/900], step: [1201/2479], loss: 1.919308\n",
      "Epoch: [88/900], step: [1401/2479], loss: 1.488552\n",
      "Epoch: [88/900], step: [1601/2479], loss: 1.217637\n",
      "Epoch: [88/900], step: [1801/2479], loss: 1.446921\n",
      "Epoch: [88/900], step: [2001/2479], loss: 2.906174\n",
      "Epoch: [88/900], step: [2201/2479], loss: 1.626540\n",
      "Epoch: [88/900], step: [2401/2479], loss: 1.671097\n",
      "Test: epoch 88 loss: 5.4607058\n",
      "Train: epoch 89\n",
      "Epoch: [89/900], step: [1/2479], loss: 1.077876\n",
      "Epoch: [89/900], step: [201/2479], loss: 1.485887\n",
      "Epoch: [89/900], step: [401/2479], loss: 1.756130\n",
      "Epoch: [89/900], step: [601/2479], loss: 1.460391\n",
      "Epoch: [89/900], step: [801/2479], loss: 1.908725\n",
      "Epoch: [89/900], step: [1001/2479], loss: 1.566424\n",
      "Epoch: [89/900], step: [1201/2479], loss: 1.736388\n",
      "Epoch: [89/900], step: [1401/2479], loss: 1.774060\n",
      "Epoch: [89/900], step: [1601/2479], loss: 2.010826\n",
      "Epoch: [89/900], step: [1801/2479], loss: 1.618357\n",
      "Epoch: [89/900], step: [2001/2479], loss: 2.827849\n",
      "Epoch: [89/900], step: [2201/2479], loss: 2.328731\n",
      "Epoch: [89/900], step: [2401/2479], loss: 1.940527\n",
      "Test: epoch 89 loss: 6.7879224\n",
      "Train: epoch 90\n",
      "Epoch: [90/900], step: [1/2479], loss: 2.356544\n",
      "Epoch: [90/900], step: [201/2479], loss: 2.114496\n",
      "Epoch: [90/900], step: [401/2479], loss: 2.197515\n",
      "Epoch: [90/900], step: [601/2479], loss: 1.921902\n",
      "Epoch: [90/900], step: [801/2479], loss: 1.369273\n",
      "Epoch: [90/900], step: [1001/2479], loss: 2.581565\n",
      "Epoch: [90/900], step: [1201/2479], loss: 2.257051\n",
      "Epoch: [90/900], step: [1401/2479], loss: 2.003004\n",
      "Epoch: [90/900], step: [1601/2479], loss: 1.896435\n",
      "Epoch: [90/900], step: [1801/2479], loss: 2.060459\n",
      "Epoch: [90/900], step: [2001/2479], loss: 2.025610\n",
      "Epoch: [90/900], step: [2201/2479], loss: 1.807396\n",
      "Epoch: [90/900], step: [2401/2479], loss: 2.495388\n",
      "Test: epoch 90 loss: 6.9186997\n",
      "Train: epoch 91\n",
      "Epoch: [91/900], step: [1/2479], loss: 2.188521\n",
      "Epoch: [91/900], step: [201/2479], loss: 2.711780\n",
      "Epoch: [91/900], step: [401/2479], loss: 1.923850\n",
      "Epoch: [91/900], step: [601/2479], loss: 1.511346\n",
      "Epoch: [91/900], step: [801/2479], loss: 1.856609\n",
      "Epoch: [91/900], step: [1001/2479], loss: 1.603540\n",
      "Epoch: [91/900], step: [1201/2479], loss: 1.823114\n",
      "Epoch: [91/900], step: [1401/2479], loss: 1.726202\n",
      "Epoch: [91/900], step: [1601/2479], loss: 1.501358\n",
      "Epoch: [91/900], step: [1801/2479], loss: 1.973209\n",
      "Epoch: [91/900], step: [2001/2479], loss: 2.174481\n",
      "Epoch: [91/900], step: [2201/2479], loss: 1.823832\n",
      "Epoch: [91/900], step: [2401/2479], loss: 2.076743\n",
      "Test: epoch 91 loss: 5.292173\n",
      "Train: epoch 92\n",
      "Epoch: [92/900], step: [1/2479], loss: 2.295414\n",
      "Epoch: [92/900], step: [201/2479], loss: 2.049361\n",
      "Epoch: [92/900], step: [401/2479], loss: 2.483467\n",
      "Epoch: [92/900], step: [601/2479], loss: 1.696086\n",
      "Epoch: [92/900], step: [801/2479], loss: 2.384758\n",
      "Epoch: [92/900], step: [1001/2479], loss: 2.437702\n",
      "Epoch: [92/900], step: [1201/2479], loss: 2.327592\n",
      "Epoch: [92/900], step: [1401/2479], loss: 1.355614\n",
      "Epoch: [92/900], step: [1601/2479], loss: 2.247825\n",
      "Epoch: [92/900], step: [1801/2479], loss: 1.790511\n",
      "Epoch: [92/900], step: [2001/2479], loss: 1.479631\n",
      "Epoch: [92/900], step: [2201/2479], loss: 2.056197\n",
      "Epoch: [92/900], step: [2401/2479], loss: 1.765932\n",
      "Test: epoch 92 loss: 6.8405733\n",
      "Train: epoch 93\n",
      "Epoch: [93/900], step: [1/2479], loss: 1.860753\n",
      "Epoch: [93/900], step: [201/2479], loss: 1.920261\n",
      "Epoch: [93/900], step: [401/2479], loss: 1.790804\n",
      "Epoch: [93/900], step: [601/2479], loss: 1.813828\n",
      "Epoch: [93/900], step: [801/2479], loss: 1.826195\n",
      "Epoch: [93/900], step: [1001/2479], loss: 1.368333\n",
      "Epoch: [93/900], step: [1201/2479], loss: 2.175492\n",
      "Epoch: [93/900], step: [1401/2479], loss: 2.100978\n",
      "Epoch: [93/900], step: [1601/2479], loss: 1.886817\n",
      "Epoch: [93/900], step: [1801/2479], loss: 1.199657\n",
      "Epoch: [93/900], step: [2001/2479], loss: 2.306624\n",
      "Epoch: [93/900], step: [2201/2479], loss: 2.018142\n",
      "Epoch: [93/900], step: [2401/2479], loss: 1.838951\n",
      "Test: epoch 93 loss: 5.6925035\n",
      "Train: epoch 94\n",
      "Epoch: [94/900], step: [1/2479], loss: 1.523828\n",
      "Epoch: [94/900], step: [201/2479], loss: 1.641600\n",
      "Epoch: [94/900], step: [401/2479], loss: 2.624120\n",
      "Epoch: [94/900], step: [601/2479], loss: 1.243162\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [94/900], step: [801/2479], loss: 2.394591\n",
      "Epoch: [94/900], step: [1001/2479], loss: 2.156112\n",
      "Epoch: [94/900], step: [1201/2479], loss: 2.102726\n",
      "Epoch: [94/900], step: [1401/2479], loss: 2.430955\n",
      "Epoch: [94/900], step: [1601/2479], loss: 2.045424\n",
      "Epoch: [94/900], step: [1801/2479], loss: 2.265533\n",
      "Epoch: [94/900], step: [2001/2479], loss: 2.345140\n",
      "Epoch: [94/900], step: [2201/2479], loss: 1.973570\n",
      "Epoch: [94/900], step: [2401/2479], loss: 2.319056\n",
      "Test: epoch 94 loss: 6.096442\n",
      "Train: epoch 95\n",
      "Epoch: [95/900], step: [1/2479], loss: 1.699121\n",
      "Epoch: [95/900], step: [201/2479], loss: 1.672376\n",
      "Epoch: [95/900], step: [401/2479], loss: 2.113259\n",
      "Epoch: [95/900], step: [601/2479], loss: 2.036323\n",
      "Epoch: [95/900], step: [801/2479], loss: 1.555509\n",
      "Epoch: [95/900], step: [1001/2479], loss: 1.652851\n",
      "Epoch: [95/900], step: [1201/2479], loss: 2.241258\n",
      "Epoch: [95/900], step: [1401/2479], loss: 2.380244\n",
      "Epoch: [95/900], step: [1601/2479], loss: 1.878907\n",
      "Epoch: [95/900], step: [1801/2479], loss: 1.862926\n",
      "Epoch: [95/900], step: [2001/2479], loss: 2.750830\n",
      "Epoch: [95/900], step: [2201/2479], loss: 2.036304\n",
      "Epoch: [95/900], step: [2401/2479], loss: 2.320250\n",
      "Test: epoch 95 loss: 3.9243598\n",
      "Train: epoch 96\n",
      "Epoch: [96/900], step: [1/2479], loss: 1.543624\n",
      "Epoch: [96/900], step: [201/2479], loss: 1.165317\n",
      "Epoch: [96/900], step: [401/2479], loss: 1.400277\n",
      "Epoch: [96/900], step: [601/2479], loss: 1.695795\n",
      "Epoch: [96/900], step: [801/2479], loss: 1.405559\n",
      "Epoch: [96/900], step: [1001/2479], loss: 1.985540\n",
      "Epoch: [96/900], step: [1201/2479], loss: 1.724535\n",
      "Epoch: [96/900], step: [1401/2479], loss: 2.072826\n",
      "Epoch: [96/900], step: [1601/2479], loss: 0.922111\n",
      "Epoch: [96/900], step: [1801/2479], loss: 2.053348\n",
      "Epoch: [96/900], step: [2001/2479], loss: 1.956359\n",
      "Epoch: [96/900], step: [2201/2479], loss: 2.041781\n",
      "Epoch: [96/900], step: [2401/2479], loss: 1.775974\n",
      "Test: epoch 96 loss: 3.684697\n",
      "Train: epoch 97\n",
      "Epoch: [97/900], step: [1/2479], loss: 1.415470\n",
      "Epoch: [97/900], step: [201/2479], loss: 1.984692\n",
      "Epoch: [97/900], step: [401/2479], loss: 1.969487\n",
      "Epoch: [97/900], step: [601/2479], loss: 1.469121\n",
      "Epoch: [97/900], step: [801/2479], loss: 2.970015\n",
      "Epoch: [97/900], step: [1001/2479], loss: 1.437759\n",
      "Epoch: [97/900], step: [1201/2479], loss: 2.016114\n",
      "Epoch: [97/900], step: [1401/2479], loss: 1.134490\n",
      "Epoch: [97/900], step: [1601/2479], loss: 1.751488\n",
      "Epoch: [97/900], step: [1801/2479], loss: 1.712237\n",
      "Epoch: [97/900], step: [2001/2479], loss: 3.056617\n",
      "Epoch: [97/900], step: [2201/2479], loss: 1.539493\n",
      "Epoch: [97/900], step: [2401/2479], loss: 1.536230\n",
      "Test: epoch 97 loss: 9.097198\n",
      "Train: epoch 98\n",
      "Epoch: [98/900], step: [1/2479], loss: 1.763021\n",
      "Epoch: [98/900], step: [201/2479], loss: 2.024798\n",
      "Epoch: [98/900], step: [401/2479], loss: 1.487838\n",
      "Epoch: [98/900], step: [601/2479], loss: 1.527649\n",
      "Epoch: [98/900], step: [801/2479], loss: 1.724239\n",
      "Epoch: [98/900], step: [1001/2479], loss: 2.090724\n",
      "Epoch: [98/900], step: [1201/2479], loss: 1.886511\n",
      "Epoch: [98/900], step: [1401/2479], loss: 1.891867\n",
      "Epoch: [98/900], step: [1601/2479], loss: 1.577291\n",
      "Epoch: [98/900], step: [1801/2479], loss: 1.923062\n",
      "Epoch: [98/900], step: [2001/2479], loss: 2.837022\n",
      "Epoch: [98/900], step: [2201/2479], loss: 2.149643\n",
      "Epoch: [98/900], step: [2401/2479], loss: 1.841004\n",
      "Test: epoch 98 loss: 7.44266\n",
      "Train: epoch 99\n",
      "Epoch: [99/900], step: [1/2479], loss: 1.810557\n",
      "Epoch: [99/900], step: [201/2479], loss: 1.560625\n",
      "Epoch: [99/900], step: [401/2479], loss: 2.860817\n",
      "Epoch: [99/900], step: [601/2479], loss: 1.189282\n",
      "Epoch: [99/900], step: [801/2479], loss: 2.124333\n",
      "Epoch: [99/900], step: [1001/2479], loss: 2.040696\n",
      "Epoch: [99/900], step: [1201/2479], loss: 1.915936\n",
      "Epoch: [99/900], step: [1401/2479], loss: 1.931093\n",
      "Epoch: [99/900], step: [1601/2479], loss: 2.146577\n",
      "Epoch: [99/900], step: [1801/2479], loss: 1.910916\n",
      "Epoch: [99/900], step: [2001/2479], loss: 2.266808\n",
      "Epoch: [99/900], step: [2201/2479], loss: 2.449441\n",
      "Epoch: [99/900], step: [2401/2479], loss: 1.639433\n",
      "Test: epoch 99 loss: 4.6036057\n",
      "Train: epoch 100\n",
      "Epoch: [100/900], step: [1/2479], loss: 1.570566\n",
      "Epoch: [100/900], step: [201/2479], loss: 1.625282\n",
      "Epoch: [100/900], step: [401/2479], loss: 1.321732\n",
      "Epoch: [100/900], step: [601/2479], loss: 2.180938\n",
      "Epoch: [100/900], step: [801/2479], loss: 1.354813\n",
      "Epoch: [100/900], step: [1001/2479], loss: 2.365794\n",
      "Epoch: [100/900], step: [1201/2479], loss: 1.648818\n",
      "Epoch: [100/900], step: [1401/2479], loss: 2.177127\n",
      "Epoch: [100/900], step: [1601/2479], loss: 2.093899\n",
      "Epoch: [100/900], step: [1801/2479], loss: 2.101559\n",
      "Epoch: [100/900], step: [2001/2479], loss: 2.177426\n",
      "Epoch: [100/900], step: [2201/2479], loss: 1.586446\n",
      "Epoch: [100/900], step: [2401/2479], loss: 1.780829\n",
      "Test: epoch 100 loss: 7.066058\n",
      "Train: epoch 101\n",
      "Epoch: [101/900], step: [1/2479], loss: 1.120613\n",
      "Epoch: [101/900], step: [201/2479], loss: 1.606035\n",
      "Epoch: [101/900], step: [401/2479], loss: 1.479095\n",
      "Epoch: [101/900], step: [601/2479], loss: 1.790470\n",
      "Epoch: [101/900], step: [801/2479], loss: 1.282493\n",
      "Epoch: [101/900], step: [1001/2479], loss: 2.130246\n",
      "Epoch: [101/900], step: [1201/2479], loss: 1.891044\n",
      "Epoch: [101/900], step: [1401/2479], loss: 2.114159\n",
      "Epoch: [101/900], step: [1601/2479], loss: 2.129644\n",
      "Epoch: [101/900], step: [1801/2479], loss: 1.428946\n",
      "Epoch: [101/900], step: [2001/2479], loss: 1.341108\n",
      "Epoch: [101/900], step: [2201/2479], loss: 2.027006\n",
      "Epoch: [101/900], step: [2401/2479], loss: 1.536495\n",
      "Test: epoch 101 loss: 5.585691\n",
      "Train: epoch 102\n",
      "Epoch: [102/900], step: [1/2479], loss: 1.632077\n",
      "Epoch: [102/900], step: [201/2479], loss: 1.300626\n",
      "Epoch: [102/900], step: [401/2479], loss: 1.332435\n",
      "Epoch: [102/900], step: [601/2479], loss: 1.711501\n",
      "Epoch: [102/900], step: [801/2479], loss: 1.888062\n",
      "Epoch: [102/900], step: [1001/2479], loss: 1.800032\n",
      "Epoch: [102/900], step: [1201/2479], loss: 2.411049\n",
      "Epoch: [102/900], step: [1401/2479], loss: 1.930272\n",
      "Epoch: [102/900], step: [1601/2479], loss: 1.652325\n",
      "Epoch: [102/900], step: [1801/2479], loss: 2.018992\n",
      "Epoch: [102/900], step: [2001/2479], loss: 1.672398\n",
      "Epoch: [102/900], step: [2201/2479], loss: 1.351316\n",
      "Epoch: [102/900], step: [2401/2479], loss: 2.602949\n",
      "Test: epoch 102 loss: 5.9802194\n",
      "Train: epoch 103\n",
      "Epoch: [103/900], step: [1/2479], loss: 1.785599\n",
      "Epoch: [103/900], step: [201/2479], loss: 1.208830\n",
      "Epoch: [103/900], step: [401/2479], loss: 1.994343\n",
      "Epoch: [103/900], step: [601/2479], loss: 1.728972\n",
      "Epoch: [103/900], step: [801/2479], loss: 2.052024\n",
      "Epoch: [103/900], step: [1001/2479], loss: 1.955254\n",
      "Epoch: [103/900], step: [1201/2479], loss: 2.449340\n",
      "Epoch: [103/900], step: [1401/2479], loss: 1.609743\n",
      "Epoch: [103/900], step: [1601/2479], loss: 1.412298\n",
      "Epoch: [103/900], step: [1801/2479], loss: 2.227477\n",
      "Epoch: [103/900], step: [2001/2479], loss: 1.847449\n",
      "Epoch: [103/900], step: [2201/2479], loss: 1.992076\n",
      "Epoch: [103/900], step: [2401/2479], loss: 2.162056\n",
      "Test: epoch 103 loss: 7.005364\n",
      "Train: epoch 104\n",
      "Epoch: [104/900], step: [1/2479], loss: 1.570286\n",
      "Epoch: [104/900], step: [201/2479], loss: 1.092053\n",
      "Epoch: [104/900], step: [401/2479], loss: 1.717869\n",
      "Epoch: [104/900], step: [601/2479], loss: 1.247527\n",
      "Epoch: [104/900], step: [801/2479], loss: 1.904635\n",
      "Epoch: [104/900], step: [1001/2479], loss: 1.774417\n",
      "Epoch: [104/900], step: [1201/2479], loss: 1.451946\n",
      "Epoch: [104/900], step: [1401/2479], loss: 1.051942\n",
      "Epoch: [104/900], step: [1601/2479], loss: 1.870527\n",
      "Epoch: [104/900], step: [1801/2479], loss: 1.928527\n",
      "Epoch: [104/900], step: [2001/2479], loss: 1.962263\n",
      "Epoch: [104/900], step: [2201/2479], loss: 1.832567\n",
      "Epoch: [104/900], step: [2401/2479], loss: 2.750012\n",
      "Test: epoch 104 loss: 7.064008\n",
      "Train: epoch 105\n",
      "Epoch: [105/900], step: [1/2479], loss: 1.412493\n",
      "Epoch: [105/900], step: [201/2479], loss: 1.694150\n",
      "Epoch: [105/900], step: [401/2479], loss: 1.652604\n",
      "Epoch: [105/900], step: [601/2479], loss: 1.635330\n",
      "Epoch: [105/900], step: [801/2479], loss: 1.977955\n",
      "Epoch: [105/900], step: [1001/2479], loss: 1.736816\n",
      "Epoch: [105/900], step: [1201/2479], loss: 1.725927\n",
      "Epoch: [105/900], step: [1401/2479], loss: 1.409400\n",
      "Epoch: [105/900], step: [1601/2479], loss: 1.487053\n",
      "Epoch: [105/900], step: [1801/2479], loss: 1.858801\n",
      "Epoch: [105/900], step: [2001/2479], loss: 1.530739\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [105/900], step: [2201/2479], loss: 1.369045\n",
      "Epoch: [105/900], step: [2401/2479], loss: 1.864955\n",
      "Test: epoch 105 loss: 6.347972\n",
      "Train: epoch 106\n",
      "Epoch: [106/900], step: [1/2479], loss: 2.275174\n",
      "Epoch: [106/900], step: [201/2479], loss: 1.804294\n",
      "Epoch: [106/900], step: [401/2479], loss: 2.060491\n",
      "Epoch: [106/900], step: [601/2479], loss: 1.959521\n",
      "Epoch: [106/900], step: [801/2479], loss: 1.738414\n",
      "Epoch: [106/900], step: [1001/2479], loss: 1.956438\n",
      "Epoch: [106/900], step: [1201/2479], loss: 1.335440\n",
      "Epoch: [106/900], step: [1401/2479], loss: 1.327876\n",
      "Epoch: [106/900], step: [1601/2479], loss: 1.369149\n",
      "Epoch: [106/900], step: [1801/2479], loss: 1.088819\n",
      "Epoch: [106/900], step: [2001/2479], loss: 1.769154\n",
      "Epoch: [106/900], step: [2201/2479], loss: 2.256861\n",
      "Epoch: [106/900], step: [2401/2479], loss: 2.288472\n",
      "Test: epoch 106 loss: 6.2445993\n",
      "Train: epoch 107\n",
      "Epoch: [107/900], step: [1/2479], loss: 1.673892\n",
      "Epoch: [107/900], step: [201/2479], loss: 2.046188\n",
      "Epoch: [107/900], step: [401/2479], loss: 2.083192\n",
      "Epoch: [107/900], step: [601/2479], loss: 2.164499\n",
      "Epoch: [107/900], step: [801/2479], loss: 1.940531\n",
      "Epoch: [107/900], step: [1001/2479], loss: 1.797268\n",
      "Epoch: [107/900], step: [1201/2479], loss: 1.842373\n",
      "Epoch: [107/900], step: [1401/2479], loss: 2.426326\n",
      "Epoch: [107/900], step: [1601/2479], loss: 1.398653\n",
      "Epoch: [107/900], step: [1801/2479], loss: 1.830684\n",
      "Epoch: [107/900], step: [2001/2479], loss: 1.599377\n",
      "Epoch: [107/900], step: [2201/2479], loss: 2.137907\n",
      "Epoch: [107/900], step: [2401/2479], loss: 1.932964\n",
      "Test: epoch 107 loss: 5.783153\n",
      "Train: epoch 108\n",
      "Epoch: [108/900], step: [1/2479], loss: 2.043437\n",
      "Epoch: [108/900], step: [201/2479], loss: 2.206242\n",
      "Epoch: [108/900], step: [401/2479], loss: 1.547767\n",
      "Epoch: [108/900], step: [601/2479], loss: 2.220188\n",
      "Epoch: [108/900], step: [801/2479], loss: 1.957150\n",
      "Epoch: [108/900], step: [1001/2479], loss: 2.241628\n",
      "Epoch: [108/900], step: [1201/2479], loss: 2.018714\n",
      "Epoch: [108/900], step: [1401/2479], loss: 1.549725\n",
      "Epoch: [108/900], step: [1601/2479], loss: 2.409650\n",
      "Epoch: [108/900], step: [1801/2479], loss: 1.340301\n",
      "Epoch: [108/900], step: [2001/2479], loss: 1.861670\n",
      "Epoch: [108/900], step: [2201/2479], loss: 1.421111\n",
      "Epoch: [108/900], step: [2401/2479], loss: 1.640334\n",
      "Test: epoch 108 loss: 5.027272\n",
      "Train: epoch 109\n",
      "Epoch: [109/900], step: [1/2479], loss: 1.663846\n",
      "Epoch: [109/900], step: [201/2479], loss: 1.449765\n",
      "Epoch: [109/900], step: [401/2479], loss: 1.538071\n",
      "Epoch: [109/900], step: [601/2479], loss: 2.198353\n",
      "Epoch: [109/900], step: [801/2479], loss: 1.488128\n",
      "Epoch: [109/900], step: [1001/2479], loss: 1.849967\n",
      "Epoch: [109/900], step: [1201/2479], loss: 1.028987\n",
      "Epoch: [109/900], step: [1401/2479], loss: 2.297233\n",
      "Epoch: [109/900], step: [1601/2479], loss: 1.933163\n",
      "Epoch: [109/900], step: [1801/2479], loss: 1.345306\n",
      "Epoch: [109/900], step: [2001/2479], loss: 1.913403\n",
      "Epoch: [109/900], step: [2201/2479], loss: 1.779323\n",
      "Epoch: [109/900], step: [2401/2479], loss: 1.419600\n",
      "Test: epoch 109 loss: 8.006144\n",
      "Train: epoch 110\n",
      "Epoch: [110/900], step: [1/2479], loss: 1.454189\n",
      "Epoch: [110/900], step: [201/2479], loss: 1.921327\n",
      "Epoch: [110/900], step: [401/2479], loss: 1.690074\n",
      "Epoch: [110/900], step: [601/2479], loss: 2.086334\n",
      "Epoch: [110/900], step: [801/2479], loss: 1.445224\n",
      "Epoch: [110/900], step: [1001/2479], loss: 2.244002\n",
      "Epoch: [110/900], step: [1201/2479], loss: 1.876302\n",
      "Epoch: [110/900], step: [1401/2479], loss: 2.279310\n",
      "Epoch: [110/900], step: [1601/2479], loss: 2.356725\n",
      "Epoch: [110/900], step: [1801/2479], loss: 1.616993\n",
      "Epoch: [110/900], step: [2001/2479], loss: 1.794860\n",
      "Epoch: [110/900], step: [2201/2479], loss: 2.081236\n",
      "Epoch: [110/900], step: [2401/2479], loss: 2.174969\n",
      "Test: epoch 110 loss: 5.6155562\n",
      "Train: epoch 111\n",
      "Epoch: [111/900], step: [1/2479], loss: 1.743990\n",
      "Epoch: [111/900], step: [201/2479], loss: 2.040715\n",
      "Epoch: [111/900], step: [401/2479], loss: 1.701884\n",
      "Epoch: [111/900], step: [601/2479], loss: 1.904933\n",
      "Epoch: [111/900], step: [801/2479], loss: 1.293866\n",
      "Epoch: [111/900], step: [1001/2479], loss: 0.897049\n",
      "Epoch: [111/900], step: [1201/2479], loss: 1.806344\n",
      "Epoch: [111/900], step: [1401/2479], loss: 1.609616\n",
      "Epoch: [111/900], step: [1601/2479], loss: 2.077250\n",
      "Epoch: [111/900], step: [1801/2479], loss: 1.661329\n",
      "Epoch: [111/900], step: [2001/2479], loss: 2.238377\n",
      "Epoch: [111/900], step: [2201/2479], loss: 1.693928\n",
      "Epoch: [111/900], step: [2401/2479], loss: 1.381198\n",
      "Test: epoch 111 loss: 5.5904284\n",
      "Train: epoch 112\n",
      "Epoch: [112/900], step: [1/2479], loss: 1.825248\n",
      "Epoch: [112/900], step: [201/2479], loss: 1.888977\n",
      "Epoch: [112/900], step: [401/2479], loss: 2.257648\n",
      "Epoch: [112/900], step: [601/2479], loss: 2.415709\n",
      "Epoch: [112/900], step: [801/2479], loss: 2.608161\n",
      "Epoch: [112/900], step: [1001/2479], loss: 1.164735\n",
      "Epoch: [112/900], step: [1201/2479], loss: 1.662171\n",
      "Epoch: [112/900], step: [1401/2479], loss: 2.278126\n",
      "Epoch: [112/900], step: [1601/2479], loss: 1.996825\n",
      "Epoch: [112/900], step: [1801/2479], loss: 0.795181\n",
      "Epoch: [112/900], step: [2001/2479], loss: 1.661531\n",
      "Epoch: [112/900], step: [2201/2479], loss: 1.919845\n",
      "Epoch: [112/900], step: [2401/2479], loss: 2.448241\n",
      "Test: epoch 112 loss: 6.4286017\n",
      "Train: epoch 113\n",
      "Epoch: [113/900], step: [1/2479], loss: 1.807321\n",
      "Epoch: [113/900], step: [201/2479], loss: 1.115762\n",
      "Epoch: [113/900], step: [401/2479], loss: 1.162035\n",
      "Epoch: [113/900], step: [601/2479], loss: 1.989079\n",
      "Epoch: [113/900], step: [801/2479], loss: 2.212590\n",
      "Epoch: [113/900], step: [1001/2479], loss: 1.910098\n",
      "Epoch: [113/900], step: [1201/2479], loss: 2.288113\n",
      "Epoch: [113/900], step: [1401/2479], loss: 2.233191\n",
      "Epoch: [113/900], step: [1601/2479], loss: 0.771663\n",
      "Epoch: [113/900], step: [1801/2479], loss: 1.757878\n",
      "Epoch: [113/900], step: [2001/2479], loss: 1.568990\n",
      "Epoch: [113/900], step: [2201/2479], loss: 1.377010\n",
      "Epoch: [113/900], step: [2401/2479], loss: 1.362733\n",
      "Test: epoch 113 loss: 6.815789\n",
      "Train: epoch 114\n",
      "Epoch: [114/900], step: [1/2479], loss: 2.151453\n",
      "Epoch: [114/900], step: [201/2479], loss: 1.744069\n",
      "Epoch: [114/900], step: [401/2479], loss: 1.530796\n",
      "Epoch: [114/900], step: [601/2479], loss: 2.160375\n",
      "Epoch: [114/900], step: [801/2479], loss: 1.944468\n",
      "Epoch: [114/900], step: [1001/2479], loss: 2.561048\n",
      "Epoch: [114/900], step: [1201/2479], loss: 1.988178\n",
      "Epoch: [114/900], step: [1401/2479], loss: 2.087282\n",
      "Epoch: [114/900], step: [1601/2479], loss: 1.933838\n",
      "Epoch: [114/900], step: [1801/2479], loss: 1.766125\n",
      "Epoch: [114/900], step: [2001/2479], loss: 1.483601\n",
      "Epoch: [114/900], step: [2201/2479], loss: 1.653448\n",
      "Epoch: [114/900], step: [2401/2479], loss: 1.809481\n",
      "Test: epoch 114 loss: 5.565206\n",
      "Train: epoch 115\n",
      "Epoch: [115/900], step: [1/2479], loss: 1.247714\n",
      "Epoch: [115/900], step: [201/2479], loss: 1.285034\n",
      "Epoch: [115/900], step: [401/2479], loss: 2.119358\n",
      "Epoch: [115/900], step: [601/2479], loss: 1.779644\n",
      "Epoch: [115/900], step: [801/2479], loss: 1.782633\n",
      "Epoch: [115/900], step: [1001/2479], loss: 1.748687\n",
      "Epoch: [115/900], step: [1201/2479], loss: 2.298065\n",
      "Epoch: [115/900], step: [1401/2479], loss: 2.218390\n",
      "Epoch: [115/900], step: [1601/2479], loss: 1.661200\n",
      "Epoch: [115/900], step: [1801/2479], loss: 2.354575\n",
      "Epoch: [115/900], step: [2001/2479], loss: 1.941723\n",
      "Epoch: [115/900], step: [2201/2479], loss: 1.262680\n",
      "Epoch: [115/900], step: [2401/2479], loss: 1.749388\n",
      "Test: epoch 115 loss: 4.4803023\n",
      "Train: epoch 116\n",
      "Epoch: [116/900], step: [1/2479], loss: 2.287522\n",
      "Epoch: [116/900], step: [201/2479], loss: 1.493995\n",
      "Epoch: [116/900], step: [401/2479], loss: 1.708082\n",
      "Epoch: [116/900], step: [601/2479], loss: 1.679710\n",
      "Epoch: [116/900], step: [801/2479], loss: 1.554523\n",
      "Epoch: [116/900], step: [1001/2479], loss: 2.465807\n",
      "Epoch: [116/900], step: [1201/2479], loss: 1.505104\n",
      "Epoch: [116/900], step: [1401/2479], loss: 2.385852\n",
      "Epoch: [116/900], step: [1601/2479], loss: 1.044799\n",
      "Epoch: [116/900], step: [1801/2479], loss: 1.740700\n",
      "Epoch: [116/900], step: [2001/2479], loss: 2.223632\n",
      "Epoch: [116/900], step: [2201/2479], loss: 1.691146\n",
      "Epoch: [116/900], step: [2401/2479], loss: 1.762712\n",
      "Test: epoch 116 loss: 5.462716\n",
      "Train: epoch 117\n",
      "Epoch: [117/900], step: [1/2479], loss: 1.603123\n",
      "Epoch: [117/900], step: [201/2479], loss: 1.965063\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [117/900], step: [401/2479], loss: 2.242872\n",
      "Epoch: [117/900], step: [601/2479], loss: 1.932852\n",
      "Epoch: [117/900], step: [801/2479], loss: 1.622887\n",
      "Epoch: [117/900], step: [1001/2479], loss: 1.906115\n",
      "Epoch: [117/900], step: [1201/2479], loss: 1.127891\n",
      "Epoch: [117/900], step: [1401/2479], loss: 1.297666\n",
      "Epoch: [117/900], step: [1601/2479], loss: 1.678508\n",
      "Epoch: [117/900], step: [1801/2479], loss: 1.881782\n",
      "Epoch: [117/900], step: [2001/2479], loss: 2.012921\n",
      "Epoch: [117/900], step: [2201/2479], loss: 2.001248\n",
      "Epoch: [117/900], step: [2401/2479], loss: 1.957415\n",
      "Test: epoch 117 loss: 7.4489822\n",
      "Train: epoch 118\n",
      "Epoch: [118/900], step: [1/2479], loss: 2.143893\n",
      "Epoch: [118/900], step: [201/2479], loss: 2.846869\n",
      "Epoch: [118/900], step: [401/2479], loss: 1.936080\n",
      "Epoch: [118/900], step: [601/2479], loss: 1.874600\n",
      "Epoch: [118/900], step: [801/2479], loss: 2.392325\n",
      "Epoch: [118/900], step: [1001/2479], loss: 2.198176\n",
      "Epoch: [118/900], step: [1201/2479], loss: 2.661246\n",
      "Epoch: [118/900], step: [1401/2479], loss: 1.805281\n",
      "Epoch: [118/900], step: [1601/2479], loss: 2.091156\n",
      "Epoch: [118/900], step: [1801/2479], loss: 1.379836\n",
      "Epoch: [118/900], step: [2001/2479], loss: 1.792406\n",
      "Epoch: [118/900], step: [2201/2479], loss: 2.361827\n",
      "Epoch: [118/900], step: [2401/2479], loss: 2.561544\n",
      "Test: epoch 118 loss: 6.6217737\n",
      "Train: epoch 119\n",
      "Epoch: [119/900], step: [1/2479], loss: 2.319482\n",
      "Epoch: [119/900], step: [201/2479], loss: 1.612183\n",
      "Epoch: [119/900], step: [401/2479], loss: 2.238101\n",
      "Epoch: [119/900], step: [601/2479], loss: 1.730448\n",
      "Epoch: [119/900], step: [801/2479], loss: 1.904048\n",
      "Epoch: [119/900], step: [1001/2479], loss: 1.981822\n",
      "Epoch: [119/900], step: [1201/2479], loss: 1.760803\n",
      "Epoch: [119/900], step: [1401/2479], loss: 1.275185\n",
      "Epoch: [119/900], step: [1601/2479], loss: 2.074978\n",
      "Epoch: [119/900], step: [1801/2479], loss: 1.705187\n",
      "Epoch: [119/900], step: [2001/2479], loss: 2.155141\n",
      "Epoch: [119/900], step: [2201/2479], loss: 1.995837\n",
      "Epoch: [119/900], step: [2401/2479], loss: 1.676458\n",
      "Test: epoch 119 loss: 3.1864183\n",
      "Train: epoch 120\n",
      "Epoch: [120/900], step: [1/2479], loss: 1.328688\n",
      "Epoch: [120/900], step: [201/2479], loss: 2.391605\n",
      "Epoch: [120/900], step: [401/2479], loss: 1.275138\n",
      "Epoch: [120/900], step: [601/2479], loss: 2.070137\n",
      "Epoch: [120/900], step: [801/2479], loss: 1.475103\n",
      "Epoch: [120/900], step: [1001/2479], loss: 1.700454\n",
      "Epoch: [120/900], step: [1201/2479], loss: 1.960945\n",
      "Epoch: [120/900], step: [1401/2479], loss: 1.580105\n",
      "Epoch: [120/900], step: [1601/2479], loss: 1.560027\n",
      "Epoch: [120/900], step: [1801/2479], loss: 1.911424\n",
      "Epoch: [120/900], step: [2001/2479], loss: 2.118409\n",
      "Epoch: [120/900], step: [2201/2479], loss: 2.214886\n",
      "Epoch: [120/900], step: [2401/2479], loss: 1.846546\n",
      "Test: epoch 120 loss: 5.132847\n",
      "Train: epoch 121\n",
      "Epoch: [121/900], step: [1/2479], loss: 1.814267\n",
      "Epoch: [121/900], step: [201/2479], loss: 2.482122\n",
      "Epoch: [121/900], step: [401/2479], loss: 1.568641\n",
      "Epoch: [121/900], step: [601/2479], loss: 2.556937\n",
      "Epoch: [121/900], step: [801/2479], loss: 1.699124\n",
      "Epoch: [121/900], step: [1001/2479], loss: 1.741677\n",
      "Epoch: [121/900], step: [1201/2479], loss: 2.579506\n",
      "Epoch: [121/900], step: [1401/2479], loss: 1.810323\n",
      "Epoch: [121/900], step: [1601/2479], loss: 1.265982\n",
      "Epoch: [121/900], step: [1801/2479], loss: 2.168164\n",
      "Epoch: [121/900], step: [2001/2479], loss: 2.161831\n",
      "Epoch: [121/900], step: [2201/2479], loss: 1.771645\n",
      "Epoch: [121/900], step: [2401/2479], loss: 1.497999\n",
      "Test: epoch 121 loss: 7.1264853\n",
      "Train: epoch 122\n",
      "Epoch: [122/900], step: [1/2479], loss: 1.284135\n",
      "Epoch: [122/900], step: [201/2479], loss: 1.923952\n",
      "Epoch: [122/900], step: [401/2479], loss: 1.493075\n",
      "Epoch: [122/900], step: [601/2479], loss: 2.190018\n",
      "Epoch: [122/900], step: [801/2479], loss: 1.903737\n",
      "Epoch: [122/900], step: [1001/2479], loss: 2.015783\n",
      "Epoch: [122/900], step: [1201/2479], loss: 1.943054\n",
      "Epoch: [122/900], step: [1401/2479], loss: 1.504054\n",
      "Epoch: [122/900], step: [1601/2479], loss: 1.792899\n",
      "Epoch: [122/900], step: [1801/2479], loss: 2.244740\n",
      "Epoch: [122/900], step: [2001/2479], loss: 1.790282\n",
      "Epoch: [122/900], step: [2201/2479], loss: 1.660188\n",
      "Epoch: [122/900], step: [2401/2479], loss: 1.424480\n",
      "Test: epoch 122 loss: 8.528236\n",
      "Train: epoch 123\n",
      "Epoch: [123/900], step: [1/2479], loss: 1.434296\n",
      "Epoch: [123/900], step: [201/2479], loss: 2.043460\n",
      "Epoch: [123/900], step: [401/2479], loss: 1.649517\n",
      "Epoch: [123/900], step: [601/2479], loss: 1.996125\n",
      "Epoch: [123/900], step: [801/2479], loss: 2.112735\n",
      "Epoch: [123/900], step: [1001/2479], loss: 1.618697\n",
      "Epoch: [123/900], step: [1201/2479], loss: 2.472391\n",
      "Epoch: [123/900], step: [1401/2479], loss: 1.637530\n",
      "Epoch: [123/900], step: [1601/2479], loss: 1.825400\n",
      "Epoch: [123/900], step: [1801/2479], loss: 1.711403\n",
      "Epoch: [123/900], step: [2001/2479], loss: 1.599132\n",
      "Epoch: [123/900], step: [2201/2479], loss: 2.170573\n",
      "Epoch: [123/900], step: [2401/2479], loss: 2.302265\n",
      "Test: epoch 123 loss: 6.5037136\n",
      "Train: epoch 124\n",
      "Epoch: [124/900], step: [1/2479], loss: 1.056013\n",
      "Epoch: [124/900], step: [201/2479], loss: 2.500278\n",
      "Epoch: [124/900], step: [401/2479], loss: 1.568279\n",
      "Epoch: [124/900], step: [601/2479], loss: 1.972847\n",
      "Epoch: [124/900], step: [801/2479], loss: 1.109082\n",
      "Epoch: [124/900], step: [1001/2479], loss: 2.073880\n",
      "Epoch: [124/900], step: [1201/2479], loss: 2.082180\n",
      "Epoch: [124/900], step: [1401/2479], loss: 0.944925\n",
      "Epoch: [124/900], step: [1601/2479], loss: 1.614306\n",
      "Epoch: [124/900], step: [1801/2479], loss: 1.765233\n",
      "Epoch: [124/900], step: [2001/2479], loss: 2.112697\n",
      "Epoch: [124/900], step: [2201/2479], loss: 1.565402\n",
      "Epoch: [124/900], step: [2401/2479], loss: 1.719452\n",
      "Test: epoch 124 loss: 5.7992024\n",
      "Train: epoch 125\n",
      "Epoch: [125/900], step: [1/2479], loss: 1.559150\n",
      "Epoch: [125/900], step: [201/2479], loss: 1.792069\n",
      "Epoch: [125/900], step: [401/2479], loss: 1.770601\n",
      "Epoch: [125/900], step: [601/2479], loss: 1.955322\n",
      "Epoch: [125/900], step: [801/2479], loss: 1.289793\n",
      "Epoch: [125/900], step: [1001/2479], loss: 2.356695\n",
      "Epoch: [125/900], step: [1201/2479], loss: 1.535409\n",
      "Epoch: [125/900], step: [1401/2479], loss: 1.277909\n",
      "Epoch: [125/900], step: [1601/2479], loss: 2.138249\n",
      "Epoch: [125/900], step: [1801/2479], loss: 1.972157\n",
      "Epoch: [125/900], step: [2001/2479], loss: 1.952720\n",
      "Epoch: [125/900], step: [2201/2479], loss: 1.480325\n",
      "Epoch: [125/900], step: [2401/2479], loss: 1.798615\n",
      "Test: epoch 125 loss: 6.1415734\n",
      "Train: epoch 126\n",
      "Epoch: [126/900], step: [1/2479], loss: 2.103420\n",
      "Epoch: [126/900], step: [201/2479], loss: 1.463235\n",
      "Epoch: [126/900], step: [401/2479], loss: 1.486313\n",
      "Epoch: [126/900], step: [601/2479], loss: 1.262130\n",
      "Epoch: [126/900], step: [801/2479], loss: 1.655903\n",
      "Epoch: [126/900], step: [1001/2479], loss: 1.982680\n",
      "Epoch: [126/900], step: [1201/2479], loss: 2.067473\n",
      "Epoch: [126/900], step: [1401/2479], loss: 2.355863\n",
      "Epoch: [126/900], step: [1601/2479], loss: 2.535250\n",
      "Epoch: [126/900], step: [1801/2479], loss: 1.839651\n",
      "Epoch: [126/900], step: [2001/2479], loss: 1.276233\n",
      "Epoch: [126/900], step: [2201/2479], loss: 1.933385\n",
      "Epoch: [126/900], step: [2401/2479], loss: 1.341585\n",
      "Test: epoch 126 loss: 6.356338\n",
      "Train: epoch 127\n",
      "Epoch: [127/900], step: [1/2479], loss: 1.494689\n",
      "Epoch: [127/900], step: [201/2479], loss: 1.717299\n",
      "Epoch: [127/900], step: [401/2479], loss: 1.576339\n",
      "Epoch: [127/900], step: [601/2479], loss: 1.299535\n",
      "Epoch: [127/900], step: [801/2479], loss: 1.512670\n",
      "Epoch: [127/900], step: [1001/2479], loss: 1.999099\n",
      "Epoch: [127/900], step: [1201/2479], loss: 1.211633\n",
      "Epoch: [127/900], step: [1401/2479], loss: 1.694569\n",
      "Epoch: [127/900], step: [1601/2479], loss: 1.510174\n",
      "Epoch: [127/900], step: [1801/2479], loss: 2.246767\n",
      "Epoch: [127/900], step: [2001/2479], loss: 2.295321\n",
      "Epoch: [127/900], step: [2201/2479], loss: 1.896543\n",
      "Epoch: [127/900], step: [2401/2479], loss: 2.017149\n",
      "Test: epoch 127 loss: 7.168068\n",
      "Train: epoch 128\n",
      "Epoch: [128/900], step: [1/2479], loss: 1.578354\n",
      "Epoch: [128/900], step: [201/2479], loss: 1.347603\n",
      "Epoch: [128/900], step: [401/2479], loss: 1.765479\n",
      "Epoch: [128/900], step: [601/2479], loss: 1.909352\n",
      "Epoch: [128/900], step: [801/2479], loss: 1.530676\n",
      "Epoch: [128/900], step: [1001/2479], loss: 1.477676\n",
      "Epoch: [128/900], step: [1201/2479], loss: 1.675558\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [128/900], step: [1401/2479], loss: 1.323053\n",
      "Epoch: [128/900], step: [1601/2479], loss: 1.842431\n",
      "Epoch: [128/900], step: [1801/2479], loss: 1.776824\n",
      "Epoch: [128/900], step: [2001/2479], loss: 2.056471\n",
      "Epoch: [128/900], step: [2201/2479], loss: 1.718626\n",
      "Epoch: [128/900], step: [2401/2479], loss: 1.220021\n",
      "Test: epoch 128 loss: 5.0458107\n",
      "Train: epoch 129\n",
      "Epoch: [129/900], step: [1/2479], loss: 1.821203\n",
      "Epoch: [129/900], step: [201/2479], loss: 2.050339\n",
      "Epoch: [129/900], step: [401/2479], loss: 2.037117\n",
      "Epoch: [129/900], step: [601/2479], loss: 2.025604\n",
      "Epoch: [129/900], step: [801/2479], loss: 1.759601\n",
      "Epoch: [129/900], step: [1001/2479], loss: 1.521868\n",
      "Epoch: [129/900], step: [1201/2479], loss: 1.905104\n",
      "Epoch: [129/900], step: [1401/2479], loss: 1.501236\n",
      "Epoch: [129/900], step: [1601/2479], loss: 1.662072\n",
      "Epoch: [129/900], step: [1801/2479], loss: 1.796186\n",
      "Epoch: [129/900], step: [2001/2479], loss: 1.888135\n",
      "Epoch: [129/900], step: [2201/2479], loss: 2.007354\n",
      "Epoch: [129/900], step: [2401/2479], loss: 1.790076\n",
      "Test: epoch 129 loss: 7.227166\n",
      "Train: epoch 130\n",
      "Epoch: [130/900], step: [1/2479], loss: 1.422684\n",
      "Epoch: [130/900], step: [201/2479], loss: 2.157948\n",
      "Epoch: [130/900], step: [401/2479], loss: 1.817159\n",
      "Epoch: [130/900], step: [601/2479], loss: 1.508253\n",
      "Epoch: [130/900], step: [801/2479], loss: 1.532261\n",
      "Epoch: [130/900], step: [1001/2479], loss: 1.399787\n",
      "Epoch: [130/900], step: [1201/2479], loss: 2.060738\n",
      "Epoch: [130/900], step: [1401/2479], loss: 1.555721\n",
      "Epoch: [130/900], step: [1601/2479], loss: 2.011994\n",
      "Epoch: [130/900], step: [1801/2479], loss: 1.954398\n",
      "Epoch: [130/900], step: [2001/2479], loss: 1.446057\n",
      "Epoch: [130/900], step: [2201/2479], loss: 1.969397\n",
      "Epoch: [130/900], step: [2401/2479], loss: 1.789925\n",
      "Test: epoch 130 loss: 5.342093\n",
      "Train: epoch 131\n",
      "Epoch: [131/900], step: [1/2479], loss: 1.511826\n",
      "Epoch: [131/900], step: [201/2479], loss: 1.151925\n",
      "Epoch: [131/900], step: [401/2479], loss: 1.912419\n",
      "Epoch: [131/900], step: [601/2479], loss: 1.501944\n",
      "Epoch: [131/900], step: [801/2479], loss: 1.622032\n",
      "Epoch: [131/900], step: [1001/2479], loss: 1.411322\n",
      "Epoch: [131/900], step: [1201/2479], loss: 0.814785\n",
      "Epoch: [131/900], step: [1401/2479], loss: 1.757805\n",
      "Epoch: [131/900], step: [1601/2479], loss: 1.737367\n",
      "Epoch: [131/900], step: [1801/2479], loss: 1.911722\n",
      "Epoch: [131/900], step: [2001/2479], loss: 2.526247\n",
      "Epoch: [131/900], step: [2201/2479], loss: 1.095154\n",
      "Epoch: [131/900], step: [2401/2479], loss: 1.574822\n",
      "Test: epoch 131 loss: 4.03353\n",
      "Train: epoch 132\n",
      "Epoch: [132/900], step: [1/2479], loss: 2.120265\n",
      "Epoch: [132/900], step: [201/2479], loss: 1.414628\n",
      "Epoch: [132/900], step: [401/2479], loss: 1.937011\n",
      "Epoch: [132/900], step: [601/2479], loss: 1.706231\n",
      "Epoch: [132/900], step: [801/2479], loss: 2.169656\n",
      "Epoch: [132/900], step: [1001/2479], loss: 1.195974\n",
      "Epoch: [132/900], step: [1201/2479], loss: 1.222006\n",
      "Epoch: [132/900], step: [1401/2479], loss: 1.773921\n",
      "Epoch: [132/900], step: [1601/2479], loss: 1.731204\n",
      "Epoch: [132/900], step: [1801/2479], loss: 1.632454\n",
      "Epoch: [132/900], step: [2001/2479], loss: 1.225248\n",
      "Epoch: [132/900], step: [2201/2479], loss: 1.514467\n",
      "Epoch: [132/900], step: [2401/2479], loss: 1.484489\n",
      "Test: epoch 132 loss: 5.6702924\n",
      "Train: epoch 133\n",
      "Epoch: [133/900], step: [1/2479], loss: 1.750162\n",
      "Epoch: [133/900], step: [201/2479], loss: 1.474247\n",
      "Epoch: [133/900], step: [401/2479], loss: 1.246550\n",
      "Epoch: [133/900], step: [601/2479], loss: 1.089876\n",
      "Epoch: [133/900], step: [801/2479], loss: 1.577477\n",
      "Epoch: [133/900], step: [1001/2479], loss: 2.174277\n",
      "Epoch: [133/900], step: [1201/2479], loss: 1.433284\n",
      "Epoch: [133/900], step: [1401/2479], loss: 1.471385\n",
      "Epoch: [133/900], step: [1601/2479], loss: 1.301318\n",
      "Epoch: [133/900], step: [1801/2479], loss: 1.209894\n",
      "Epoch: [133/900], step: [2001/2479], loss: 2.467785\n",
      "Epoch: [133/900], step: [2201/2479], loss: 1.663290\n",
      "Epoch: [133/900], step: [2401/2479], loss: 1.335713\n",
      "Test: epoch 133 loss: 5.203777\n",
      "Train: epoch 134\n",
      "Epoch: [134/900], step: [1/2479], loss: 1.770533\n",
      "Epoch: [134/900], step: [201/2479], loss: 1.642409\n",
      "Epoch: [134/900], step: [401/2479], loss: 2.278753\n",
      "Epoch: [134/900], step: [601/2479], loss: 1.636169\n",
      "Epoch: [134/900], step: [801/2479], loss: 2.610303\n",
      "Epoch: [134/900], step: [1001/2479], loss: 1.287664\n",
      "Epoch: [134/900], step: [1201/2479], loss: 1.519983\n",
      "Epoch: [134/900], step: [1401/2479], loss: 1.380557\n",
      "Epoch: [134/900], step: [1601/2479], loss: 2.194949\n",
      "Epoch: [134/900], step: [1801/2479], loss: 1.174492\n",
      "Epoch: [134/900], step: [2001/2479], loss: 1.973740\n",
      "Epoch: [134/900], step: [2201/2479], loss: 2.198607\n",
      "Epoch: [134/900], step: [2401/2479], loss: 1.115205\n",
      "Test: epoch 134 loss: 6.759867\n",
      "Train: epoch 135\n",
      "Epoch: [135/900], step: [1/2479], loss: 1.748408\n",
      "Epoch: [135/900], step: [201/2479], loss: 1.551052\n",
      "Epoch: [135/900], step: [401/2479], loss: 1.681751\n",
      "Epoch: [135/900], step: [601/2479], loss: 1.586343\n",
      "Epoch: [135/900], step: [801/2479], loss: 2.530226\n",
      "Epoch: [135/900], step: [1001/2479], loss: 2.287439\n",
      "Epoch: [135/900], step: [1201/2479], loss: 1.700804\n",
      "Epoch: [135/900], step: [1401/2479], loss: 2.220428\n",
      "Epoch: [135/900], step: [1601/2479], loss: 2.697505\n",
      "Epoch: [135/900], step: [1801/2479], loss: 1.907606\n",
      "Epoch: [135/900], step: [2001/2479], loss: 1.890409\n",
      "Epoch: [135/900], step: [2201/2479], loss: 1.401348\n",
      "Epoch: [135/900], step: [2401/2479], loss: 1.559915\n",
      "Test: epoch 135 loss: 5.5785775\n",
      "Train: epoch 136\n",
      "Epoch: [136/900], step: [1/2479], loss: 1.524091\n",
      "Epoch: [136/900], step: [201/2479], loss: 1.461214\n",
      "Epoch: [136/900], step: [401/2479], loss: 1.240229\n",
      "Epoch: [136/900], step: [601/2479], loss: 1.916046\n",
      "Epoch: [136/900], step: [801/2479], loss: 1.735494\n",
      "Epoch: [136/900], step: [1001/2479], loss: 1.386070\n",
      "Epoch: [136/900], step: [1201/2479], loss: 1.662909\n",
      "Epoch: [136/900], step: [1401/2479], loss: 2.004048\n",
      "Epoch: [136/900], step: [1601/2479], loss: 1.530590\n",
      "Epoch: [136/900], step: [1801/2479], loss: 1.627733\n",
      "Epoch: [136/900], step: [2001/2479], loss: 1.761445\n",
      "Epoch: [136/900], step: [2201/2479], loss: 1.514990\n",
      "Epoch: [136/900], step: [2401/2479], loss: 1.564378\n",
      "Test: epoch 136 loss: 7.3432636\n",
      "Train: epoch 137\n",
      "Epoch: [137/900], step: [1/2479], loss: 1.519683\n",
      "Epoch: [137/900], step: [201/2479], loss: 1.508170\n",
      "Epoch: [137/900], step: [401/2479], loss: 1.482256\n",
      "Epoch: [137/900], step: [601/2479], loss: 1.660285\n",
      "Epoch: [137/900], step: [801/2479], loss: 1.814358\n",
      "Epoch: [137/900], step: [1001/2479], loss: 1.357639\n",
      "Epoch: [137/900], step: [1201/2479], loss: 2.133414\n",
      "Epoch: [137/900], step: [1401/2479], loss: 1.955547\n",
      "Epoch: [137/900], step: [1601/2479], loss: 1.265812\n",
      "Epoch: [137/900], step: [1801/2479], loss: 1.162823\n",
      "Epoch: [137/900], step: [2001/2479], loss: 2.200768\n",
      "Epoch: [137/900], step: [2201/2479], loss: 1.884178\n",
      "Epoch: [137/900], step: [2401/2479], loss: 1.545171\n",
      "Test: epoch 137 loss: 6.5761385\n",
      "Train: epoch 138\n",
      "Epoch: [138/900], step: [1/2479], loss: 1.741310\n",
      "Epoch: [138/900], step: [201/2479], loss: 1.683936\n",
      "Epoch: [138/900], step: [401/2479], loss: 1.949064\n",
      "Epoch: [138/900], step: [601/2479], loss: 2.215796\n",
      "Epoch: [138/900], step: [801/2479], loss: 2.102077\n",
      "Epoch: [138/900], step: [1001/2479], loss: 1.383806\n",
      "Epoch: [138/900], step: [1201/2479], loss: 1.146437\n",
      "Epoch: [138/900], step: [1401/2479], loss: 0.904380\n",
      "Epoch: [138/900], step: [1601/2479], loss: 1.854615\n",
      "Epoch: [138/900], step: [1801/2479], loss: 2.301843\n",
      "Epoch: [138/900], step: [2001/2479], loss: 2.221367\n",
      "Epoch: [138/900], step: [2201/2479], loss: 1.764230\n",
      "Epoch: [138/900], step: [2401/2479], loss: 1.243562\n",
      "Test: epoch 138 loss: 5.63796\n",
      "Train: epoch 139\n",
      "Epoch: [139/900], step: [1/2479], loss: 2.178940\n",
      "Epoch: [139/900], step: [201/2479], loss: 1.703598\n",
      "Epoch: [139/900], step: [401/2479], loss: 1.839368\n",
      "Epoch: [139/900], step: [601/2479], loss: 1.912214\n",
      "Epoch: [139/900], step: [801/2479], loss: 1.607255\n",
      "Epoch: [139/900], step: [1001/2479], loss: 2.267417\n",
      "Epoch: [139/900], step: [1201/2479], loss: 1.987210\n",
      "Epoch: [139/900], step: [1401/2479], loss: 1.859167\n",
      "Epoch: [139/900], step: [1601/2479], loss: 2.096069\n",
      "Epoch: [139/900], step: [1801/2479], loss: 1.752515\n",
      "Epoch: [139/900], step: [2001/2479], loss: 2.077526\n",
      "Epoch: [139/900], step: [2201/2479], loss: 1.597916\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [139/900], step: [2401/2479], loss: 2.412434\n",
      "Test: epoch 139 loss: 5.0466104\n",
      "Train: epoch 140\n",
      "Epoch: [140/900], step: [1/2479], loss: 2.515866\n",
      "Epoch: [140/900], step: [201/2479], loss: 2.016138\n",
      "Epoch: [140/900], step: [401/2479], loss: 1.433321\n",
      "Epoch: [140/900], step: [601/2479], loss: 2.000807\n",
      "Epoch: [140/900], step: [801/2479], loss: 1.944053\n",
      "Epoch: [140/900], step: [1001/2479], loss: 1.823358\n",
      "Epoch: [140/900], step: [1201/2479], loss: 1.384757\n",
      "Epoch: [140/900], step: [1401/2479], loss: 1.317672\n",
      "Epoch: [140/900], step: [1601/2479], loss: 1.900336\n",
      "Epoch: [140/900], step: [1801/2479], loss: 1.685714\n",
      "Epoch: [140/900], step: [2001/2479], loss: 1.783702\n",
      "Epoch: [140/900], step: [2201/2479], loss: 1.640712\n",
      "Epoch: [140/900], step: [2401/2479], loss: 2.253725\n",
      "Test: epoch 140 loss: 6.448006\n",
      "Train: epoch 141\n",
      "Epoch: [141/900], step: [1/2479], loss: 1.769103\n",
      "Epoch: [141/900], step: [201/2479], loss: 2.487294\n",
      "Epoch: [141/900], step: [401/2479], loss: 1.849964\n",
      "Epoch: [141/900], step: [601/2479], loss: 1.583062\n",
      "Epoch: [141/900], step: [801/2479], loss: 1.771364\n",
      "Epoch: [141/900], step: [1001/2479], loss: 2.004830\n",
      "Epoch: [141/900], step: [1201/2479], loss: 1.628682\n",
      "Epoch: [141/900], step: [1401/2479], loss: 1.433179\n",
      "Epoch: [141/900], step: [1601/2479], loss: 1.585049\n",
      "Epoch: [141/900], step: [1801/2479], loss: 1.652374\n",
      "Epoch: [141/900], step: [2001/2479], loss: 0.961192\n",
      "Epoch: [141/900], step: [2201/2479], loss: 1.899306\n",
      "Epoch: [141/900], step: [2401/2479], loss: 2.117100\n",
      "Test: epoch 141 loss: 4.3078575\n",
      "Train: epoch 142\n",
      "Epoch: [142/900], step: [1/2479], loss: 1.658551\n",
      "Epoch: [142/900], step: [201/2479], loss: 1.580100\n",
      "Epoch: [142/900], step: [401/2479], loss: 1.869845\n",
      "Epoch: [142/900], step: [601/2479], loss: 1.591379\n",
      "Epoch: [142/900], step: [801/2479], loss: 2.022915\n",
      "Epoch: [142/900], step: [1001/2479], loss: 0.837036\n",
      "Epoch: [142/900], step: [1201/2479], loss: 1.750247\n",
      "Epoch: [142/900], step: [1401/2479], loss: 1.431368\n",
      "Epoch: [142/900], step: [1601/2479], loss: 2.195868\n",
      "Epoch: [142/900], step: [1801/2479], loss: 1.605006\n",
      "Epoch: [142/900], step: [2001/2479], loss: 1.625168\n",
      "Epoch: [142/900], step: [2201/2479], loss: 1.617908\n",
      "Epoch: [142/900], step: [2401/2479], loss: 2.494322\n",
      "Test: epoch 142 loss: 7.124511\n",
      "Train: epoch 143\n",
      "Epoch: [143/900], step: [1/2479], loss: 1.501307\n",
      "Epoch: [143/900], step: [201/2479], loss: 1.549708\n",
      "Epoch: [143/900], step: [401/2479], loss: 1.429586\n",
      "Epoch: [143/900], step: [601/2479], loss: 1.234540\n",
      "Epoch: [143/900], step: [801/2479], loss: 1.274546\n",
      "Epoch: [143/900], step: [1001/2479], loss: 2.011648\n",
      "Epoch: [143/900], step: [1201/2479], loss: 1.586872\n",
      "Epoch: [143/900], step: [1401/2479], loss: 1.567268\n",
      "Epoch: [143/900], step: [1601/2479], loss: 1.654915\n",
      "Epoch: [143/900], step: [1801/2479], loss: 1.166586\n",
      "Epoch: [143/900], step: [2001/2479], loss: 2.311872\n",
      "Epoch: [143/900], step: [2201/2479], loss: 1.792400\n",
      "Epoch: [143/900], step: [2401/2479], loss: 1.550593\n",
      "Test: epoch 143 loss: 4.794777\n",
      "Train: epoch 144\n",
      "Epoch: [144/900], step: [1/2479], loss: 1.715999\n",
      "Epoch: [144/900], step: [201/2479], loss: 2.088173\n",
      "Epoch: [144/900], step: [401/2479], loss: 1.588721\n",
      "Epoch: [144/900], step: [601/2479], loss: 1.505243\n",
      "Epoch: [144/900], step: [801/2479], loss: 1.552086\n",
      "Epoch: [144/900], step: [1001/2479], loss: 1.244652\n",
      "Epoch: [144/900], step: [1201/2479], loss: 1.685457\n",
      "Epoch: [144/900], step: [1401/2479], loss: 1.982901\n",
      "Epoch: [144/900], step: [1601/2479], loss: 1.630233\n",
      "Epoch: [144/900], step: [1801/2479], loss: 1.636263\n",
      "Epoch: [144/900], step: [2001/2479], loss: 1.685824\n",
      "Epoch: [144/900], step: [2201/2479], loss: 1.592689\n",
      "Epoch: [144/900], step: [2401/2479], loss: 1.312301\n",
      "Test: epoch 144 loss: 6.698015\n",
      "Train: epoch 145\n",
      "Epoch: [145/900], step: [1/2479], loss: 1.642551\n",
      "Epoch: [145/900], step: [201/2479], loss: 1.486220\n",
      "Epoch: [145/900], step: [401/2479], loss: 1.901472\n",
      "Epoch: [145/900], step: [601/2479], loss: 1.791783\n",
      "Epoch: [145/900], step: [801/2479], loss: 2.360416\n",
      "Epoch: [145/900], step: [1001/2479], loss: 1.618815\n",
      "Epoch: [145/900], step: [1201/2479], loss: 1.145318\n",
      "Epoch: [145/900], step: [1401/2479], loss: 2.683106\n",
      "Epoch: [145/900], step: [1601/2479], loss: 2.043620\n",
      "Epoch: [145/900], step: [1801/2479], loss: 1.601766\n",
      "Epoch: [145/900], step: [2001/2479], loss: 1.903913\n",
      "Epoch: [145/900], step: [2201/2479], loss: 1.800732\n",
      "Epoch: [145/900], step: [2401/2479], loss: 1.132600\n",
      "Test: epoch 145 loss: 6.4931917\n",
      "Train: epoch 146\n",
      "Epoch: [146/900], step: [1/2479], loss: 1.821810\n",
      "Epoch: [146/900], step: [201/2479], loss: 1.226802\n",
      "Epoch: [146/900], step: [401/2479], loss: 1.603568\n",
      "Epoch: [146/900], step: [601/2479], loss: 1.806291\n",
      "Epoch: [146/900], step: [801/2479], loss: 1.591422\n",
      "Epoch: [146/900], step: [1001/2479], loss: 1.796522\n",
      "Epoch: [146/900], step: [1201/2479], loss: 1.226469\n",
      "Epoch: [146/900], step: [1401/2479], loss: 1.764823\n",
      "Epoch: [146/900], step: [1601/2479], loss: 1.920265\n",
      "Epoch: [146/900], step: [1801/2479], loss: 1.886123\n",
      "Epoch: [146/900], step: [2001/2479], loss: 1.730506\n",
      "Epoch: [146/900], step: [2201/2479], loss: 2.639201\n",
      "Epoch: [146/900], step: [2401/2479], loss: 1.712537\n",
      "Test: epoch 146 loss: 6.5168505\n",
      "Train: epoch 147\n",
      "Epoch: [147/900], step: [1/2479], loss: 2.096978\n",
      "Epoch: [147/900], step: [201/2479], loss: 1.525123\n",
      "Epoch: [147/900], step: [401/2479], loss: 1.779679\n",
      "Epoch: [147/900], step: [601/2479], loss: 1.083916\n",
      "Epoch: [147/900], step: [801/2479], loss: 1.507800\n",
      "Epoch: [147/900], step: [1001/2479], loss: 1.503027\n",
      "Epoch: [147/900], step: [1201/2479], loss: 1.900830\n",
      "Epoch: [147/900], step: [1401/2479], loss: 2.023417\n",
      "Epoch: [147/900], step: [1601/2479], loss: 1.932009\n",
      "Epoch: [147/900], step: [1801/2479], loss: 1.909256\n",
      "Epoch: [147/900], step: [2001/2479], loss: 1.738696\n",
      "Epoch: [147/900], step: [2201/2479], loss: 2.144527\n",
      "Epoch: [147/900], step: [2401/2479], loss: 2.202747\n",
      "Test: epoch 147 loss: 6.846548\n",
      "Train: epoch 148\n",
      "Epoch: [148/900], step: [1/2479], loss: 1.754810\n",
      "Epoch: [148/900], step: [201/2479], loss: 2.112380\n",
      "Epoch: [148/900], step: [401/2479], loss: 2.105328\n",
      "Epoch: [148/900], step: [601/2479], loss: 1.998470\n",
      "Epoch: [148/900], step: [801/2479], loss: 1.715187\n",
      "Epoch: [148/900], step: [1001/2479], loss: 1.536092\n",
      "Epoch: [148/900], step: [1201/2479], loss: 0.665877\n",
      "Epoch: [148/900], step: [1401/2479], loss: 1.642552\n",
      "Epoch: [148/900], step: [1601/2479], loss: 2.313423\n",
      "Epoch: [148/900], step: [1801/2479], loss: 1.178492\n",
      "Epoch: [148/900], step: [2001/2479], loss: 2.019315\n",
      "Epoch: [148/900], step: [2201/2479], loss: 1.789018\n",
      "Epoch: [148/900], step: [2401/2479], loss: 1.008626\n",
      "Test: epoch 148 loss: 6.121079\n",
      "Train: epoch 149\n",
      "Epoch: [149/900], step: [1/2479], loss: 1.851306\n",
      "Epoch: [149/900], step: [201/2479], loss: 1.611167\n",
      "Epoch: [149/900], step: [401/2479], loss: 1.659981\n",
      "Epoch: [149/900], step: [601/2479], loss: 1.655626\n",
      "Epoch: [149/900], step: [801/2479], loss: 1.068870\n",
      "Epoch: [149/900], step: [1001/2479], loss: 1.363333\n",
      "Epoch: [149/900], step: [1201/2479], loss: 1.489939\n",
      "Epoch: [149/900], step: [1401/2479], loss: 2.117284\n",
      "Epoch: [149/900], step: [1601/2479], loss: 1.951989\n",
      "Epoch: [149/900], step: [1801/2479], loss: 1.283877\n",
      "Epoch: [149/900], step: [2001/2479], loss: 1.577657\n",
      "Epoch: [149/900], step: [2201/2479], loss: 2.200908\n",
      "Epoch: [149/900], step: [2401/2479], loss: 1.978546\n",
      "Test: epoch 149 loss: 5.9895806\n",
      "Train: epoch 150\n",
      "Epoch: [150/900], step: [1/2479], loss: 2.030470\n",
      "Epoch: [150/900], step: [201/2479], loss: 1.974653\n",
      "Epoch: [150/900], step: [401/2479], loss: 1.527028\n",
      "Epoch: [150/900], step: [601/2479], loss: 1.116814\n",
      "Epoch: [150/900], step: [801/2479], loss: 1.759064\n",
      "Epoch: [150/900], step: [1001/2479], loss: 1.871417\n",
      "Epoch: [150/900], step: [1201/2479], loss: 2.024872\n",
      "Epoch: [150/900], step: [1401/2479], loss: 1.508744\n",
      "Epoch: [150/900], step: [1601/2479], loss: 1.992749\n",
      "Epoch: [150/900], step: [1801/2479], loss: 1.986719\n",
      "Epoch: [150/900], step: [2001/2479], loss: 1.158600\n",
      "Epoch: [150/900], step: [2201/2479], loss: 2.421664\n",
      "Epoch: [150/900], step: [2401/2479], loss: 1.412580\n",
      "Test: epoch 150 loss: 5.6708565\n",
      "Train: epoch 151\n",
      "Epoch: [151/900], step: [1/2479], loss: 1.499350\n",
      "Epoch: [151/900], step: [201/2479], loss: 1.889245\n",
      "Epoch: [151/900], step: [401/2479], loss: 2.202178\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [151/900], step: [601/2479], loss: 2.199646\n",
      "Epoch: [151/900], step: [801/2479], loss: 2.370520\n",
      "Epoch: [151/900], step: [1001/2479], loss: 2.001425\n",
      "Epoch: [151/900], step: [1201/2479], loss: 1.485856\n",
      "Epoch: [151/900], step: [1401/2479], loss: 1.739993\n",
      "Epoch: [151/900], step: [1601/2479], loss: 1.623869\n",
      "Epoch: [151/900], step: [1801/2479], loss: 1.515748\n",
      "Epoch: [151/900], step: [2001/2479], loss: 2.318938\n",
      "Epoch: [151/900], step: [2201/2479], loss: 1.732295\n",
      "Epoch: [151/900], step: [2401/2479], loss: 1.647642\n",
      "Test: epoch 151 loss: 4.960156\n",
      "Train: epoch 152\n",
      "Epoch: [152/900], step: [1/2479], loss: 1.560990\n",
      "Epoch: [152/900], step: [201/2479], loss: 1.512743\n",
      "Epoch: [152/900], step: [401/2479], loss: 1.891825\n",
      "Epoch: [152/900], step: [601/2479], loss: 2.058457\n",
      "Epoch: [152/900], step: [801/2479], loss: 1.247728\n",
      "Epoch: [152/900], step: [1001/2479], loss: 2.000068\n",
      "Epoch: [152/900], step: [1201/2479], loss: 1.674198\n",
      "Epoch: [152/900], step: [1401/2479], loss: 2.165054\n",
      "Epoch: [152/900], step: [1601/2479], loss: 2.475853\n",
      "Epoch: [152/900], step: [1801/2479], loss: 1.985337\n",
      "Epoch: [152/900], step: [2001/2479], loss: 1.921931\n",
      "Epoch: [152/900], step: [2201/2479], loss: 1.536728\n",
      "Epoch: [152/900], step: [2401/2479], loss: 1.604028\n",
      "Test: epoch 152 loss: 6.6221685\n",
      "Train: epoch 153\n",
      "Epoch: [153/900], step: [1/2479], loss: 1.810214\n",
      "Epoch: [153/900], step: [201/2479], loss: 0.926607\n",
      "Epoch: [153/900], step: [401/2479], loss: 1.142060\n",
      "Epoch: [153/900], step: [601/2479], loss: 1.145683\n",
      "Epoch: [153/900], step: [801/2479], loss: 1.953560\n",
      "Epoch: [153/900], step: [1001/2479], loss: 2.039459\n",
      "Epoch: [153/900], step: [1201/2479], loss: 1.630042\n",
      "Epoch: [153/900], step: [1401/2479], loss: 2.785253\n",
      "Epoch: [153/900], step: [1601/2479], loss: 1.710800\n",
      "Epoch: [153/900], step: [1801/2479], loss: 1.291620\n",
      "Epoch: [153/900], step: [2001/2479], loss: 1.636822\n",
      "Epoch: [153/900], step: [2201/2479], loss: 2.089744\n",
      "Epoch: [153/900], step: [2401/2479], loss: 1.965213\n",
      "Test: epoch 153 loss: 3.9164069\n",
      "Train: epoch 154\n",
      "Epoch: [154/900], step: [1/2479], loss: 2.386058\n",
      "Epoch: [154/900], step: [201/2479], loss: 1.683113\n",
      "Epoch: [154/900], step: [401/2479], loss: 1.599205\n",
      "Epoch: [154/900], step: [601/2479], loss: 1.866877\n",
      "Epoch: [154/900], step: [801/2479], loss: 1.906981\n",
      "Epoch: [154/900], step: [1001/2479], loss: 1.700537\n",
      "Epoch: [154/900], step: [1201/2479], loss: 1.690115\n",
      "Epoch: [154/900], step: [1401/2479], loss: 2.407164\n",
      "Epoch: [154/900], step: [1601/2479], loss: 2.079879\n",
      "Epoch: [154/900], step: [1801/2479], loss: 1.332714\n",
      "Epoch: [154/900], step: [2001/2479], loss: 1.438528\n",
      "Epoch: [154/900], step: [2201/2479], loss: 2.308848\n",
      "Epoch: [154/900], step: [2401/2479], loss: 1.434156\n",
      "Test: epoch 154 loss: 5.7838044\n",
      "Train: epoch 155\n",
      "Epoch: [155/900], step: [1/2479], loss: 1.210009\n",
      "Epoch: [155/900], step: [201/2479], loss: 2.229858\n",
      "Epoch: [155/900], step: [401/2479], loss: 1.450439\n",
      "Epoch: [155/900], step: [601/2479], loss: 1.683147\n",
      "Epoch: [155/900], step: [801/2479], loss: 1.436893\n",
      "Epoch: [155/900], step: [1001/2479], loss: 1.772829\n",
      "Epoch: [155/900], step: [1201/2479], loss: 1.904061\n",
      "Epoch: [155/900], step: [1401/2479], loss: 1.731467\n",
      "Epoch: [155/900], step: [1601/2479], loss: 2.027323\n",
      "Epoch: [155/900], step: [1801/2479], loss: 1.858831\n",
      "Epoch: [155/900], step: [2001/2479], loss: 1.285137\n",
      "Epoch: [155/900], step: [2201/2479], loss: 1.696424\n",
      "Epoch: [155/900], step: [2401/2479], loss: 1.959700\n",
      "Test: epoch 155 loss: 5.757169\n",
      "Train: epoch 156\n",
      "Epoch: [156/900], step: [1/2479], loss: 1.321568\n",
      "Epoch: [156/900], step: [201/2479], loss: 1.849435\n",
      "Epoch: [156/900], step: [401/2479], loss: 1.884040\n",
      "Epoch: [156/900], step: [601/2479], loss: 1.901737\n",
      "Epoch: [156/900], step: [801/2479], loss: 1.489925\n",
      "Epoch: [156/900], step: [1001/2479], loss: 1.681620\n",
      "Epoch: [156/900], step: [1201/2479], loss: 1.302072\n",
      "Epoch: [156/900], step: [1401/2479], loss: 1.920769\n",
      "Epoch: [156/900], step: [1601/2479], loss: 1.263403\n",
      "Epoch: [156/900], step: [1801/2479], loss: 1.745847\n",
      "Epoch: [156/900], step: [2001/2479], loss: 1.756003\n",
      "Epoch: [156/900], step: [2201/2479], loss: 1.429947\n",
      "Epoch: [156/900], step: [2401/2479], loss: 2.147683\n",
      "Test: epoch 156 loss: 3.9373899\n",
      "Train: epoch 157\n",
      "Epoch: [157/900], step: [1/2479], loss: 1.673491\n",
      "Epoch: [157/900], step: [201/2479], loss: 1.361090\n",
      "Epoch: [157/900], step: [401/2479], loss: 1.582616\n",
      "Epoch: [157/900], step: [601/2479], loss: 1.914550\n",
      "Epoch: [157/900], step: [801/2479], loss: 1.575018\n",
      "Epoch: [157/900], step: [1001/2479], loss: 1.316831\n",
      "Epoch: [157/900], step: [1201/2479], loss: 2.455638\n",
      "Epoch: [157/900], step: [1401/2479], loss: 1.695367\n",
      "Epoch: [157/900], step: [1601/2479], loss: 1.851371\n",
      "Epoch: [157/900], step: [1801/2479], loss: 1.961261\n",
      "Epoch: [157/900], step: [2001/2479], loss: 1.462219\n",
      "Epoch: [157/900], step: [2201/2479], loss: 1.426603\n",
      "Epoch: [157/900], step: [2401/2479], loss: 1.976651\n",
      "Test: epoch 157 loss: 8.026014\n",
      "Train: epoch 158\n",
      "Epoch: [158/900], step: [1/2479], loss: 1.547646\n",
      "Epoch: [158/900], step: [201/2479], loss: 1.666312\n",
      "Epoch: [158/900], step: [401/2479], loss: 1.575593\n",
      "Epoch: [158/900], step: [601/2479], loss: 1.994060\n",
      "Epoch: [158/900], step: [801/2479], loss: 1.653123\n",
      "Epoch: [158/900], step: [1001/2479], loss: 1.963282\n",
      "Epoch: [158/900], step: [1201/2479], loss: 1.915327\n",
      "Epoch: [158/900], step: [1401/2479], loss: 1.548907\n",
      "Epoch: [158/900], step: [1601/2479], loss: 1.991282\n",
      "Epoch: [158/900], step: [1801/2479], loss: 1.263834\n",
      "Epoch: [158/900], step: [2001/2479], loss: 1.832983\n",
      "Epoch: [158/900], step: [2201/2479], loss: 1.453108\n",
      "Epoch: [158/900], step: [2401/2479], loss: 2.386446\n",
      "Test: epoch 158 loss: 6.3603654\n",
      "Train: epoch 159\n",
      "Epoch: [159/900], step: [1/2479], loss: 1.529836\n",
      "Epoch: [159/900], step: [201/2479], loss: 1.767535\n",
      "Epoch: [159/900], step: [401/2479], loss: 1.648067\n",
      "Epoch: [159/900], step: [601/2479], loss: 1.047213\n",
      "Epoch: [159/900], step: [801/2479], loss: 2.264277\n",
      "Epoch: [159/900], step: [1001/2479], loss: 1.428922\n",
      "Epoch: [159/900], step: [1201/2479], loss: 1.752660\n",
      "Epoch: [159/900], step: [1401/2479], loss: 1.474031\n",
      "Epoch: [159/900], step: [1601/2479], loss: 1.995565\n",
      "Epoch: [159/900], step: [1801/2479], loss: 2.165068\n",
      "Epoch: [159/900], step: [2001/2479], loss: 1.554581\n",
      "Epoch: [159/900], step: [2201/2479], loss: 1.737121\n",
      "Epoch: [159/900], step: [2401/2479], loss: 1.992934\n",
      "Test: epoch 159 loss: 4.324977\n",
      "Train: epoch 160\n",
      "Epoch: [160/900], step: [1/2479], loss: 1.489872\n",
      "Epoch: [160/900], step: [201/2479], loss: 1.976547\n",
      "Epoch: [160/900], step: [401/2479], loss: 1.619854\n",
      "Epoch: [160/900], step: [601/2479], loss: 1.638528\n",
      "Epoch: [160/900], step: [801/2479], loss: 1.856378\n",
      "Epoch: [160/900], step: [1001/2479], loss: 1.072309\n",
      "Epoch: [160/900], step: [1201/2479], loss: 1.753685\n",
      "Epoch: [160/900], step: [1401/2479], loss: 2.406373\n",
      "Epoch: [160/900], step: [1601/2479], loss: 1.829959\n",
      "Epoch: [160/900], step: [1801/2479], loss: 2.509606\n",
      "Epoch: [160/900], step: [2001/2479], loss: 1.203710\n",
      "Epoch: [160/900], step: [2201/2479], loss: 1.604091\n",
      "Epoch: [160/900], step: [2401/2479], loss: 1.695811\n",
      "Test: epoch 160 loss: 7.35477\n",
      "Train: epoch 161\n",
      "Epoch: [161/900], step: [1/2479], loss: 1.415887\n",
      "Epoch: [161/900], step: [201/2479], loss: 1.537327\n",
      "Epoch: [161/900], step: [401/2479], loss: 2.185266\n",
      "Epoch: [161/900], step: [601/2479], loss: 1.926379\n",
      "Epoch: [161/900], step: [801/2479], loss: 1.433776\n",
      "Epoch: [161/900], step: [1001/2479], loss: 1.749798\n",
      "Epoch: [161/900], step: [1201/2479], loss: 1.363564\n",
      "Epoch: [161/900], step: [1401/2479], loss: 1.839558\n",
      "Epoch: [161/900], step: [1601/2479], loss: 2.230121\n",
      "Epoch: [161/900], step: [1801/2479], loss: 1.588173\n",
      "Epoch: [161/900], step: [2001/2479], loss: 1.383286\n",
      "Epoch: [161/900], step: [2201/2479], loss: 1.634697\n",
      "Epoch: [161/900], step: [2401/2479], loss: 1.933490\n",
      "Test: epoch 161 loss: 4.020755\n",
      "Train: epoch 162\n",
      "Epoch: [162/900], step: [1/2479], loss: 1.501962\n",
      "Epoch: [162/900], step: [201/2479], loss: 2.029074\n",
      "Epoch: [162/900], step: [401/2479], loss: 1.335210\n",
      "Epoch: [162/900], step: [601/2479], loss: 1.728011\n",
      "Epoch: [162/900], step: [801/2479], loss: 1.934831\n",
      "Epoch: [162/900], step: [1001/2479], loss: 1.409275\n",
      "Epoch: [162/900], step: [1201/2479], loss: 1.212435\n",
      "Epoch: [162/900], step: [1401/2479], loss: 1.746740\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [162/900], step: [1601/2479], loss: 1.414323\n",
      "Epoch: [162/900], step: [1801/2479], loss: 2.625894\n",
      "Epoch: [162/900], step: [2001/2479], loss: 1.997676\n",
      "Epoch: [162/900], step: [2201/2479], loss: 1.950178\n",
      "Epoch: [162/900], step: [2401/2479], loss: 1.826089\n",
      "Test: epoch 162 loss: 5.183047\n",
      "Train: epoch 163\n",
      "Epoch: [163/900], step: [1/2479], loss: 1.990616\n",
      "Epoch: [163/900], step: [201/2479], loss: 2.155948\n",
      "Epoch: [163/900], step: [401/2479], loss: 1.678336\n",
      "Epoch: [163/900], step: [601/2479], loss: 2.034271\n",
      "Epoch: [163/900], step: [801/2479], loss: 1.369135\n",
      "Epoch: [163/900], step: [1001/2479], loss: 2.274663\n",
      "Epoch: [163/900], step: [1201/2479], loss: 1.605816\n",
      "Epoch: [163/900], step: [1401/2479], loss: 1.713410\n",
      "Epoch: [163/900], step: [1601/2479], loss: 2.113104\n",
      "Epoch: [163/900], step: [1801/2479], loss: 1.953470\n",
      "Epoch: [163/900], step: [2001/2479], loss: 2.122232\n",
      "Epoch: [163/900], step: [2201/2479], loss: 1.590900\n",
      "Epoch: [163/900], step: [2401/2479], loss: 1.687369\n",
      "Test: epoch 163 loss: 4.57821\n",
      "Train: epoch 164\n",
      "Epoch: [164/900], step: [1/2479], loss: 1.775475\n",
      "Epoch: [164/900], step: [201/2479], loss: 1.020647\n",
      "Epoch: [164/900], step: [401/2479], loss: 1.706062\n",
      "Epoch: [164/900], step: [601/2479], loss: 1.442510\n",
      "Epoch: [164/900], step: [801/2479], loss: 1.407028\n",
      "Epoch: [164/900], step: [1001/2479], loss: 1.494192\n",
      "Epoch: [164/900], step: [1201/2479], loss: 1.902073\n",
      "Epoch: [164/900], step: [1401/2479], loss: 2.014102\n",
      "Epoch: [164/900], step: [1601/2479], loss: 1.414462\n",
      "Epoch: [164/900], step: [1801/2479], loss: 1.606590\n",
      "Epoch: [164/900], step: [2001/2479], loss: 1.502283\n",
      "Epoch: [164/900], step: [2201/2479], loss: 2.105671\n",
      "Epoch: [164/900], step: [2401/2479], loss: 1.640691\n",
      "Test: epoch 164 loss: 7.6811495\n",
      "Train: epoch 165\n",
      "Epoch: [165/900], step: [1/2479], loss: 1.696227\n",
      "Epoch: [165/900], step: [201/2479], loss: 1.503272\n",
      "Epoch: [165/900], step: [401/2479], loss: 1.761230\n",
      "Epoch: [165/900], step: [601/2479], loss: 1.906045\n",
      "Epoch: [165/900], step: [801/2479], loss: 1.248896\n",
      "Epoch: [165/900], step: [1001/2479], loss: 0.843451\n",
      "Epoch: [165/900], step: [1201/2479], loss: 1.823907\n",
      "Epoch: [165/900], step: [1401/2479], loss: 1.932034\n",
      "Epoch: [165/900], step: [1601/2479], loss: 1.394367\n",
      "Epoch: [165/900], step: [1801/2479], loss: 1.247241\n",
      "Epoch: [165/900], step: [2001/2479], loss: 1.772837\n",
      "Epoch: [165/900], step: [2201/2479], loss: 1.776073\n",
      "Epoch: [165/900], step: [2401/2479], loss: 1.959359\n",
      "Test: epoch 165 loss: 7.2285647\n",
      "Train: epoch 166\n",
      "Epoch: [166/900], step: [1/2479], loss: 1.672288\n",
      "Epoch: [166/900], step: [201/2479], loss: 2.210365\n",
      "Epoch: [166/900], step: [401/2479], loss: 1.723205\n",
      "Epoch: [166/900], step: [601/2479], loss: 1.930966\n",
      "Epoch: [166/900], step: [801/2479], loss: 1.530987\n",
      "Epoch: [166/900], step: [1001/2479], loss: 1.836733\n",
      "Epoch: [166/900], step: [1201/2479], loss: 0.973829\n",
      "Epoch: [166/900], step: [1401/2479], loss: 0.918056\n",
      "Epoch: [166/900], step: [1601/2479], loss: 1.455121\n",
      "Epoch: [166/900], step: [1801/2479], loss: 1.542902\n",
      "Epoch: [166/900], step: [2001/2479], loss: 1.377989\n",
      "Epoch: [166/900], step: [2201/2479], loss: 2.141294\n",
      "Epoch: [166/900], step: [2401/2479], loss: 1.781322\n",
      "Test: epoch 166 loss: 6.199406\n",
      "Train: epoch 167\n",
      "Epoch: [167/900], step: [1/2479], loss: 1.315021\n",
      "Epoch: [167/900], step: [201/2479], loss: 1.130290\n",
      "Epoch: [167/900], step: [401/2479], loss: 1.284019\n",
      "Epoch: [167/900], step: [601/2479], loss: 1.911175\n",
      "Epoch: [167/900], step: [801/2479], loss: 2.100339\n",
      "Epoch: [167/900], step: [1001/2479], loss: 1.168954\n",
      "Epoch: [167/900], step: [1201/2479], loss: 1.161543\n",
      "Epoch: [167/900], step: [1401/2479], loss: 1.248679\n",
      "Epoch: [167/900], step: [1601/2479], loss: 1.513072\n",
      "Epoch: [167/900], step: [1801/2479], loss: 1.274766\n",
      "Epoch: [167/900], step: [2001/2479], loss: 1.667738\n",
      "Epoch: [167/900], step: [2201/2479], loss: 2.334376\n",
      "Epoch: [167/900], step: [2401/2479], loss: 1.925324\n",
      "Test: epoch 167 loss: 5.349606\n",
      "Train: epoch 168\n",
      "Epoch: [168/900], step: [1/2479], loss: 1.007942\n",
      "Epoch: [168/900], step: [201/2479], loss: 1.504217\n",
      "Epoch: [168/900], step: [401/2479], loss: 2.038185\n",
      "Epoch: [168/900], step: [601/2479], loss: 2.251022\n",
      "Epoch: [168/900], step: [801/2479], loss: 1.683893\n",
      "Epoch: [168/900], step: [1001/2479], loss: 1.377557\n",
      "Epoch: [168/900], step: [1201/2479], loss: 1.426020\n",
      "Epoch: [168/900], step: [1401/2479], loss: 1.426835\n",
      "Epoch: [168/900], step: [1601/2479], loss: 1.557408\n",
      "Epoch: [168/900], step: [1801/2479], loss: 1.846332\n",
      "Epoch: [168/900], step: [2001/2479], loss: 1.435009\n",
      "Epoch: [168/900], step: [2201/2479], loss: 1.494934\n",
      "Epoch: [168/900], step: [2401/2479], loss: 1.785028\n",
      "Test: epoch 168 loss: 5.63423\n",
      "Train: epoch 169\n",
      "Epoch: [169/900], step: [1/2479], loss: 1.501029\n",
      "Epoch: [169/900], step: [201/2479], loss: 1.419625\n",
      "Epoch: [169/900], step: [401/2479], loss: 1.599610\n",
      "Epoch: [169/900], step: [601/2479], loss: 1.374532\n",
      "Epoch: [169/900], step: [801/2479], loss: 1.392180\n",
      "Epoch: [169/900], step: [1001/2479], loss: 1.246836\n",
      "Epoch: [169/900], step: [1201/2479], loss: 1.667594\n",
      "Epoch: [169/900], step: [1401/2479], loss: 1.003990\n",
      "Epoch: [169/900], step: [1601/2479], loss: 1.500134\n",
      "Epoch: [169/900], step: [1801/2479], loss: 2.064851\n",
      "Epoch: [169/900], step: [2001/2479], loss: 1.950189\n",
      "Epoch: [169/900], step: [2201/2479], loss: 2.059339\n",
      "Epoch: [169/900], step: [2401/2479], loss: 1.374577\n",
      "Test: epoch 169 loss: 6.042189\n",
      "Train: epoch 170\n",
      "Epoch: [170/900], step: [1/2479], loss: 1.631088\n",
      "Epoch: [170/900], step: [201/2479], loss: 1.282616\n",
      "Epoch: [170/900], step: [401/2479], loss: 1.569189\n",
      "Epoch: [170/900], step: [601/2479], loss: 2.378794\n",
      "Epoch: [170/900], step: [801/2479], loss: 1.097221\n",
      "Epoch: [170/900], step: [1001/2479], loss: 1.133180\n",
      "Epoch: [170/900], step: [1201/2479], loss: 1.252553\n",
      "Epoch: [170/900], step: [1401/2479], loss: 1.627396\n",
      "Epoch: [170/900], step: [1601/2479], loss: 1.834290\n",
      "Epoch: [170/900], step: [1801/2479], loss: 2.362108\n",
      "Epoch: [170/900], step: [2001/2479], loss: 1.761737\n",
      "Epoch: [170/900], step: [2201/2479], loss: 2.038349\n",
      "Epoch: [170/900], step: [2401/2479], loss: 1.133810\n",
      "Test: epoch 170 loss: 5.836886\n",
      "Train: epoch 171\n",
      "Epoch: [171/900], step: [1/2479], loss: 1.750736\n",
      "Epoch: [171/900], step: [201/2479], loss: 1.446853\n",
      "Epoch: [171/900], step: [401/2479], loss: 1.808644\n",
      "Epoch: [171/900], step: [601/2479], loss: 2.119215\n",
      "Epoch: [171/900], step: [801/2479], loss: 1.706521\n",
      "Epoch: [171/900], step: [1001/2479], loss: 1.428643\n",
      "Epoch: [171/900], step: [1201/2479], loss: 2.527863\n",
      "Epoch: [171/900], step: [1401/2479], loss: 1.306129\n",
      "Epoch: [171/900], step: [1601/2479], loss: 1.459321\n",
      "Epoch: [171/900], step: [1801/2479], loss: 1.671333\n",
      "Epoch: [171/900], step: [2001/2479], loss: 1.383543\n",
      "Epoch: [171/900], step: [2201/2479], loss: 1.586174\n",
      "Epoch: [171/900], step: [2401/2479], loss: 1.892334\n",
      "Test: epoch 171 loss: 6.4263644\n",
      "Train: epoch 172\n",
      "Epoch: [172/900], step: [1/2479], loss: 1.525824\n",
      "Epoch: [172/900], step: [201/2479], loss: 1.793596\n",
      "Epoch: [172/900], step: [401/2479], loss: 2.221556\n",
      "Epoch: [172/900], step: [601/2479], loss: 1.749657\n",
      "Epoch: [172/900], step: [801/2479], loss: 1.488399\n",
      "Epoch: [172/900], step: [1001/2479], loss: 1.235965\n",
      "Epoch: [172/900], step: [1201/2479], loss: 2.371409\n",
      "Epoch: [172/900], step: [1401/2479], loss: 1.815084\n",
      "Epoch: [172/900], step: [1601/2479], loss: 1.459046\n",
      "Epoch: [172/900], step: [1801/2479], loss: 1.885676\n",
      "Epoch: [172/900], step: [2001/2479], loss: 1.955866\n",
      "Epoch: [172/900], step: [2201/2479], loss: 1.757996\n",
      "Epoch: [172/900], step: [2401/2479], loss: 1.644916\n",
      "Test: epoch 172 loss: 7.150293\n",
      "Train: epoch 173\n",
      "Epoch: [173/900], step: [1/2479], loss: 1.731771\n",
      "Epoch: [173/900], step: [201/2479], loss: 1.625468\n",
      "Epoch: [173/900], step: [401/2479], loss: 1.706965\n",
      "Epoch: [173/900], step: [601/2479], loss: 0.873136\n",
      "Epoch: [173/900], step: [801/2479], loss: 1.971327\n",
      "Epoch: [173/900], step: [1001/2479], loss: 1.564285\n",
      "Epoch: [173/900], step: [1201/2479], loss: 1.764940\n",
      "Epoch: [173/900], step: [1401/2479], loss: 1.242958\n",
      "Epoch: [173/900], step: [1601/2479], loss: 2.018461\n",
      "Epoch: [173/900], step: [1801/2479], loss: 1.777570\n",
      "Epoch: [173/900], step: [2001/2479], loss: 1.763867\n",
      "Epoch: [173/900], step: [2201/2479], loss: 2.054647\n",
      "Epoch: [173/900], step: [2401/2479], loss: 2.096744\n",
      "Test: epoch 173 loss: 6.8611107\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: epoch 174\n",
      "Epoch: [174/900], step: [1/2479], loss: 1.336418\n",
      "Epoch: [174/900], step: [201/2479], loss: 1.776424\n",
      "Epoch: [174/900], step: [401/2479], loss: 1.732653\n",
      "Epoch: [174/900], step: [601/2479], loss: 2.205593\n",
      "Epoch: [174/900], step: [801/2479], loss: 1.403462\n",
      "Epoch: [174/900], step: [1001/2479], loss: 1.905711\n",
      "Epoch: [174/900], step: [1201/2479], loss: 1.750279\n",
      "Epoch: [174/900], step: [1401/2479], loss: 1.758340\n",
      "Epoch: [174/900], step: [1601/2479], loss: 1.778705\n",
      "Epoch: [174/900], step: [1801/2479], loss: 1.698875\n",
      "Epoch: [174/900], step: [2001/2479], loss: 1.818724\n",
      "Epoch: [174/900], step: [2201/2479], loss: 1.319787\n",
      "Epoch: [174/900], step: [2401/2479], loss: 1.975859\n",
      "Test: epoch 174 loss: 4.9384\n",
      "Train: epoch 175\n",
      "Epoch: [175/900], step: [1/2479], loss: 1.343374\n",
      "Epoch: [175/900], step: [201/2479], loss: 1.497141\n",
      "Epoch: [175/900], step: [401/2479], loss: 1.755767\n",
      "Epoch: [175/900], step: [601/2479], loss: 1.555662\n",
      "Epoch: [175/900], step: [801/2479], loss: 1.602208\n",
      "Epoch: [175/900], step: [1001/2479], loss: 2.140596\n",
      "Epoch: [175/900], step: [1201/2479], loss: 1.466767\n",
      "Epoch: [175/900], step: [1401/2479], loss: 2.268749\n",
      "Epoch: [175/900], step: [1601/2479], loss: 2.490079\n",
      "Epoch: [175/900], step: [1801/2479], loss: 2.015570\n",
      "Epoch: [175/900], step: [2001/2479], loss: 2.195995\n",
      "Epoch: [175/900], step: [2201/2479], loss: 1.964363\n",
      "Epoch: [175/900], step: [2401/2479], loss: 1.079856\n",
      "Test: epoch 175 loss: 3.1545653\n",
      "Train: epoch 176\n",
      "Epoch: [176/900], step: [1/2479], loss: 1.640433\n",
      "Epoch: [176/900], step: [201/2479], loss: 2.073740\n",
      "Epoch: [176/900], step: [401/2479], loss: 1.935808\n",
      "Epoch: [176/900], step: [601/2479], loss: 1.456744\n",
      "Epoch: [176/900], step: [801/2479], loss: 1.482225\n",
      "Epoch: [176/900], step: [1001/2479], loss: 1.906823\n",
      "Epoch: [176/900], step: [1201/2479], loss: 1.206619\n",
      "Epoch: [176/900], step: [1401/2479], loss: 1.255275\n",
      "Epoch: [176/900], step: [1601/2479], loss: 1.530171\n",
      "Epoch: [176/900], step: [1801/2479], loss: 1.973937\n",
      "Epoch: [176/900], step: [2001/2479], loss: 1.410650\n",
      "Epoch: [176/900], step: [2201/2479], loss: 2.029482\n",
      "Epoch: [176/900], step: [2401/2479], loss: 1.700294\n",
      "Test: epoch 176 loss: 7.080483\n",
      "Train: epoch 177\n",
      "Epoch: [177/900], step: [1/2479], loss: 1.617769\n",
      "Epoch: [177/900], step: [201/2479], loss: 1.975346\n",
      "Epoch: [177/900], step: [401/2479], loss: 2.124305\n",
      "Epoch: [177/900], step: [601/2479], loss: 1.558756\n",
      "Epoch: [177/900], step: [801/2479], loss: 1.756812\n",
      "Epoch: [177/900], step: [1001/2479], loss: 1.765756\n",
      "Epoch: [177/900], step: [1201/2479], loss: 1.712531\n",
      "Epoch: [177/900], step: [1401/2479], loss: 2.263061\n",
      "Epoch: [177/900], step: [1601/2479], loss: 2.418795\n",
      "Epoch: [177/900], step: [1801/2479], loss: 1.638312\n",
      "Epoch: [177/900], step: [2001/2479], loss: 1.786263\n",
      "Epoch: [177/900], step: [2201/2479], loss: 1.223535\n",
      "Epoch: [177/900], step: [2401/2479], loss: 1.822327\n",
      "Test: epoch 177 loss: 6.0251093\n",
      "Train: epoch 178\n",
      "Epoch: [178/900], step: [1/2479], loss: 1.485844\n",
      "Epoch: [178/900], step: [201/2479], loss: 1.055311\n",
      "Epoch: [178/900], step: [401/2479], loss: 1.602864\n",
      "Epoch: [178/900], step: [601/2479], loss: 1.042207\n",
      "Epoch: [178/900], step: [801/2479], loss: 1.569292\n",
      "Epoch: [178/900], step: [1001/2479], loss: 1.005830\n",
      "Epoch: [178/900], step: [1201/2479], loss: 1.595048\n",
      "Epoch: [178/900], step: [1401/2479], loss: 1.220452\n",
      "Epoch: [178/900], step: [1601/2479], loss: 1.447672\n",
      "Epoch: [178/900], step: [1801/2479], loss: 2.068532\n",
      "Epoch: [178/900], step: [2001/2479], loss: 2.230201\n",
      "Epoch: [178/900], step: [2201/2479], loss: 1.384081\n",
      "Epoch: [178/900], step: [2401/2479], loss: 2.307992\n",
      "Test: epoch 178 loss: 5.677368\n",
      "Train: epoch 179\n",
      "Epoch: [179/900], step: [1/2479], loss: 1.831132\n",
      "Epoch: [179/900], step: [201/2479], loss: 1.768622\n",
      "Epoch: [179/900], step: [401/2479], loss: 1.807592\n",
      "Epoch: [179/900], step: [601/2479], loss: 1.870233\n",
      "Epoch: [179/900], step: [801/2479], loss: 2.101647\n",
      "Epoch: [179/900], step: [1001/2479], loss: 1.616134\n",
      "Epoch: [179/900], step: [1201/2479], loss: 1.412417\n",
      "Epoch: [179/900], step: [1401/2479], loss: 1.897771\n",
      "Epoch: [179/900], step: [1601/2479], loss: 1.443986\n",
      "Epoch: [179/900], step: [1801/2479], loss: 1.560581\n",
      "Epoch: [179/900], step: [2001/2479], loss: 2.208080\n",
      "Epoch: [179/900], step: [2201/2479], loss: 2.223080\n",
      "Epoch: [179/900], step: [2401/2479], loss: 1.176740\n",
      "Test: epoch 179 loss: 5.8179293\n",
      "Train: epoch 180\n",
      "Epoch: [180/900], step: [1/2479], loss: 1.457360\n",
      "Epoch: [180/900], step: [201/2479], loss: 1.102320\n",
      "Epoch: [180/900], step: [401/2479], loss: 1.789128\n",
      "Epoch: [180/900], step: [601/2479], loss: 1.776646\n",
      "Epoch: [180/900], step: [801/2479], loss: 1.857629\n",
      "Epoch: [180/900], step: [1001/2479], loss: 2.221920\n",
      "Epoch: [180/900], step: [1201/2479], loss: 2.245032\n",
      "Epoch: [180/900], step: [1401/2479], loss: 2.189290\n",
      "Epoch: [180/900], step: [1601/2479], loss: 1.891709\n",
      "Epoch: [180/900], step: [1801/2479], loss: 1.883000\n",
      "Epoch: [180/900], step: [2001/2479], loss: 1.760895\n",
      "Epoch: [180/900], step: [2201/2479], loss: 1.984652\n",
      "Epoch: [180/900], step: [2401/2479], loss: 2.120969\n",
      "Test: epoch 180 loss: 5.906035\n",
      "Train: epoch 181\n",
      "Epoch: [181/900], step: [1/2479], loss: 1.777941\n",
      "Epoch: [181/900], step: [201/2479], loss: 1.481283\n",
      "Epoch: [181/900], step: [401/2479], loss: 1.426324\n",
      "Epoch: [181/900], step: [601/2479], loss: 1.742843\n",
      "Epoch: [181/900], step: [801/2479], loss: 1.389465\n",
      "Epoch: [181/900], step: [1001/2479], loss: 1.679210\n",
      "Epoch: [181/900], step: [1201/2479], loss: 1.620032\n",
      "Epoch: [181/900], step: [1401/2479], loss: 1.257074\n",
      "Epoch: [181/900], step: [1601/2479], loss: 1.340011\n",
      "Epoch: [181/900], step: [1801/2479], loss: 1.680930\n",
      "Epoch: [181/900], step: [2001/2479], loss: 0.992671\n",
      "Epoch: [181/900], step: [2201/2479], loss: 1.799075\n",
      "Epoch: [181/900], step: [2401/2479], loss: 1.515711\n",
      "Test: epoch 181 loss: 4.849998\n",
      "Train: epoch 182\n",
      "Epoch: [182/900], step: [1/2479], loss: 1.533713\n",
      "Epoch: [182/900], step: [201/2479], loss: 1.749869\n",
      "Epoch: [182/900], step: [401/2479], loss: 1.939573\n",
      "Epoch: [182/900], step: [601/2479], loss: 1.548730\n",
      "Epoch: [182/900], step: [801/2479], loss: 2.704689\n",
      "Epoch: [182/900], step: [1001/2479], loss: 1.615067\n",
      "Epoch: [182/900], step: [1201/2479], loss: 1.780779\n",
      "Epoch: [182/900], step: [1401/2479], loss: 1.982876\n",
      "Epoch: [182/900], step: [1601/2479], loss: 1.469993\n",
      "Epoch: [182/900], step: [1801/2479], loss: 1.926135\n",
      "Epoch: [182/900], step: [2001/2479], loss: 1.517875\n",
      "Epoch: [182/900], step: [2201/2479], loss: 1.907328\n",
      "Epoch: [182/900], step: [2401/2479], loss: 1.157173\n",
      "Test: epoch 182 loss: 5.609184\n",
      "Train: epoch 183\n",
      "Epoch: [183/900], step: [1/2479], loss: 1.593094\n",
      "Epoch: [183/900], step: [201/2479], loss: 1.447164\n",
      "Epoch: [183/900], step: [401/2479], loss: 1.254673\n"
     ]
    }
   ],
   "source": [
    "import random \n",
    "session.run(tf.global_variables_initializer())\n",
    "            \n",
    "invalid_number_prediction_counts = []\n",
    "all_model_predictions = []\n",
    "all_ground_truth = []\n",
    "\n",
    "saver = tf.train.Saver(max_to_keep=1000)\n",
    "print('Start training... \\n')\n",
    "for epoch in range(n_epochs):  \n",
    "    random.shuffle(train_set)\n",
    "    random.shuffle(test_set)\n",
    "    \n",
    "    print('Train: epoch', epoch + 1)\n",
    "    for n_iter, (X_batch, Y_batch) in enumerate(generate_batches(train_set, batch_size=batch_size)):\n",
    "\n",
    "        X_batch_ids, X_batch_ids_len = batch_to_ids(X_batch, word2id, max_len)\n",
    "        Y_batch_ids, Y_batch_ids_len = batch_to_ids(Y_batch, word2id, max_len)\n",
    "\n",
    "        predictions, loss = model.train_on_batch(session, \n",
    "                                                 X_batch_ids,\n",
    "                                                 X_batch_ids_len, \n",
    "                                                 Y_batch_ids, \n",
    "                                                 Y_batch_ids_len, \n",
    "                                                 learning_rate, \n",
    "                                                 dropout_keep_probability)\n",
    "    \n",
    "    \n",
    "        if n_iter % 200 == 0:\n",
    "            print(\"Epoch: [%d/%d], step: [%d/%d], loss: %f\" % (epoch + 1, n_epochs, n_iter + 1, n_step, loss))\n",
    "                \n",
    "    X_sent, Y_sent = next(generate_batches(test_set, batch_size=batch_size))\n",
    "\n",
    "\n",
    "    X_test_batch_ids, X_test_batch_ids_len = batch_to_ids(X_sent, word2id, max_len)\n",
    "    Y_test_batch_ids, Y_test_batch_ids_len = batch_to_ids(Y_sent, word2id, max_len)\n",
    "\n",
    "\n",
    "    predictions, loss = model.predict_for_batch_with_loss(session=session, \n",
    "                                                          X=X_test_batch_ids, \n",
    "                                                          X_seq_len=X_test_batch_ids_len,\n",
    "                                                          Y=Y_test_batch_ids, \n",
    "                                                          Y_seq_len=Y_test_batch_ids_len)\n",
    "    print('Test: epoch', epoch + 1, 'loss:', loss,)\n",
    "    saver.save(session, 'checkpoints/model_four_'+str(epoch))\n",
    "\n",
    "\n",
    "#         print('2.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = model.get_response(session, \"what's up?\")\n",
    "print(response)# response = model.get_reply(session, \"Will it ran tomorrow?\")\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = model.get_reply(session, \"do you love to dance\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
